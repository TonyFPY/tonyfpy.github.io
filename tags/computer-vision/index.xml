<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Computer Vision on Tony Feng</title>
        <link>https://tonyfpy.github.io/tags/computer-vision/</link>
        <description>Recent content in Computer Vision on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 14 Feb 2023 14:54:35 -0500</lastBuildDate><atom:link href="https://tonyfpy.github.io/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Computer Vision] Feature Matching</title>
        <link>https://tonyfpy.github.io/p/computer-vision-feature-matching/</link>
        <pubDate>Tue, 14 Feb 2023 14:54:35 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-feature-matching/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 14th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 14th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Feature Points &amp; Corners</title>
        <link>https://tonyfpy.github.io/p/computer-vision-feature-points-corners/</link>
        <pubDate>Fri, 10 Feb 2023 14:42:23 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-feature-points-corners/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 10nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 14nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Edges and corners are typically called &lt;strong&gt;features&lt;/strong&gt; that are unique to some part of the image. &lt;strong&gt;Local features&lt;/strong&gt;  describe one component of the image, not the entirely image.&lt;/p&gt;
&lt;p&gt;Corners are also called &lt;strong&gt;interest points&lt;/strong&gt; or &lt;strong&gt;key points&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Corners are important, because we can get &lt;strong&gt;spare correspondences&lt;/strong&gt;, which allows for fast matching between images.&lt;/p&gt;
&lt;p&gt;Finding distinctive and repeatable feature points can be difficult when we want our features to be &lt;strong&gt;invariant to large transformations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;geometric variation (translation, rotation, scale, perspective)&lt;/li&gt;
&lt;li&gt;appearance variation (reflectance, illumination)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-panorama-stitching&#34;&gt;Example: Panorama Stitching&lt;/h3&gt;
&lt;p&gt;Using panorama requires &lt;strong&gt;correspondences&lt;/strong&gt;. We have two images of a scene, and we can see that there is some overlay, but the images are not quite the same.&lt;/p&gt;
&lt;p&gt;We need our features/corners to have some sort of properties in this case: 1) &lt;strong&gt;distinctiveness&lt;/strong&gt;, 2) &lt;strong&gt;repeatability&lt;/strong&gt;, 3) &lt;strong&gt;compactness&lt;/strong&gt;, 4) &lt;strong&gt;efficiency&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;local-feature-matching&#34;&gt;Local Feature Matching&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Detection
&lt;ul&gt;
&lt;li&gt;Find a set of distinctive features (e.g. key points, edges)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Description:
&lt;ul&gt;
&lt;li&gt;Extract feature descriptor around each interest point as vector.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Matching:
&lt;ul&gt;
&lt;li&gt;Compute distance between feature vectors to find correspondence.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;corner-detection&#34;&gt;Corner Detection&lt;/h2&gt;
&lt;h3 id=&#34;basic-idea&#34;&gt;Basic idea&lt;/h3&gt;
&lt;p&gt;We do not know which other image locations the feature will end up being matched against. But we can compute how stable a location is in appearance with respect to small variations in its position. In other words, &lt;strong&gt;we can compare image patch against local neighbors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We want a window &lt;strong&gt;shift in any direction&lt;/strong&gt; to give a &lt;strong&gt;large change&lt;/strong&gt; in intensity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/CD.jpg&#34;
	width=&#34;539&#34;
	height=&#34;201&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/CD_hub5186c6b3aa5c483690a5be405bb9d7b_22375_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/CD_hub5186c6b3aa5c483690a5be405bb9d7b_22375_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;643px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;auto-correlation&#34;&gt;Auto-correlation&lt;/h3&gt;
&lt;p&gt;$$E(u, v)=\sum_{x, y} w(x, y)[I(x+u, y+v)-I(x, y)]^{2}$$&lt;/p&gt;
&lt;p&gt;, where $w(x, y)$ is window function, $I(x+u, y+v)$ is shifted intensity, $I(x, y)$ is intensity. If our &amp;ldquo;red box&amp;rdquo; goes outside the image, then we would need to pad the image.&lt;/p&gt;
&lt;p&gt;However, this approach is &lt;strong&gt;computationally expensive&lt;/strong&gt;. It needs $O(W^2+S^2+I^2)$, where $W$, $S$, $I$ stand for window size, shift range and image size, respectively.&lt;/p&gt;
&lt;h3 id=&#34;quadratic-surface&#34;&gt;Quadratic Surface&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/QS.jpg&#34;
	width=&#34;512&#34;
	height=&#34;214&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/QS_hu4409070e4a33ffe4681a89ec0705fd61_21561_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/QS_hu4409070e4a33ffe4681a89ec0705fd61_21561_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We can approximate $E(u,v)$ locally by a &lt;strong&gt;quadratic surface&lt;/strong&gt;, and look for that instead.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Taylor Series Expansion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A function $f$ can be represented by an infinite series of its derivatives at a single point $a$.&lt;/p&gt;
&lt;p&gt;$$ f = f(a)+\frac{f^{\prime}(a)}{1 !}(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^{2}+\frac{f^{\prime \prime \prime}(a)}{3 !}(x-a)^{3}+\cdots $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Local Quadratic Approximation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Local quadratic approximation of $E(u,v)$ in the neighborhood of $(0,0)$ is given by the &lt;strong&gt;second-order Taylor expansion&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA1.jpg&#34;
	width=&#34;599&#34;
	height=&#34;171&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA1_hub5f34c22bfb9b58a3200fadfe53457a9_25218_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA1_hub5f34c22bfb9b58a3200fadfe53457a9_25218_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;350&#34;
		data-flex-basis=&#34;840px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;This could be simplified as the following equation, where $M$ is a &lt;strong&gt;structure tensor&lt;/strong&gt; computed from image derivatives.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA2.jpg&#34;
	width=&#34;607&#34;
	height=&#34;200&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA2_hu58ee0901517331e3436221a7560a45c9_18732_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA2_hu58ee0901517331e3436221a7560a45c9_18732_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;303&#34;
		data-flex-basis=&#34;728px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/M.jpg&#34;
	width=&#34;680&#34;
	height=&#34;200&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/M_hud4531995c10f99040ff50540ad6e262d_16734_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/M_hud4531995c10f99040ff50540ad6e262d_16734_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;340&#34;
		data-flex-basis=&#34;816px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/IxIy.jpg&#34;
	width=&#34;484&#34;
	height=&#34;156&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/IxIy_huddd6a4e4adacf5420d732890bc827c89_17364_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/IxIy_huddd6a4e4adacf5420d732890bc827c89_17364_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;310&#34;
		data-flex-basis=&#34;744px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpreting the Second Moment Matrix&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M$ is symmetric around the diagonal
&lt;ul&gt;
&lt;li&gt;Symmetric matrices have &lt;strong&gt;orthogonal eigenvectors&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$M$ is square
&lt;ul&gt;
&lt;li&gt;Square matrices are diagonalizable if some matrix $R$ exists such that $M = R^{-1}AR$ where $A$ has only diagonal entries and $R$ represents a change of basis (in 2D, a rotation).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/MRAR.jpg&#34;
	width=&#34;200&#34;
	height=&#34;68&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/MRAR_huad8e7fff2e9f1b263e070ffa4a705856_4224_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/MRAR_huad8e7fff2e9f1b263e070ffa4a705856_4224_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;294&#34;
		data-flex-basis=&#34;705px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We want to decompose $M$ to be able to &lt;strong&gt;analyze the scale of these quadratic surfaces&lt;/strong&gt; while &lt;strong&gt;ignoring the rotation&lt;/strong&gt;. This means we care that it is a corner or an edge, not which rotation of edge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/eigenvalues.jpg&#34;
	width=&#34;549&#34;
	height=&#34;273&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/eigenvalues_hufdf0a7aba31a58dbfa4f3b7a8911d286_29304_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/eigenvalues_hufdf0a7aba31a58dbfa4f3b7a8911d286_29304_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;482px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Or we can combine $\lambda _1$, $\lambda _2$ with some constant $\alpha$ to computer the &lt;strong&gt;cornerness score&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$ C={\lambda _1}{\lambda _2}-{\alpha}({\lambda _1}+{\lambda _2})^{2} $$&lt;/p&gt;
&lt;p&gt;In linear algebra, for diagonal matrices,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Determinant of a diagonal matrix is also the product of its diagonal entries&lt;/li&gt;
&lt;li&gt;Trace is just the sum of its diagonal entries&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can avoid explicit eigenvalue computation by
$$C=\operatorname{det}(A)-\alpha \operatorname{trace}(A)^{2}$$&lt;/p&gt;
&lt;p&gt;Since $S^{-1}$ is the inverse matrix of $S$, we have $S^{-1}\cdot S = I$&lt;/p&gt;
&lt;p&gt;$$\operatorname{det}(I) = \operatorname{det}(S\cdot S^{-1}) = \operatorname{det}(S)\cdot \operatorname{det}(S) = 1$$&lt;/p&gt;
&lt;p&gt;We can now obtain that $\operatorname{det}(M) = \operatorname{det}(A)$&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;p&gt;$$C=\operatorname{det}(M)-\alpha \operatorname{trace}(M)^{2}$$&lt;/p&gt;
&lt;h3 id=&#34;harris-corner-detector&#34;&gt;Harris Corner Detector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Input image: computer $M$ at each pixel level&lt;/li&gt;
&lt;li&gt;Compute image derivatives $I_{x}$, $I_{y}$&lt;/li&gt;
&lt;li&gt;Compute $M$ components as squares of derivatives $I_{x}^{2}$, $I_{y}^{2}$&lt;/li&gt;
&lt;li&gt;Gaussian filter $g$ with width $s$: $g\left(I_{x}^{2}\right)$, $g\left(I_{y}^{2}\right)$, $g\left(I_{x} \circ I_{y}\right)$&lt;/li&gt;
&lt;li&gt;Compute cornerness $C=g\left(I_{x}^{2}\right) \circ g\left(I_{y}^{2}\right)-g\left(I_{x} \circ I_{y}\right)^{2} -\alpha\left[g\left(I_{x}^{2}\right)+g\left(I_{y}^{2}\right)\right]^{2}$&lt;/li&gt;
&lt;li&gt;Threshold $C$ to pick high cornerness&lt;/li&gt;
&lt;li&gt;Non-maximal suppression to pick peaks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: $a \circ b$ is &lt;strong&gt;Hadamard product&lt;/strong&gt; (element-wise multiplication).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;scale-invariant-detection&#34;&gt;Scale Invariant Detection&lt;/h2&gt;
&lt;h3 id=&#34;invariance--covariance&#34;&gt;Invariance &amp;amp; Covariance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Invariance&lt;/strong&gt;: image is transformed and corner locations do not change.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Covariance / Equivariance&lt;/strong&gt;: if we have two transformed versions of the same image, features should be detected in corresponding locations.&lt;/p&gt;
&lt;h3 id=&#34;properties-of-harris-detector&#34;&gt;Properties of Harris Detector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Corner response is invariant to image rotation
&lt;ul&gt;
&lt;li&gt;Ellipse rotates but its shape (i.e. eigenvalues) remains the same&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial invariance to &lt;strong&gt;affine intensity change&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Invariance to intensity shift $I \rightarrow I + b$&lt;/li&gt;
&lt;li&gt;Still affected by intensity scaling $I \rightarrow aI$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-invariant to image scale&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/scale.jpg&#34;
	width=&#34;752&#34;
	height=&#34;220&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/scale_hu49b520efb2e95a6ee537b02ec5b1b836_19368_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/scale_hu49b520efb2e95a6ee537b02ec5b1b836_19368_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;341&#34;
		data-flex-basis=&#34;820px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;automatic-scale-selection&#34;&gt;Automatic Scale Selection&lt;/h3&gt;
&lt;p&gt;For a point in one image, we can consider it as a function of region size&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/ASS.jpg&#34;
	width=&#34;997&#34;
	height=&#34;390&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/ASS_hu49b520efb2e95a6ee537b02ec5b1b836_79250_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/ASS_hu49b520efb2e95a6ee537b02ec5b1b836_79250_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;255&#34;
		data-flex-basis=&#34;613px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;For both image, we want to chose a scale that corresponds to the &lt;strong&gt;peak&lt;/strong&gt; in both images. It’s likely that the peaks is the best scale for finding the correpsondences.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/FR.jpg&#34;
	width=&#34;1352&#34;
	height=&#34;234&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/FR_hu49b520efb2e95a6ee537b02ec5b1b836_39560_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/FR_hu49b520efb2e95a6ee537b02ec5b1b836_39560_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;577&#34;
		data-flex-basis=&#34;1386px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Edge Detection</title>
        <link>https://tonyfpy.github.io/p/computer-vision-edge-detection/</link>
        <pubDate>Fri, 10 Feb 2023 10:54:42 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-edge-detection/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 10nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 10nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The process of identifying parts of a digital image with &lt;strong&gt;sharp changes&lt;/strong&gt; (discontinuities) in image intensity.&lt;/p&gt;
&lt;p&gt;It is helpful to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recognize objects&lt;/li&gt;
&lt;li&gt;Reconstruct scenes&lt;/li&gt;
&lt;li&gt;Edit images (artistically)&lt;/li&gt;
&lt;li&gt;Recover geometry and viewpoint&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;causes-of-edges&#34;&gt;Causes of Edges&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Surface orientation discontinuity
&lt;ul&gt;
&lt;li&gt;Physical edges in the 3D object&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Depth discontinuity
&lt;ul&gt;
&lt;li&gt;Viewing objects from a certain perspective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Surface color discontinuity
&lt;ul&gt;
&lt;li&gt;Differences in colors between&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Illumination discontinuity
&lt;ul&gt;
&lt;li&gt;Lighting and shadows&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;characterizing-edges&#34;&gt;Characterizing Edges&lt;/h2&gt;
&lt;h3 id=&#34;gradient&#34;&gt;Gradient&lt;/h3&gt;
&lt;p&gt;Simple algorithm for edge detection is to find gradient. The first derivative is strongest (i.e. has the highest magnitude) where the intensity changes most rapidly. The sign of the derivative depends on whether the intensity falls from high to low or rises from low to high.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/ED.jpg&#34;
	width=&#34;1102&#34;
	height=&#34;476&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/ED_hu49b520efb2e95a6ee537b02ec5b1b836_54934_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/ED_hu49b520efb2e95a6ee537b02ec5b1b836_54934_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;231&#34;
		data-flex-basis=&#34;555px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;effects-of-noise&#34;&gt;Effects of Noise&lt;/h3&gt;
&lt;p&gt;Real images are &lt;strong&gt;noisy&lt;/strong&gt;, which results in lots of noise in the first derivative and starts to overwhelm the peaks. It would be almost impossible to detect where the edge occurred by inspecting the derivative graph with all that noise.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/noise.jpg&#34;
	width=&#34;782&#34;
	height=&#34;496&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/noise_hu49b520efb2e95a6ee537b02ec5b1b836_93450_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/noise_hu49b520efb2e95a6ee537b02ec5b1b836_93450_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/smooth.jpg&#34;
	width=&#34;824&#34;
	height=&#34;578&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/smooth_hu49b520efb2e95a6ee537b02ec5b1b836_99823_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/smooth_hu49b520efb2e95a6ee537b02ec5b1b836_99823_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;342px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We can fix this by filtering the signal (image) first to &lt;strong&gt;smooth out the noise&lt;/strong&gt; before computing the derivative. Here, a gaussian filter is common. To find edges, look for peaks in $\frac{\mathrm{d}}{\mathrm{d} x}(f*g)$&lt;/p&gt;
&lt;h3 id=&#34;derivative-theorem&#34;&gt;Derivative Theorem&lt;/h3&gt;
&lt;p&gt;$$\frac{d}{d x}(f * g)=f * \frac{d}{d x} g$$&lt;/p&gt;
&lt;p&gt;This means that we can compute the derivative just on the filter $g$ and convolve the image by the differentiated filter to get the same result. &lt;strong&gt;Instead of convolving the image twice (once to denoise and once to compute the derivative), we only need to convolve once.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;tradeoff&#34;&gt;Tradeoff&lt;/h3&gt;
&lt;p&gt;Smoothed derivative removes noise, but blurs edge, also finds edges at different frequencies. The bigger the kernel, the more smoothed out the edges of the image are since we start removing lower frequencies.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/SLT.jpg&#34;
	width=&#34;1478&#34;
	height=&#34;466&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/SLT_hu49b520efb2e95a6ee537b02ec5b1b836_77045_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/SLT_hu49b520efb2e95a6ee537b02ec5b1b836_77045_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;317&#34;
		data-flex-basis=&#34;761px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;designing-an-edge-detector&#34;&gt;Designing an Edge Detector&lt;/h2&gt;
&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Good detection
&lt;ul&gt;
&lt;li&gt;Finding all real edges&lt;/li&gt;
&lt;li&gt;Ignoring noise or other artifacts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Good localization
&lt;ul&gt;
&lt;li&gt;The edges detected must be as close as possible to the true edges&lt;/li&gt;
&lt;li&gt;Returning one point only for each true edge point&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cues-of-edge-detection&#34;&gt;Cues of Edge Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Differences in color, intensity, or texture across the boundary&lt;/li&gt;
&lt;li&gt;Continuity and closure&lt;/li&gt;
&lt;li&gt;High-level knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;canny-edge-detector&#34;&gt;Canny Edge Detector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Filter image with $x$, $y$ derivatives of Gaussian&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/DGF.jpg&#34;
	width=&#34;382&#34;
	height=&#34;157&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/DGF_hube11b2e98728249cb58beb349eb32f62_14110_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/DGF_hube11b2e98728249cb58beb349eb32f62_14110_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;583px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find magnitude and orientation of gradient&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ |G| =\sqrt{G_{x}^{2}+G_{y}^{2}} $$
$$\theta(x, y) =\arctan \left(\frac{G_{y}}{G_{x}}\right)$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-maximum Suppression e.g. Bilinear Interpolation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;BI.jpg&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;lsquo;Hysteresis&amp;rsquo; Thresholding
&lt;ul&gt;
&lt;li&gt;Define two thresholds: low and high
&lt;ul&gt;
&lt;li&gt;Grad. mag. &amp;gt; high threshold = strong edge&lt;/li&gt;
&lt;li&gt;Grad. mag. &amp;lt; low threshold = noise&lt;/li&gt;
&lt;li&gt;In between = weak edge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use the high threshold to start edge curves&lt;/li&gt;
&lt;li&gt;Use the low threshold to continue them into weak edges&lt;/li&gt;
&lt;li&gt;&amp;lsquo;Follow&amp;rsquo; edges starting from strong edge pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/canny.jpg&#34;
	width=&#34;448&#34;
	height=&#34;301&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/canny_hu0162cb701aa7ddb57f7a208d7daf3250_70554_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/canny_hu0162cb701aa7ddb57f7a208d7daf3250_70554_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Final Result&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;357px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://justin-liang.com/tutorials/canny/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CANNY EDGE DETECTION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/canny-edge-detection-step-by-step-in-python-computer-vision-b49c3a2d8123&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Canny Edge Detection Step by Step in Python — Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Fourier Series &amp; Fourier Transform</title>
        <link>https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/</link>
        <pubDate>Thu, 02 Feb 2023 11:16:17 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 2nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 9nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fourier-series&#34;&gt;Fourier Series&lt;/h2&gt;
&lt;h3 id=&#34;a-general-idea&#34;&gt;A General Idea&lt;/h3&gt;
&lt;p&gt;Any univariate function can be rewritten as a weighted sum of sines and cosines of different frequencies.&lt;/p&gt;
&lt;p&gt;$$F_{Target} = F_{1}+F_{2}+F_{3} \ldots$$&lt;/p&gt;
&lt;p&gt;Here is the &lt;strong&gt;sine-cosine&lt;/strong&gt; form&lt;/p&gt;
&lt;p&gt;$$F = \sum_{n=1}^{\infty}\left(a_{n} \cos (n t)+b_{n} \sin (n t)\right)$$&lt;/p&gt;
&lt;h3 id=&#34;spatial-and-frequency-domain&#34;&gt;Spatial and Frequency Domain&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/SF.jpg&#34;
	width=&#34;372&#34;
	height=&#34;178&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/SF_hu2d9f041ef587ac7cce011e256b0c0385_9672_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/SF_hu2d9f041ef587ac7cce011e256b0c0385_9672_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;208&#34;
		data-flex-basis=&#34;501px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;amplitude--phase&#34;&gt;Amplitude &amp;amp; Phase&lt;/h3&gt;
&lt;p&gt;We can convert ine-cosine representation to &lt;strong&gt;amplitude-phase&lt;/strong&gt; form.&lt;/p&gt;
&lt;p&gt;$$F = \sum_{n=1}^{\infty}\left(a_{n} \sin \left(n x+\emptyset_{n}\right)\right)$$&lt;/p&gt;
&lt;p&gt;Amplitude encodes &lt;strong&gt;how much signal&lt;/strong&gt; there is at a particular frequency, while Phase encodes &lt;strong&gt;spatial information&lt;/strong&gt;. In other words, Amplitude tells you &amp;ldquo;how much&amp;rdquo; and Phase tells you &amp;ldquo;where&amp;rdquo;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fourier-transform&#34;&gt;Fourier transform&lt;/h2&gt;
&lt;h3 id=&#34;computing-the-fourier-transform&#34;&gt;Computing the Fourier Transform&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Continuous&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$H(\omega)=\int_{-\infty}^{\infty} h(x) e^{-j \omega x} d x$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discrete&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$H(k)=\frac{1}{N} \sum_{x=0}^{N-1} h(x) e^{-j \frac{2 \pi k x}{N}} \quad ,k=-\frac{k}{2} &amp;hellip; \frac{k}{2}$$&lt;/p&gt;
&lt;h3 id=&#34;properties-of-fourier-transforms&#34;&gt;Properties of Fourier Transforms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linearity $\mathrm{F}[a x(t)+b y(t)]=a \mathrm{~F}[x(t)]+b \mathrm{~F}[y(t)]$&lt;/li&gt;
&lt;li&gt;Fourier transform of a real signal is &lt;strong&gt;symmetric&lt;/strong&gt; about the origin.&lt;/li&gt;
&lt;li&gt;The energy of the signal is the same as the energy of its Fourier transform.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gaussian-filter-duality&#34;&gt;Gaussian Filter Duality&lt;/h3&gt;
&lt;p&gt;Fourier transform of one Gaussian is another Gaussian (with inverse variance).&lt;/p&gt;
&lt;p&gt;Why is this useful?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smooth degradation in frequency components&lt;/li&gt;
&lt;li&gt;No sharp cut-off&lt;/li&gt;
&lt;li&gt;No negative values&lt;/li&gt;
&lt;li&gt;Never zero (infinite extent)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2d-discrete-fourier-transform&#34;&gt;2D Discrete Fourier Transform&lt;/h3&gt;
&lt;p&gt;$$F[u, v]=\frac{1}{M N} \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} I[m, n] \cdot e^{-i 2 \pi\left(\frac{u m}{M}+\frac{v n}{N}\right)}$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/2DDFT.jpg&#34;
	width=&#34;627&#34;
	height=&#34;392&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/2DDFT_hu0f77aecba7dbdde634723272c2b72c55_50865_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/2DDFT_hu0f77aecba7dbdde634723272c2b72c55_50865_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;383px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Nyquist frequency is half the sampling rate of the signal. Sampling rate is size of image $M*N$, so Fourier transform images are ($\pm \frac{N}{2} $, $\pm \frac{N}{2} $).&lt;/p&gt;
&lt;p&gt;Image is rotationally symmetric about center because of negative frequencies.&lt;/p&gt;
&lt;p&gt;If we have infinite frequencies, why does the image end?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frequencies higher than Nyquist frequency end up falling on an existing sample.&lt;/li&gt;
&lt;li&gt;Nyquist frequency is half the sampling frequency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fourier-decomposition-image&#34;&gt;Fourier Decomposition Image&lt;/h3&gt;
&lt;p&gt;Intuitively, we can obtain the image by correlating the signal with a set of waves of increasing frequency.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In 2D, $O(N^2)$ operation&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;Fast Fourier Transform (FFT)&lt;/strong&gt;, $O(NlogN)$ (effective for larger image)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first row is set of the Fourier amplitude images and the second row is set of spatial domain imahe.
&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/amplitudematch.jpg&#34;
	width=&#34;697&#34;
	height=&#34;283&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/amplitudematch_huce222094ececc1ef2dc6fb35fb3a164b_59800_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/amplitudematch_huce222094ececc1ef2dc6fb35fb3a164b_59800_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;591px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The frequency amplitude of natural images are quite similar. &lt;strong&gt;Most information in the image is carried in the phase, not the amplitude.&lt;/strong&gt;
In Fourier space, Phase is more of the information that we see in the visual world.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/cheebra.jpg&#34;
	width=&#34;500&#34;
	height=&#34;250&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/cheebra_hudf23f477bf613526b59c8c9b9726af62_56824_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/cheebra_hudf23f477bf613526b59c8c9b9726af62_56824_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Zebra Phase &amp;#43; Cheetah Amplitude &amp; Cheetah Phase &amp;#43; Zebra Amplitude&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;200&#34;
		data-flex-basis=&#34;480px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-convolution-theorem&#34;&gt;The Convolution Theorem&lt;/h3&gt;
&lt;p&gt;The Fourier transform of the convolution of two functions is the product of their Fourier transforms.&lt;/p&gt;
&lt;p&gt;$$F[g \otimes h] = F[g]F[h]$$&lt;/p&gt;
&lt;p&gt;Convolution in spatial domain is equivalent to multiplication in frequency domain.&lt;/p&gt;
&lt;p&gt;$$g \otimes h=\mathrm{F}^{-1}[\mathrm{~F}[g] \mathrm{F}[h]]$$&lt;/p&gt;
&lt;p&gt;If convolution is just multiplication in the Fourier domain, isn’t deconvolution just using division?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sometimes, it clearly is invertible (e.g. a convolution with an identity filter)&lt;/li&gt;
&lt;li&gt;In one case, it clearly isn&amp;rsquo;t invertible (e.g. convolution with an all zero filter)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A Gaussian is only zero at infinity, so it is invertible&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/conv.jpg&#34;
	width=&#34;610&#34;
	height=&#34;350&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/conv_hu9395e367661edb0f9c21ab2dc3961cbf_56034_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/conv_hu9395e367661edb0f9c21ab2dc3961cbf_56034_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;418px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/deconv.jpg&#34;
	width=&#34;610&#34;
	height=&#34;349&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/deconv_huff484edc0f7480e440782c6f912da8c0_58189_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/deconv_huff484edc0f7480e440782c6f912da8c0_58189_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;filtering-in-frequency-domain&#34;&gt;Filtering in Frequency Domain&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convert image and filter to Fourier domain&lt;/li&gt;
&lt;li&gt;Element-wise multiply their decompositions&lt;/li&gt;
&lt;li&gt;Convert result to spatial domain with inverse Fourier transform&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Image Filtering</title>
        <link>https://tonyfpy.github.io/p/computer-vision-image-filtering/</link>
        <pubDate>Tue, 31 Jan 2023 14:49:27 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-image-filtering/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Jan 31st, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 2nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;filtering&#34;&gt;Filtering&lt;/h2&gt;
&lt;h3 id=&#34;def&#34;&gt;Def&lt;/h3&gt;
&lt;p&gt;An operation that modifies a (measured) signal, which includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;removing undesirable components&lt;/li&gt;
&lt;li&gt;transforming signal in a desirable way&lt;/li&gt;
&lt;li&gt;extract specific components of a signal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in measured or discrete signals because that is what we get from hardware such as sensors.&lt;/p&gt;
&lt;h3 id=&#34;1d-filtering-moving-average&#34;&gt;1D Filtering: Moving Average&lt;/h3&gt;
&lt;p&gt;We have a signal $S \in \mathcal{R}^{m \times 1}$, where $m$ is the number of samples we take of the signal, and each sample is a single scalar value. The moving average $h[n]$ at the point $n$ is the average value of the signal over the $k$ most recent samples, which can be represented as:&lt;/p&gt;
&lt;p&gt;$$h[n]=\frac{1}{k} \sum_{i=n-k+1}^{n} S[i]$$&lt;/p&gt;
&lt;p&gt;or
$$h[n]=\frac{1}{k} {I}^{T} S[n-k+1: n]$$&lt;/p&gt;
&lt;p&gt;, where $I$ is and identity vector.&lt;/p&gt;
&lt;h3 id=&#34;2d-filtering&#34;&gt;2D Filtering&lt;/h3&gt;
&lt;p&gt;For images, we’re interested in filtering along two spatial axes ($x$ and $y$) rather than a single one.&lt;/p&gt;
&lt;p&gt;$$h[m, n]=\sum_{k, l} f[k, l] S[m+k, n+l]$$&lt;/p&gt;
&lt;p&gt;The equation says that the filtered value at a location $(m,n)$ is the sum over products of the filtering function $f$ and the image $S$ in a local neighborhood. The size of the window is determined by $k$ and $l$.&lt;/p&gt;
&lt;h3 id=&#34;image-filtering&#34;&gt;Image Filtering&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/IF.jpg&#34;
	width=&#34;442&#34;
	height=&#34;226&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/IF_hud60089819ef7b0115b7722b64f02a782_17374_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/IF_hud60089819ef7b0115b7722b64f02a782_17374_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;195&#34;
		data-flex-basis=&#34;469px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Box Filter/Mean Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The box filter blurs the image because for each pixel, the values of its neighbors is &lt;strong&gt;averaged&lt;/strong&gt; with it, and &lt;strong&gt;preserve mean image intensity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gaussian Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The filter function is constructed by sampling the &lt;strong&gt;gaussian&lt;/strong&gt; function at uniform intervals. &lt;strong&gt;It&amp;rsquo;s a linear filter&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Gaussian pdfs have &lt;strong&gt;infinite extent&lt;/strong&gt;. &lt;strong&gt;Gaussian filters are discrete finite samplings of the Gaussian pdf&lt;/strong&gt;. What that means is that the filter can be arbitrarily large, and it will never be zero (to numerical precision).&lt;/p&gt;
&lt;p&gt;$$ G_{\sigma}=\frac{1}{2 \pi \sigma^{2}} e^{-\frac{\left(x^{2}+y^{2}\right)}{2 \sigma^{2}}} $$&lt;/p&gt;
&lt;p&gt;Unlike box filter, does not result in &amp;lsquo;grid&amp;rsquo;-like artifacts.&lt;/p&gt;
&lt;p&gt;Gaussian Filter Properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian convolved with Gaussian is another Gaussian&lt;/li&gt;
&lt;li&gt;We can smooth with small-width kernel, repeat, and get same result as larger-width kernel&lt;/li&gt;
&lt;li&gt;Convolving twice with Gaussian kernel of width $\sigma$ is same as convolving once with kernel of width $\sigma \sqrt{2} $&lt;/li&gt;
&lt;li&gt;How big should the Gaussian filter be?
&lt;ul&gt;
&lt;li&gt;Values at edges should be near zero&lt;/li&gt;
&lt;li&gt;Gaussians have infinite extent&lt;/li&gt;
&lt;li&gt;Set filter half-width to about $3\sigma$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sobel Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It has &amp;ldquo;1 2 1&amp;rdquo; pattern and is often used in &lt;strong&gt;edge detection&lt;/strong&gt;. It works by calculating the gradient of image intensity at each pixel within the image. It&amp;rsquo;s a &lt;strong&gt;high pass filter&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/sobel.jpg&#34;
	width=&#34;375&#34;
	height=&#34;66&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/sobel_hu2240eefee00e248333dc7c3fdcbcacc9_1728_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/sobel_hu2240eefee00e248333dc7c3fdcbcacc9_1728_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;568&#34;
		data-flex-basis=&#34;1363px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Median Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is a &lt;strong&gt;non-linear&lt;/strong&gt; filter. It operates over a window by selecting the &lt;strong&gt;median intensity&lt;/strong&gt; in the window. Median filtering is sorting.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s good at &lt;strong&gt;removing noises&lt;/strong&gt;. If we increase the size of the window, it can maintain strong edges, while other parts become blurred.&lt;/p&gt;
&lt;h3 id=&#34;practice-with-linear-filters&#34;&gt;Practice with Linear Filters&lt;/h3&gt;
&lt;p&gt;We can use filter to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enhance images
&lt;ul&gt;
&lt;li&gt;Denoise, resize, increase contrast, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Extract information from images
&lt;ul&gt;
&lt;li&gt;Texture, edges, distinctive points, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Detect patterns
&lt;ul&gt;
&lt;li&gt;Template matching&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F1.jpg&#34;
	width=&#34;521&#34;
	height=&#34;207&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F1_hu392a3c70df25a1f9896f7ab2d21ecd55_32259_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F1_hu392a3c70df25a1f9896f7ab2d21ecd55_32259_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;251&#34;
		data-flex-basis=&#34;604px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F2.jpg&#34;
	width=&#34;521&#34;
	height=&#34;198&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F2_huac7025d1a9ef42b2460b73ca69389e76_49504_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F2_huac7025d1a9ef42b2460b73ca69389e76_49504_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;263&#34;
		data-flex-basis=&#34;631px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F5.jpg&#34;
	width=&#34;564&#34;
	height=&#34;206&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F5_hu90f480af6279f8bfb5f99bd9066a024f_58167_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F5_hu90f480af6279f8bfb5f99bd9066a024f_58167_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;657px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F3.jpg&#34;
	width=&#34;628&#34;
	height=&#34;369&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F3_hu3f97afa3a3e8d31bfcaaf1a13dd2cccd_192991_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F3_hu3f97afa3a3e8d31bfcaaf1a13dd2cccd_192991_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;408px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F4.jpg&#34;
	width=&#34;630&#34;
	height=&#34;373&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F4_hu3549cfd22b45a8393c24eb0751f31c80_191334_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F4_hu3549cfd22b45a8393c24eb0751f31c80_191334_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;405px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dealing with Borders&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pixels are undefined outside the border of our image, so to apply the filter near the boundary, we nee to extend the image past its boundary using one of many possible methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Clip filter&lt;/strong&gt;: any value outside of the image is set to 0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Copy edge&lt;/strong&gt;: copy the pixel value of the nearest edge&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wrap around&lt;/strong&gt;: copy the values near the opposite edge&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reflect across edge&lt;/strong&gt;: treat out-of-bounds regions as ‘mirrors’ that reflect near-surface image pixels in reverse order&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b1.jpg&#34;
	width=&#34;318&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b1_hu58ab2db678a16a964fd350dc4a15b080_44902_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/b1_hu58ab2db678a16a964fd350dc4a15b080_44902_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b2.jpg&#34;
	width=&#34;318&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b2_hu60a746b593d6fe1a0dbc27ff358f7aac_51468_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/b2_hu60a746b593d6fe1a0dbc27ff358f7aac_51468_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b3.jpg&#34;
	width=&#34;318&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b3_hu661dca684b998aa4475277ab8d2445df_46269_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/b3_hu661dca684b998aa4475277ab8d2445df_46269_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b4.jpg&#34;
	width=&#34;318&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b4_hubec94fa1a5e4aff4b5caf37c182c502e_50111_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/b4_hubec94fa1a5e4aff4b5caf37c182c502e_50111_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;properties-of-image-filtering-methods&#34;&gt;Properties of Image Filtering Methods&lt;/h2&gt;
&lt;h3 id=&#34;correlation-and-convolution&#34;&gt;Correlation and Convolution&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;2D Correlation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$h[m, n]=\sum_{k, l} f[k, l] S[m+k, n+l]$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2D Convolution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$h[m, n]=\sum_{k, l} f[k, l] S[m-k, n-l]$$&lt;/p&gt;
&lt;p&gt;Convolution is the same as correlation with a 180° rotated filter kernel.&lt;/p&gt;
&lt;p&gt;Correlation and convolution are identical when the filter kernel is rotationally &lt;strong&gt;symmetric&lt;/strong&gt; (a square matrix that is equal to its transpose).&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;symmetric&lt;/strong&gt; filters: use either convolution or correlation&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;non-symmetric&lt;/strong&gt; filters: correlation is &lt;strong&gt;template matching&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;More can be found &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=tS-ib_mgGbU&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;linear-filter-properties&#34;&gt;Linear Filter Properties&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt; means that filtering an image with two (different) filters $f1$ and $f2$ results in the same output as filtering the images separately first before summing the intermediate outputs.
$$ imfilter(I, f1 + f2) = imfilter(I,f1) + imfilter(I,f2) $$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Translation invariance&lt;/strong&gt; means that shifting the original image before filtering results in the same output as filtering then shifting.
$$imfilter(I,shift(f)) = shift(imfilter(I,f))$$&lt;/li&gt;
&lt;li&gt;Any linear, shift-invariant operator can be represented as a convolution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;separability&#34;&gt;Separability&lt;/h3&gt;
&lt;p&gt;We can use an outer product to decompose the 2D filter into two 1D filters, where one is represented as a row column and the other a row vector.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/separability.jpg&#34;
	width=&#34;606&#34;
	height=&#34;366&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/separability_hu92cc13f735780ee724c3b009326abe22_53002_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/separability_hu92cc13f735780ee724c3b009326abe22_53002_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;397px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Note: Convolution vs filtering doesn’t matter here because the filter is &lt;strong&gt;symmetric&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Assume we have $M * N$ image, $P*Q$ filter&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;2D convolution: $MNPQ$ multiply-adds&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Separable 2D: $MN(P+Q)$ multiply-adds&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speed up = $\frac{PQ}{(P+Q)}$, e.g. 9*9 filter = 4.5 times faster&lt;/p&gt;
&lt;p&gt;In Gaussian Filters, a 2D Gaussian can be expressed as the product of two functions, a function of $x$ and a function of $y$.&lt;/p&gt;
&lt;p&gt;$$ G_{\sigma}=\frac{1}{2 \pi \sigma^{2}} e^{-\frac{\left(x^{2}+y^{2}\right)}{2 \sigma^{2}}} =  \left(\frac{1}{2 \pi \sigma^{2}} e^{-\frac{x^{2}}{2 \sigma^{2}}}\right)\left(\frac{1}{2 \pi \sigma^{2}} e^{-\frac{y^{2}}{2 \sigma^{2}}}\right)$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;applications-of-filters&#34;&gt;Applications of Filters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Template matching (SSD or Normxcorr2)
&lt;ul&gt;
&lt;li&gt;SSD can be done with linear filters, is sensitive to overall intensity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gaussian pyramid
&lt;ul&gt;
&lt;li&gt;Coarse-to-fine search, multi-scale detection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Laplacian pyramid
&lt;ul&gt;
&lt;li&gt;Teases apart different frequency bands while keeping spatial information&lt;/li&gt;
&lt;li&gt;Can be used for compositing in graphics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Downsampling
&lt;ul&gt;
&lt;li&gt;Need to sufficiently low-pass before downsampling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sampling&#34;&gt;Sampling&lt;/h2&gt;
&lt;h3 id=&#34;image-pyramid&#34;&gt;Image Pyramid&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/IP.jpg&#34;
	width=&#34;734&#34;
	height=&#34;365&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/IP_hu2a054bd143f1972ea72f966955bad124_213879_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/IP_hu2a054bd143f1972ea72f966955bad124_213879_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;482px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;aliasing&#34;&gt;Aliasing&lt;/h3&gt;
&lt;p&gt;Aliasing occurs when we sample a signal at a frequency that is too low such that we can’t properly reconstruct the original signal. In the image below, black points are samples, but we may get a different-looking reconstruction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/aliasing.jpg&#34;
	width=&#34;567&#34;
	height=&#34;143&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/aliasing_hu965fb104dfd6a7e46fe138c1e4e1232d_16542_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/aliasing_hu965fb104dfd6a7e46fe138c1e4e1232d_16542_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;396&#34;
		data-flex-basis=&#34;951px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;When two signals become indistinguishable from one another due to sampling, they are ‘aliases’ of one another.&lt;/p&gt;
&lt;p&gt;In real world, &lt;strong&gt;temporal and spatial aliasing&lt;/strong&gt; often happens when the sampling rate is not high enough.&lt;/p&gt;
&lt;h3 id=&#34;nyquist-shannon-sampling-theorem&#34;&gt;Nyquist-Shannon Sampling Theorem&lt;/h3&gt;
&lt;p&gt;When sampling a signal at discrete intervals, the sampling frequency must be $\ge  2 f_{max}$, max frequency of the input signal, to guarantee a perfect reconstruction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/NSST.jpg&#34;
	width=&#34;499&#34;
	height=&#34;205&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/NSST_hu7e0fa4cbda7f4a093a012710419ba7ef_35930_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/NSST_hu7e0fa4cbda7f4a093a012710419ba7ef_35930_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;584px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;anti-aliasing&#34;&gt;Anti-aliasing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sampling more often&lt;/li&gt;
&lt;li&gt;Removing all frequencies that are greater than half the new sampling frequency
&lt;ul&gt;
&lt;li&gt;Remove high frequencies with a &lt;strong&gt;low pass filter&lt;/strong&gt; (e.g. Gaussian Filter)&lt;/li&gt;
&lt;li&gt;It will lose info&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Intro to CV and Images</title>
        <link>https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/</link>
        <pubDate>Fri, 27 Jan 2023 00:16:47 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Jan 27th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Jan 27th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-computer-vision&#34;&gt;What is Computer Vision&lt;/h2&gt;
&lt;h3 id=&#34;3r-concept&#34;&gt;3R Concept&lt;/h3&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://people.eecs.berkeley.edu/~malik/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jitendra Malik @ UC Berkeley&lt;/a&gt; has stated that the classic problems of computational vision are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;econstruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;ecognition&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;e-organization&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cv--nearby-fields&#34;&gt;CV &amp;amp; Nearby Fields&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/CVCG.jpg&#34;
	width=&#34;679&#34;
	height=&#34;299&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/CVCG_hu9bac4c580a6d0d1aba38545f734c20b0_35515_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/CVCG_hu9bac4c580a6d0d1aba38545f734c20b0_35515_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;227&#34;
		data-flex-basis=&#34;545px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-an-image&#34;&gt;What is an Image&lt;/h2&gt;
&lt;h3 id=&#34;signal&#34;&gt;Signal&lt;/h3&gt;
&lt;p&gt;Signal is a (multi-dimensional) function that contains information about a &lt;strong&gt;phenomenon&lt;/strong&gt; – light, heat, gravity, population distribution, etc.&lt;/p&gt;
&lt;p&gt;Light reflected from an object creates a continuous signal that is measured by cameras. Natural signals are &lt;strong&gt;continuous&lt;/strong&gt;, but our measurements of them are &lt;strong&gt;discrete&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;sampling&#34;&gt;Sampling&lt;/h3&gt;
&lt;p&gt;It is a process of reduction of continuous signal to a discrete signal.&lt;/p&gt;
&lt;p&gt;Sampling in 1D takes a function and returns a &lt;strong&gt;vector&lt;/strong&gt; whose elements are values of that function at the sample points, while Sampling in 2D takes a function and returns a &lt;strong&gt;matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/2D.jpg&#34;
	width=&#34;553&#34;
	height=&#34;485&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/2D_hu801390394efc7bef26f8a8b90f97aa6c_27441_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/2D_hu801390394efc7bef26f8a8b90f97aa6c_27441_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;114&#34;
		data-flex-basis=&#34;273px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A 2D image is a sampling of a 2D signal.&lt;/strong&gt; Note that the 2D signal can also be a projection (or slice) of a higher-dimensional signal like in MRI or CT scans. An image stores &lt;strong&gt;brightness/intensity&lt;/strong&gt; along $x$ and $y$ dimensions, while a video contains time-varying 2D signals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/intensity.jpg&#34;
	width=&#34;628&#34;
	height=&#34;382&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/intensity_hu57e23c9222c56aafa02b2eac14781631_55530_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/intensity_hu57e23c9222c56aafa02b2eac14781631_55530_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;164&#34;
		data-flex-basis=&#34;394px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;pixel&#34;&gt;Pixel&lt;/h3&gt;
&lt;p&gt;Pixel stands for &lt;strong&gt;picture element&lt;/strong&gt; and each associated with a value. We can approximate a pixel as a &lt;strong&gt;square frustum&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/frustum.jpg&#34;
	width=&#34;740&#34;
	height=&#34;393&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/frustum_hub96a13c44519ffe5d551985eb6a14ed3_43490_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/frustum_hub96a13c44519ffe5d551985eb6a14ed3_43490_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;451px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The diagram above arguably has an error (for convenience). The scene element should be flipped as projection onto the camera plane typically turns the image &lt;strong&gt;upside down&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;quantization&#34;&gt;Quantization&lt;/h3&gt;
&lt;p&gt;The function itself is continuous over the amount of light. When we convert it into a digital image, we need to take a continuous signal and turn it into a discrete set of intensity values to store in our image.&lt;/p&gt;
&lt;p&gt;After quantization, the original signal cannot be reconstructed anymore. This is in contrast to sampling, as a sampled but not quantized signal can be reconstructed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/quantization.jpg&#34;
	width=&#34;700&#34;
	height=&#34;279&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/quantization_hua4ea65834e375aacb0dc4b99bb2b29fd_25716_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/quantization_hua4ea65834e375aacb0dc4b99bb2b29fd_25716_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;250&#34;
		data-flex-basis=&#34;602px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quantization Effects – Radiometric Resolution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We often call this &lt;strong&gt;bit depth&lt;/strong&gt;. For photography, this is also related to &lt;strong&gt;dynamic range&lt;/strong&gt;, the minimum and maximum range of light intensity that can be measured/perceived/represented/displayed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/rr.jpg&#34;
	width=&#34;677&#34;
	height=&#34;222&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/rr_huc5828ede93c608458b752f0abf27a952_43514_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/rr_huc5828ede93c608458b752f0abf27a952_43514_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;304&#34;
		data-flex-basis=&#34;731px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;color&#34;&gt;Color&lt;/h3&gt;
&lt;p&gt;We handle color by having three arrays, one for each of &lt;strong&gt;red&lt;/strong&gt;, &lt;strong&gt;green&lt;/strong&gt; or &lt;strong&gt;blue&lt;/strong&gt; color channels. Combining the channels gives a color image. Practical matters when dealing with images in Python. We deal with color as an additional dimension in our arrays.&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
