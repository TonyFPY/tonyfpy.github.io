<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Career on Tony Feng</title>
        <link>https://tonyfpy.github.io/tags/career/</link>
        <description>Recent content in Career on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 09 Oct 2022 22:31:17 -0400</lastBuildDate><atom:link href="https://tonyfpy.github.io/tags/career/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Machine Learning] Interview Questions - Part 4</title>
        <link>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-4/</link>
        <pubDate>Sun, 09 Oct 2022 22:31:17 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-4/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 9th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 9th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;The post contains a collection of questions for machine learning interview.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1) Why are small convolution kernels such as 3x3 better than larger ones?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, you can use several smaller kernels to get the same receptive field and capture &lt;strong&gt;more spatial context&lt;/strong&gt; with &lt;strong&gt;less parameters and computations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Secondly, because with smaller kernels, you will be using more filters. You&amp;rsquo;ll be able to use more activation functions and thus have a &lt;strong&gt;more discriminative mapping function being learnt&lt;/strong&gt; by your CNN.&lt;/p&gt;
&lt;p&gt;Also, small kernels would lead to &lt;strong&gt;slow reduction of image dimensions&lt;/strong&gt; making the network deep, whereas large kernels would decrease the size of the image really fast.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) The difference for dropout between training and testing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dropout is a simple way to prevent a neural network from &lt;strong&gt;overfitting&lt;/strong&gt; by making neurons output &amp;lsquo;wrong&amp;rsquo; values on purpose. It is a random process of disabling neurons in a layer with &lt;strong&gt;probability &lt;em&gt;p&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At test time, however, we do not apply dropout with the test data. But that means your neurons will receive more connections and therefore more activations during inference than what they received during training. For example, if you use a dropout rate of 50% dropping two out of four neurons in a layer during training, the neurons in the next layer will receive twice the activations during inference and thus become overexcited. Accordingly, the values produced by these neurons will be too large by 50%. To correct this over-activation at inference time, you multiply the weights of the overexcited neurons by the &lt;strong&gt;retention probability (1 – &lt;em&gt;p&lt;/em&gt;)&lt;/strong&gt; to scale them down.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3) How does inference time vary with the depth of the decision tree?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Test time complexity would be &lt;strong&gt;O(depth)&lt;/strong&gt; since we have to move from root to a leaf node of the decision tree.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4) With the increase of depth, what will be influnced in decison tree prediction?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More inference time&lt;/li&gt;
&lt;li&gt;To some degree, it reduces bias and prevent underfitting, but it may lead to overfitting (more leaf nodes).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;5) Pros and Cons of decision tree&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Once the tree is constructed, the training data does not need to be stored. Instead, we can simply store &lt;strong&gt;splitting conditions&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Inference is very &lt;strong&gt;fast&lt;/strong&gt;, as test inputs simply need to traverse down the tree to a leaf.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No metrics&lt;/strong&gt; are needed, because the splits are based on feature thresholds.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disavantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trees fail to deal with &lt;strong&gt;linear relationships&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Trees are quite &lt;strong&gt;unstable&lt;/strong&gt;. A few changes in the training dataset can create a completely different tree.&lt;/li&gt;
&lt;li&gt;The number of terminal nodes increases quickly with depth.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;6) What is the purpose of random restarts?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Random restart means restarting at a &lt;strong&gt;new random state&lt;/strong&gt; after a pre-defined
number of steps, which can turn a local search algorithm into an algorithm with &lt;strong&gt;global search capability&lt;/strong&gt;. It helps in &lt;strong&gt;non-convex optimization&lt;/strong&gt; to alleviate the problems of trapping in many local minima or flat regions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7) What is bias &amp;amp; variance decomposition?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8) Differences between gradient-boosting decision tree and random forest?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9) What&amp;rsquo;s the range of cosine distance?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10) Differences between online training and offline training?&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/andrewekhalel/MLQuestions&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ML Interview Questions collected by andrewekhalel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Interview Questions - Part 3</title>
        <link>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-3/</link>
        <pubDate>Sat, 08 Oct 2022 20:35:56 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-3/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 8th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 8th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;The post contains a collection of questions for machine learning interview.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1) What’s the difference between Type I and Type II error?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Type I error is a &lt;strong&gt;false positive&lt;/strong&gt;, while Type II error is a &lt;strong&gt;false negative&lt;/strong&gt;. Briefly stated, Type I error means claiming something has happened when it hasn’t, while Type II error means that you claim nothing is happening when in fact something is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) Generative model vs. Discriminative model&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A generative model will learn &lt;strong&gt;categories&lt;/strong&gt; of data and draw boundaries in the data space.&lt;/li&gt;
&lt;li&gt;A discriminative model will simply learn the &lt;strong&gt;distinction&lt;/strong&gt; between different categories of data or model how data is placed throughout the space. Discriminative models generally outperform generative models on classification tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3) Instance-Based Learning vs. Model-Based Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instance-based Learning: The system learns the examples by heart, then generalizes to new cases using a &lt;strong&gt;similarity&lt;/strong&gt; measure.&lt;/li&gt;
&lt;li&gt;Model-based Learning: The system generalizes from a set of examples by building a &lt;strong&gt;model&lt;/strong&gt; of these examples, then use that model to make predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4) Label Encoding vs. One-Hot Encoding?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This question generally depends on your dataset and the model which you wish to apply.&lt;/p&gt;
&lt;p&gt;One-Hot Encoding simply creates &lt;strong&gt;additional features&lt;/strong&gt; based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature. We apply One-Hot Encoding when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The categorical feature is not ordinal.&lt;/li&gt;
&lt;li&gt;The number of categorical features is less so one-hot encoding can be effectively applied.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Label Encoding means each label is assigned a &lt;strong&gt;unique integer based on alphabetical ordering&lt;/strong&gt;. We apply Label Encoding when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The categorical feature is ordinal.&lt;/li&gt;
&lt;li&gt;The number of categories is quite large, which may lead to high memory consumption.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;5) LDA vs. PCA for dimensionality reduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Both LDA and PCA are &lt;strong&gt;linear transformation&lt;/strong&gt; techniques: LDA is a &lt;strong&gt;supervised&lt;/strong&gt; whereas PCA is &lt;strong&gt;unsupervised&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PCA tries to maximize the data&amp;rsquo;s variability while reducing the dataset&amp;rsquo;s dimensionality.&lt;/li&gt;
&lt;li&gt;LDA finds the linear discriminants in order to maximize the variance between the different classes while minimizing the variance within the class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;6) How does Content-based image retrieval work?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;CBIR is the concept of using images to gather metadata on their content. Compared to the current image retrieval approach based on the keywords associated to the images, this technique generates its &lt;strong&gt;metadata&lt;/strong&gt; from computer vision techniques to extract the relevant informations that will be used for &lt;strong&gt;queries&lt;/strong&gt;. Many approach are possible from feature detection to retrieve keywords to the usage of CNN to extract &lt;strong&gt;dense features&lt;/strong&gt; that will be associated to &lt;strong&gt;a known distribution of keywords&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We care less about what is shown on the image but more about the similarity between the metadata generated by a known image and a list of known labels projected into this metadata space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7)  Why do we use convolutions for images rather than just FC layers?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Firstly, convolutions preserve, encode, and actually use the &lt;strong&gt;spatial information&lt;/strong&gt; from the image. If we used only FC layers we would have no relative spatial information.&lt;/p&gt;
&lt;p&gt;Secondly, CNNs have a partially built-in &lt;strong&gt;translation in-variance&lt;/strong&gt;, since we&amp;rsquo;re going to apply the convolution in a sliding window fashion across the entire image anyways.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8) Why do we have max-pooling in classification CNNs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Max-pooling in a CNN allows you to &lt;strong&gt;reduce computation&lt;/strong&gt; since your feature maps are smaller after the pooling. You don&amp;rsquo;t lose too much semantic information since you&amp;rsquo;re taking the maximum activation. There&amp;rsquo;s also a theory that max-pooling contributes a bit to giving CNNs more &lt;strong&gt;translation in-variance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9) What is the significance of Residual Networks?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The skip connections in ResNet solve the problem of vanishing gradient in deep neural networks by allowing for &lt;strong&gt;direct feature access from previous layers&lt;/strong&gt;. The other way that these connections help is by allowing the model to learn the identity functions which ensures that the &lt;strong&gt;higher layer will perform at least as good as the lower layer&lt;/strong&gt;, and not worse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10) What is batch normalization?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Training Deep Neural Networks is complicated by the fact that &lt;strong&gt;the distribution of each layer&amp;rsquo;s inputs changes&lt;/strong&gt; during training, as the parameters of the previous layers change. A network is just a series of layers, where &lt;strong&gt;the output of one layer becomes the input to the next&lt;/strong&gt;. The idea is then to normalize the inputs of each layer in such a way that they have &lt;strong&gt;a mean output activation of 0&lt;/strong&gt; and &lt;strong&gt;standard deviation of 1&lt;/strong&gt;. This is analogous to how the inputs to networks are standardized.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/andrewekhalel/MLQuestions&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ML Interview Questions collected by andrewekhalel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Interview Questions - Part 2</title>
        <link>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-2/</link>
        <pubDate>Fri, 07 Oct 2022 17:05:02 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-2/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 7th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 7th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;The post contains a collection of questions for machine learning interview.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1) Why do ensembles typically have higher scores than individual models?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An ensemble is the combination of &lt;strong&gt;multiple models&lt;/strong&gt; to create a &lt;strong&gt;single prediction&lt;/strong&gt;. The key idea is that the errors of one model will be compensated by the right predictions of the other models and thus the score of the ensemble will be higher.&lt;/p&gt;
&lt;p&gt;We need diverse models for creating an ensemble.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using different ML algorithms.&lt;/li&gt;
&lt;li&gt;Using different subsets of the data for training. (Bagging)&lt;/li&gt;
&lt;li&gt;Giving a different weight to each of the samples of the training set. (Boosting)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Engineers need to find a balance between &lt;strong&gt;execution time&lt;/strong&gt; and &lt;strong&gt;accuracy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) What is an imbalanced dataset? Can you list some ways to deal with it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An imbalanced dataset has different proportions of categories. There are different options to deal with imbalanced datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resampling the dataset: undersampling majority classes and oversampling minority classes&lt;/li&gt;
&lt;li&gt;Data Augmentation&lt;/li&gt;
&lt;li&gt;Cross Validation&lt;/li&gt;
&lt;li&gt;Choose a right algorithm, e.g., random forest&lt;/li&gt;
&lt;li&gt;Using appropriate metrics, e.g., F-score, Confusion Matrix, which describe the performance of the model better on an imbalanced dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3) What is data augmentation? Can you give some examples?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data augmentation is a technique for generating new data by modifying existing data in such a way that the target is not changed, or it is changed in a known way.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4) Explain how the AUC - ROC curve works&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ROC or Receiver Operating Characteristic curve represents a graphical representation of the contrast between &lt;strong&gt;true positive rates and the false positive rate&lt;/strong&gt; at various thresholds.&lt;/p&gt;
&lt;p&gt;AUC is known for Area Under the ROC curve. It calculates the two-dimensional area under the entire ROC curve ranging from 0 to 1, which means an excellent model will have AUC near 1, and hence it will show a good measure of &lt;strong&gt;separability&lt;/strong&gt;. AUC is &lt;strong&gt;scale-invariant&lt;/strong&gt;. It measures &lt;strong&gt;how well predictions are ranked&lt;/strong&gt;, rather than their absolute values. AUC is &lt;strong&gt;classification-threshold-invariant&lt;/strong&gt;. It measures the quality of the model&amp;rsquo;s predictions irrespective of what classification threshold is chosen.&lt;/p&gt;
&lt;p&gt;AUC is not preferable when we need to calibrate probability output. Further, AUC is not a useful metric when there are wide disparities in the cost of false negatives vs false positives, and it is difficult to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives even if that results in a significant increase of false negatives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5) What is Precision/Recall/F1-score?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Precision (positive predictive value) is the fraction of relevant instances among the retrieved instances.&lt;/p&gt;
&lt;p&gt;$$ Precision = \frac{TP}{TP + FP} $$&lt;/p&gt;
&lt;p&gt;Recall (sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.&lt;/p&gt;
&lt;p&gt;$$ Recall = \frac{TP}{TP + FN} $$&lt;/p&gt;
&lt;p&gt;F1-score is the weighted average of precision and recall. It considers both false positive and false negative into account.&lt;/p&gt;
&lt;p&gt;$$ F1 = \frac{2PR}{P + R} $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6) Define Learning Rate&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The learning rate is a hyper-parameter used to govern the pace at which an algorithm updates or learns the values of a parameter estimate. In other words, it controls how much we are adjusting the weights of our network with respect the loss gradient.&lt;/p&gt;
&lt;p&gt;When training loss fluctuates, we may reduce the learning rate. Otherwise, SGD jumps too far and misses the area near local minima.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7) The differences between Batch Gradient Descent and Stochastic Gradient Descent&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Batch Gradient Descent involves calculations over the &lt;strong&gt;whole training set&lt;/strong&gt; at each step as a result of which it is very slow on very large training data. Thus, it becomes very &lt;strong&gt;computationally expensive&lt;/strong&gt;. However, this is great for &lt;strong&gt;convex&lt;/strong&gt; or relatively smooth error manifolds. Also, Batch GD scales well with the number of features.&lt;/p&gt;
&lt;p&gt;Stochastic gradient descent (SGD) picks up a &lt;strong&gt;&amp;ldquo;random&amp;rdquo; instance&lt;/strong&gt; of training data at each step and then computes the gradient, making it much faster to reach the &lt;strong&gt;convergence&lt;/strong&gt;. Also, it can &lt;strong&gt;escape shallow local minima&lt;/strong&gt; more easily.&lt;/p&gt;
&lt;p&gt;Batch GD: Batch size = Size of training set&lt;br&gt;
Stochastic GD: Batch size = 1&lt;br&gt;
Mini-Batch GD: 1 &amp;lt; Batch size &amp;lt; Size of training set&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8) Epoch vs. Batch vs. Iteration&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Epoch: It&amp;rsquo;s the number of times the whole training dataset is passed through the model. &lt;br&gt;
Batch: It&amp;rsquo;s the number of examples processed together in one pass.&lt;br&gt;
Iteration: It&amp;rsquo;s the number of batches required to complete one epoch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9) What is gradient vanishing? What is gradient explosion?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As we add more and more hidden layers, back propagation becomes less and less useful in passing information to the lower layers. &lt;strong&gt;A large change in the input will cause a small change in the output.&lt;/strong&gt; In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the networks. On the contrary, if the gradients get larger or even NaN as our backpropagation progresses, &lt;strong&gt;the error gradients accumulate&lt;/strong&gt; and end up with exploding gradients having big weight updates.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;sigmoid&lt;/strong&gt;, it saturates at zero for large negative values and at one for large positive values. The same applies to the &lt;strong&gt;Tanh&lt;/strong&gt; function that saturates at -1 and 1. &lt;strong&gt;ReLU&lt;/strong&gt; returns the input if the input value is positive, and it returns 0 if the input is negative.&lt;/p&gt;
&lt;p&gt;ReLu can solve gradient vanishing but may cause gradient explosion. &lt;strong&gt;The output of ReLU is unbounded in the positive domain by design.&lt;/strong&gt; This means that the output can, in some situations, continue to grow in size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10) What&amp;rsquo;s the difference between boosting and bagging?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Boosting and bagging are similar, in that they are both ensembling techniques, where &lt;strong&gt;multiple weak learners&lt;/strong&gt; (classifiers/regressors that are barely better than guessing) combine (through averaging or max vote) to form &lt;strong&gt;a single strong learner&lt;/strong&gt; that can make accurate predictions.&lt;/p&gt;
&lt;p&gt;Bagging means that you take &lt;strong&gt;bootstrap&lt;/strong&gt; samples (with replacement) of your dataset and each sample trains a (potentially) weak learner. Boosting, on the other hand, uses all data to train each learner, but instances that were misclassified by the previous learners are given &lt;strong&gt;more weight&lt;/strong&gt; so that &lt;strong&gt;subsequent&lt;/strong&gt; learners give more focus to them during training.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/andrewekhalel/MLQuestions&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ML Interview Questions collected by andrewekhalel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Interview Questions - Part 1</title>
        <link>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-1/</link>
        <pubDate>Sat, 01 Oct 2022 19:51:42 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-1/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 1st, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 1st, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;The post contains a collection of questions for machine learning interview.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1) Explain bias and variance, and trade-off between them&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bias is the &lt;strong&gt;difference&lt;/strong&gt; between the average prediction of our model and the ground truth.&lt;/p&gt;
&lt;p&gt;Variance refers to the &lt;strong&gt;variability&lt;/strong&gt; in the model prediction. In other words, it reflects the changes in the model when using different portions of the training dataset.&lt;/p&gt;
&lt;p&gt;Bias and variance are &lt;strong&gt;inversely connected&lt;/strong&gt;. An underfitting model has high bias and low variance, while an overfitting model has high variance and low bias. So we need to find the right balance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) What is gradient descent?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GD is an &lt;strong&gt;optimization&lt;/strong&gt; algorithm that finds the values of parameters (coefficients) of a loss function that minimizes a cost function.&lt;/p&gt;
&lt;p&gt;GD is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3) Difference between Loss Function, Cost Function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Loss function is associated with &lt;strong&gt;single training example&lt;/strong&gt;, and the cost function is the average value of the loss function over the &lt;strong&gt;entire training dataset&lt;/strong&gt;. In ML, we usually try to optimize our cost
function rather than loss function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4) How to handle overfitting and underfitting&lt;/strong&gt; &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Handling Overfitting&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Handling Underfitting&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;More Training Data&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Removing Data Noise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Regularization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Reducing Regularization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Reducing Iterations&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Increasing Iterations&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Reducing Features&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Increasing Features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Increasing Learning Rate&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Reducing Learning Rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Other Strategies: Pruning, Dropout, etc.&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Increasing Model Complexity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;5) What is curse of dimensionality? How to prevent it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;CoD basically means that the error increases with the increase in the number of features. It leads to exponential increase in computational efforts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improving the ratio of observations over attributes&lt;/li&gt;
&lt;li&gt;Making distances in feature space more meaningful&lt;/li&gt;
&lt;li&gt;Removing features that have no correlation with the target distribution&lt;/li&gt;
&lt;li&gt;Removing or combining features that have redundant correlation with target distribution&lt;/li&gt;
&lt;li&gt;Extracting new features with a more direct correlation with target distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;6) What is regularization, and give some examples of common methods?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Regularization refers to techniques that are used to calibrate machine learning models so as to &lt;strong&gt;avoid the risk of overfitting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The obvious disadvantage of ridge is model interpretability. It will shrink the coefficients for least important features, &lt;strong&gt;only close to zero&lt;/strong&gt;. However, for lasso, it can force some of the coefficient estimates to &lt;strong&gt;be zero&lt;/strong&gt; when the weight is sufficiently large. Therefore, the lasso method also performs variable selection and is said to yield sparse models.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;L1 norm (Lasso)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;L2 norm (Ridge)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Penalizes the sum of absolute values of weights&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Penalizeshe the sum of squares of weights&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Sparse solution&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Non-sparse solution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Feature selection&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No feature selection&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Robust to outliers&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Not robust to outliers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Unable to learn complex data patterns&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Able to learn complex data patterns&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;7) Explain Principal Component Analysis (PCA)?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PCA is used for the &lt;strong&gt;dimensionality reduction&lt;/strong&gt;, which tries to find the lower-dimensional surface to project the high-dimensional data.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://builtin.com/data-science/step-step-explanation-principal-component-analysis&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Common Steps&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standardizing the range of continuous initial variables&lt;/li&gt;
&lt;li&gt;Computing the covariance matrix to identify correlations&lt;/li&gt;
&lt;li&gt;Computing the eigenvectors and eigenvalues of the covariance matrix to identify the principal components&lt;/li&gt;
&lt;li&gt;Creating a feature vector to decide which principal components to keep&lt;/li&gt;
&lt;li&gt;Recasting the data along the principal components axes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;8) What is data normalization and why do we need it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data normalization is a &lt;strong&gt;preprocessing&lt;/strong&gt; step to &lt;strong&gt;re-scale values&lt;/strong&gt; to fit in a specific range to assure better convergence. In general, it boils down to subtracting the &lt;strong&gt;mean&lt;/strong&gt; of each data point and dividing by its &lt;strong&gt;standard deviation&lt;/strong&gt;. The data normalization makes all features weighted equally. Otherwise, some features with high magnitude will be weighted more in the cost function, while other features with lower values will be allocated less weights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9) What is the difference between training, validation set and test set?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The training dataset is used for &lt;strong&gt;fitting the model’s parameters&lt;/strong&gt;. However, the accuracy that we achieve on the training set is not reliable for predicting if the model will be accurate on new samples.&lt;/p&gt;
&lt;p&gt;The validation dataset is used to &lt;strong&gt;measure how well the model does&lt;/strong&gt; on examples that weren’t part of the training dataset and to &lt;strong&gt;provide information&lt;/strong&gt; for adjusting the model. The more evaluations, the more information is leaked. So we can end up overfitting to the validation data.&lt;/p&gt;
&lt;p&gt;The test dataset is used to measure how well the model does on previously unseen examples. It should &lt;strong&gt;only be used once&lt;/strong&gt; we have tuned the parameters using the validation set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10) What is cross-validation?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;CV is a statistical &lt;strong&gt;resampling&lt;/strong&gt; technique that uses different parts of the dataset to train and test an ML algorithm on different iterations. The aim of CV provides the ability to estimate model performance on unseen data.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/andrewekhalel/MLQuestions&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ML Interview Questions collected by andrewekhalel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Career Talk] Topic 1: SDE or DS, How Can I Choose?</title>
        <link>https://tonyfpy.github.io/p/career-talk-topic-1-sde-or-ds-how-can-i-choose/</link>
        <pubDate>Tue, 26 Apr 2022 22:57:49 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/career-talk-topic-1-sde-or-ds-how-can-i-choose/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on April 26th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on April 28th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;Recently, I am preparing for the application of 2023 summer intership in US. Since it might be my last industrial internship, this experience would influence my career path to a large extent and pave the way to my future career development. It&amp;rsquo;s time to conisder it comprehensively.&lt;/p&gt;
&lt;p&gt;I would like to consider myself as a freshman in job hunting, because it&amp;rsquo;s my first time to seek employment in another country. In order to find a desirable job placement, I&amp;rsquo;ve gone through many related materials online. In this post, I&amp;rsquo;m gonna talk about &lt;strong&gt;Software Devlopment(SD)&lt;/strong&gt; and &lt;strong&gt;Data Science(DS)&lt;/strong&gt;, two popular job directions most CS graduates will choose.&lt;/p&gt;
&lt;h2 id=&#34;sd-vs-ds&#34;&gt;SD V.S. DS&lt;/h2&gt;
&lt;h3 id=&#34;number-of-job-opportunities&#34;&gt;Number of Job Opportunities&lt;/h3&gt;
&lt;p&gt;In general, companies offers more job positions for SD than DS evidently. The ratio is about &lt;strong&gt;10 : 1&lt;/strong&gt; in average. SD is &lt;strong&gt;more friendly to new graduates&lt;/strong&gt; than DS in that companies provides more SD placements for them. DS requires the candidates to earn a high education degree or have past working experience.&lt;/p&gt;
&lt;h3 id=&#34;job-categories&#34;&gt;Job Categories&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;SD&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Software Development Engineer&lt;/li&gt;
&lt;li&gt;Application Developer&lt;/li&gt;
&lt;li&gt;Full-stack Web Developer&lt;/li&gt;
&lt;li&gt;Frontend Developer&lt;/li&gt;
&lt;li&gt;Backend Developer&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;DS&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Scientist&lt;/li&gt;
&lt;li&gt;Applied Scientist&lt;/li&gt;
&lt;li&gt;Statistician&lt;/li&gt;
&lt;li&gt;Business Intelligence Engineer&lt;/li&gt;
&lt;li&gt;Data/Product/Business Analyst&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cross Domain&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Machine Learning Engineer&lt;/li&gt;
&lt;li&gt;Deep Learning Engineer&lt;/li&gt;
&lt;li&gt;Data Engineer&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;salary&#34;&gt;Salary&lt;/h3&gt;
&lt;p&gt;The salary may vary depending on your experience, skills, training, certifications and your employer. In general, SD-related professionals are paid higher than DS-related professionals. Reasearch-oriented DS jobs have higher salaries than other kinds of DS jobs, because they are usually demanding and only open for PhDs.&lt;/p&gt;
&lt;h3 id=&#34;degree-requirements&#34;&gt;Degree Requirements&lt;/h3&gt;
&lt;p&gt;SD: Bachlor&amp;rsquo;s degree or higher &lt;br/&gt;
DS: Master&amp;rsquo;s degree or higher. Many positions even require a Doctoral degree. &lt;br/&gt;&lt;/p&gt;
&lt;h3 id=&#34;hard-skills&#34;&gt;Hard Skills&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;SD&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Algorithm and Data Structure&lt;/li&gt;
&lt;li&gt;Programming Paradigms&lt;/li&gt;
&lt;li&gt;System Design&lt;/li&gt;
&lt;li&gt;Testing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;DS&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Statistics and Machien Learning&lt;/li&gt;
&lt;li&gt;Data Manipulation and Modeling&lt;/li&gt;
&lt;li&gt;Data Visualization&lt;/li&gt;
&lt;li&gt;Experiemnts Design and Analysis&lt;/li&gt;
&lt;li&gt;Business Case&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;soft-skills&#34;&gt;Soft Skills&lt;/h3&gt;
&lt;p&gt;Common soft skills are indispensable for SD and DS in your long-term career development.&lt;/p&gt;
&lt;p&gt;Besides, DS requires the candidates to have &lt;strong&gt;strong communications skills&lt;/strong&gt;, &lt;strong&gt;data-driven decision making&lt;/strong&gt;, &lt;strong&gt;product sense&lt;/strong&gt;, etc.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;job-descriptions&#34;&gt;Job Descriptions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Job responsibilities vary based on employers&amp;rsquo; requirements and should be learned case by case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;sd&#34;&gt;SD&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Full-Stack Engineer&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Front-End + Back-End&lt;/li&gt;
&lt;li&gt;UI + Server + Database Configuration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile App Developer&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Android / IOS&lt;/li&gt;
&lt;li&gt;Memory + Computational Power&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Graphics Engineer&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;2D and 3D Digital Platforms for Gaming and Video Production&lt;/li&gt;
&lt;li&gt;Math + CS&lt;/li&gt;
&lt;li&gt;Unity, OpenGL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embedded Systems Engineer&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Control of machines&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Software Development Engineer in Test(SDET)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Dev + Automated Testing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DevOps Engineer&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Dev + Operations + Deployment&lt;/li&gt;
&lt;li&gt;Network or Sys Admin&lt;/li&gt;
&lt;li&gt;Source Control / Infrastructure Automation / Cloud&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ds&#34;&gt;DS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Analysts&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Datasets -&amp;gt; Analysis -&amp;gt; Visulization(Reports / Resentations / &amp;hellip;)&lt;/li&gt;
&lt;li&gt;Experiment (A/B testing) + Statistic + SQL / R / Python&lt;/li&gt;
&lt;li&gt;Product Interpretation + Actionable Insights + Communication&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Scientist&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Data Manipulation + Statistical Modeling + Machine Learning&lt;/li&gt;
&lt;li&gt;Product Improvement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Applied Scientist / Research Scientist&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Independent Research + Literature Review&lt;/li&gt;
&lt;li&gt;Model Design + Implementation + Optimization&lt;/li&gt;
&lt;li&gt;Long-term Research&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quant Researcher&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Finance + Statistics + Coding&lt;/li&gt;
&lt;li&gt;Time Series&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cross-domain&#34;&gt;Cross Domain&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Engineer&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Data Pipeline&lt;/li&gt;
&lt;li&gt;Data Infrastructure&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning Engineer&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Mathematics + Statistics + Probability&lt;/li&gt;
&lt;li&gt;Data Modeling and Evaluation&lt;/li&gt;
&lt;li&gt;ML System Design&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;www.xiaogeedu.com&#34; &gt;SDE and DS 的求职难度和就业现状&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ph-education.com/%e7%95%99%e5%ad%a6%e7%94%b3%e8%af%b7/%e5%b9%b2%e8%b4%a7%e7%bb%8f%e9%aa%8c%e5%90%88%e9%9b%86/%e5%b9%b2%e8%b4%a7-%e4%b8%80%e4%b8%aa%e5%85%b8%e5%9e%8b%e5%8c%97%e7%be%8eds-master%e7%9a%84%e6%b1%82%e8%81%8c%e5%85%a8%e8%bf%87%e7%a8%8b%e5%9b%9e%e9%a1%be/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;一个典型北美DS master的求职全过程回顾&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Sf4y1p79r?spm_id_from=333.880.my_history.page.click&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;疫情下我是如何拿到FB DS offer 的？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.coursera.org/introduction-data-science-careers/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An introduction to data science careers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://intellipaat.com/blog/data-science-vs-software-engineering/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Data Science vs Software Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.dataapplab.com/differences-between-ds-da-ba-and-de-and-key-points-of-job-interview/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;数据岗位大合集｜DS、DA、BA和DE的区别及求职面试重点&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.1point3acres.com/bbs/thread-850531-1-1.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;你为什么不该选择DS - 可能是2022年最详细的劝退贴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.1point3acres.com/bbs/thread-601652-1-1.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Data Science相关岗位全面解析(DS vs DA vs MLE vs DE)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.1point3acres.com/bbs/thread-885801-1-1.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DS找工回顾及资料总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://analyticsindiamag.com/a-complete-guide-to-data-science-career-path-by-great-learning-aim/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Complete Guide To Data Science Career Path – By Great Learning &amp;amp; AIM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.thinkful.com/blog/software-engineer-career-path/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Software Engineer Career Path&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.geeksforgeeks.org/career-paths-for-software-developers-and-programmers-in-2019/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Career Paths For Software Developers and Programmers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
