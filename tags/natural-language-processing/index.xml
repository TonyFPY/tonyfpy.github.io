<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Natural Language Processing on Tony Feng</title>
        <link>https://tonyfpy.github.io/tags/natural-language-processing/</link>
        <description>Recent content in Natural Language Processing on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Thu, 22 Sep 2022 14:43:26 -0400</lastBuildDate><atom:link href="https://tonyfpy.github.io/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Comp Linguistics] Tokenization</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-tokenization/</link>
        <pubDate>Thu, 22 Sep 2022 14:43:26 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-tokenization/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 22nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 22nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;morphology&#34;&gt;Morphology&lt;/h2&gt;
&lt;p&gt;Why can&amp;rsquo;t we just split on white space?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compounding&lt;/li&gt;
&lt;li&gt;Many language systems don&amp;rsquo;t use white space&lt;/li&gt;
&lt;li&gt;Clitics&lt;/li&gt;
&lt;li&gt;Words formation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;finite-state-machines&#34;&gt;Finite State Machines&lt;/h2&gt;
&lt;p&gt;This is the main &lt;strong&gt;rule-based&lt;/strong&gt; computational tool for morphological parsing in NLP.&lt;/p&gt;
&lt;h3 id=&#34;finite-state-automata-fsa&#34;&gt;Finite State Automata (FSA)&lt;/h3&gt;
&lt;p&gt;It is used to check whether to &amp;lsquo;accept&amp;rsquo; a string.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Formal Definition&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A finite set of states: $ \mathrm{Q}={q_{0}, q_{1}, \ldots q_{N-1} } $&lt;/li&gt;
&lt;li&gt;A finite set of symbols (alphabet): $ \Sigma={i_{0}, i_{1}, \ldots i_{{M}-1}} $&lt;/li&gt;
&lt;li&gt;A start state &amp;amp; Final States&lt;/li&gt;
&lt;li&gt;A transition function that returns the new state, given a state and a symbol: $ \delta(q, i) $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Psuedocode for Implementation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;recognize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;machine&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Boolean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;loop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reach&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;finalState&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Non-Determinism&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple links out of the same node&lt;/li&gt;
&lt;li&gt;Epsilon transitions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-tokenization/fsa.jpg&#34;
	width=&#34;1200&#34;
	height=&#34;676&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-tokenization/fsa_hu5392257e4780065f174edec241aef7f5_83429_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-tokenization/fsa_hu5392257e4780065f174edec241aef7f5_83429_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;An example of FSA&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;finite-state-transducer-fst&#34;&gt;Finite State Transducer (FST)&lt;/h3&gt;
&lt;p&gt;It is used to &amp;rsquo;translate&amp;rsquo; one string into another, which produces a parse as output.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-tokenization/fst.jpg&#34;
	width=&#34;1216&#34;
	height=&#34;664&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-tokenization/fst_hud3b660378d7f13fa5005b150d28c6d7d_68404_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-tokenization/fst_hud3b660378d7f13fa5005b150d28c6d7d_68404_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;An example of FST&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;439px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;subword-tokenization&#34;&gt;Subword Tokenization&lt;/h2&gt;
&lt;p&gt;Subword Tokenization is a way to algorithmically identify common sub-sequences of characters and merge them into atomic units.&lt;/p&gt;
&lt;h3 id=&#34;problems&#34;&gt;Problems&lt;/h3&gt;
&lt;p&gt;Tokenizers result in many unknown words (OOVs), and morphological analysis can help, but:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Running morphological analyzers is expensive and language
specific&lt;/li&gt;
&lt;li&gt;Rare words might be “guessable” from cues&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;byte-pair-encoding&#34;&gt;Byte Pair Encoding&lt;/h3&gt;
&lt;p&gt;Steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Break words into characters&lt;/li&gt;
&lt;li&gt;Define a number of iterations to run&lt;/li&gt;
&lt;li&gt;Count the frequency of all pairs of symbols&lt;/li&gt;
&lt;li&gt;Merge the most frequent pair of characters, treating them as a new token&lt;/li&gt;
&lt;li&gt;Repeat&lt;/li&gt;
&lt;li&gt;Tp parse new inputs, just apply merge in order&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/course/chapter6/5?fw=pt#bytepair-encoding-tokenization&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Byte-Pair Encoding tokenization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Byte-Pair Encoding: Subword-based tokenization algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Text Classifier: Features</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/</link>
        <pubDate>Tue, 20 Sep 2022 14:35:51 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 20th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 20th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;data-pre-processing&#34;&gt;Data Pre-processing&lt;/h2&gt;
&lt;h3 id=&#34;preprocessing-vs-features&#34;&gt;Preprocessing VS. Features&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;: Defining the vocabulary&lt;br&gt;
&lt;strong&gt;Features&lt;/strong&gt;: Capturing the aspects of the language that is independent&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Differences in steps&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Load the data&lt;/li&gt;
&lt;li&gt;Preprocess the data&lt;/li&gt;
&lt;li&gt;Extract the features&lt;/li&gt;
&lt;li&gt;Split the data&lt;/li&gt;
&lt;li&gt;Train and test the model&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;preprocessing-steps&#34;&gt;Preprocessing Steps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Strip boilerplate/html/etc&lt;/li&gt;
&lt;li&gt;Tokenization&lt;/li&gt;
&lt;li&gt;Stemming/Lemmatization&lt;/li&gt;
&lt;li&gt;Lowercasing&lt;/li&gt;
&lt;li&gt;Removing punctuation, stop words, numbers, etc.&lt;/li&gt;
&lt;li&gt;Setting a Vocab Size/Defining out-of-vocabulary (OOV)&lt;/li&gt;
&lt;li&gt;Word Sense Disambiguation (one word with multiple meanings)&lt;/li&gt;
&lt;li&gt;Combining Synonyms/Paraphrases&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h2&gt;
&lt;h3 id=&#34;weighting-strategies&#34;&gt;Weighting Strategies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Basic BOW&lt;/strong&gt;: 0 and 1 for appearance&lt;br&gt;
&lt;strong&gt;Count&lt;/strong&gt;: Frequency in a document&lt;br&gt;
&lt;strong&gt;TF-IDF&lt;/strong&gt;: Weighting schemes for important words&lt;/p&gt;
&lt;p&gt;TF-IDF is about assigning higher weights to words that differentiate this document from other documents. More info could be find &lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;TF(word, doc) = # of occurrences of the word in the doc / # of words in the doc&lt;br&gt;
IDF(word) = log(# of all docs / # of docs that contain the word)&lt;br&gt;
TF-IDF(word, doc) = TF(word, doc) * IDF(word)&lt;/p&gt;
&lt;p&gt;You could try TF-IDF Demo &lt;a class=&#34;link&#34; href=&#34;https://remykarem.github.io/tfidf-demo/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;n-grams&#34;&gt;N-Grams&lt;/h3&gt;
&lt;p&gt;N-gram is a sequence of words of lenght N, such as unigram, bigram, trigram, 4-gram, etc. The algorithm uses sliding window with N length to extract features.&lt;/p&gt;
&lt;p&gt;However, it may lead to &lt;strong&gt;large feature space&lt;/strong&gt;. It&amp;rsquo;s a tradeoff between &lt;strong&gt;expressivity&lt;/strong&gt; and &lt;strong&gt;generalization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/2-gram.jpg&#34;
	width=&#34;1186&#34;
	height=&#34;490&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/2-gram_hue813c4cf87e5fd453b898c0a36f70a0d_48757_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/2-gram_hue813c4cf87e5fd453b898c0a36f70a0d_48757_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;An example of bigram&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;242&#34;
		data-flex-basis=&#34;580px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;more-advanced-features&#34;&gt;More Advanced Features&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Syntactic Features&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;People often use dependency parsers to find meaningful links
between &lt;strong&gt;non-adjacent&lt;/strong&gt; words&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Text Classifier: ML Models</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/</link>
        <pubDate>Fri, 16 Sep 2022 14:27:25 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 16th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 16th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-classification&#34;&gt;Supervised Classification&lt;/h2&gt;
&lt;h3 id=&#34;supervised-vs-unsupervised&#34;&gt;Supervised vs. Unsupervised&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Supervised&lt;/strong&gt;: I have lots of sentences. Given words 1…k in a sentence, can I predict word k+1?
&lt;strong&gt;Unsupervised&lt;/strong&gt;: I have a ton of news articles. Can I cluster them into meaningful groups?&lt;/p&gt;
&lt;h3 id=&#34;classification-vs-regression&#34;&gt;Classification vs. Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;: Is the sentiment positive or negative?&lt;br&gt;
&lt;strong&gt;Regression&lt;/strong&gt;: How many likes will this tweet get?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;feature-matrices-and-bow-models&#34;&gt;Feature Matrices and BOW Models&lt;/h2&gt;
&lt;h3 id=&#34;building-feature-matrices&#34;&gt;Building Feature Matrices&lt;/h3&gt;
&lt;p&gt;ML models require input to be represented as &lt;strong&gt;numeric features&lt;/strong&gt;, which could be encoded in a &lt;strong&gt;feature matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bag-of-words-mode&#34;&gt;Bag of Words Mode&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s a model that uses the words as features.  &lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/BoW.jpg&#34;
	width=&#34;1148&#34;
	height=&#34;434&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/BoW_hu3480f5b17b16e182c53fb0fad8e46d92_47253_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/BoW_hu3480f5b17b16e182c53fb0fad8e46d92_47253_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;BoW&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;264&#34;
		data-flex-basis=&#34;634px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No information about order or syntax&lt;/li&gt;
&lt;li&gt;Binary representation&lt;/li&gt;
&lt;li&gt;High Dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;naive-bayes-text-classifier&#34;&gt;Naive Bayes Text Classifier&lt;/h2&gt;
&lt;p&gt;It is based on &lt;strong&gt;Bayes Rule&lt;/strong&gt; to estimate/maximize $P(Y|X)$.
$$ P(Y \mid X)=\frac{P(X,Y)}{P(X)} = \frac{P(X \mid Y) P(Y)}{P(X)} $$&lt;/p&gt;
&lt;p&gt;We can apply &lt;strong&gt;chain rule&lt;/strong&gt; to compute $ P(Y \mid X) = P(Y \mid x_{1}, x_{2}, \ldots, x_{k}) $ if there are multiple features maching the class:
$$
P(Y \mid X) = P(x_{1} \mid x_{2}, \ldots, x_{k}, Y) P(x_{2} \mid x_{3}, \ldots, x_{k}, Y) \ldots P(x_{k} \mid Y) P(Y)
$$&lt;/p&gt;
&lt;p&gt;According to &lt;strong&gt;Naive Assumption&lt;/strong&gt;, we assume features are independent.
$$
P(Y \mid X) = P(x_{1} \mid Y) P(x_{2} \mid Y) \ldots P(x_{k} \mid Y) P(Y)
$$&lt;/p&gt;
&lt;p&gt;Now, we can easily compute the result. Note that $ P(Y) $ could be obtained through &lt;strong&gt;domain knwoledge&lt;/strong&gt; or could be estimated from &lt;strong&gt;data&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;logistic-regression-text-classifier&#34;&gt;Logistic Regression Text Classifier&lt;/h2&gt;
&lt;h3 id=&#34;lr-vs-nb&#34;&gt;LR vs. NB&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Logistic Regression&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Naive Bayes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conditional Distribution $P(Y \mid X)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Joint Distribution $P(X,Y)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Discriminative Model&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Generative Model&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Maybe better in general&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Better with smaller data&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;logistic-regression-overview&#34;&gt;Logistic Regression Overview&lt;/h3&gt;
&lt;p&gt;Logistic Regression is based on &lt;strong&gt;Linear Regression&lt;/strong&gt;, and is for &lt;strong&gt;classification&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
y=\frac{1}{1+e^{-(\vec{w} \cdot \vec{x})}}
$$&lt;/p&gt;
&lt;p&gt;$$
L = -Y \log \hat{Y}+(1-Y) \log (1-\hat{Y})
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;experimental-design-in-ml&#34;&gt;Experimental Design in ML&lt;/h2&gt;
&lt;h3 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h3&gt;
&lt;p&gt;Models overfit when then model idiosyncrasies of the training data
that don’t necessarily generalize.&lt;/p&gt;
&lt;h3 id=&#34;train-test-splits&#34;&gt;Train-Test Splits&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Train&lt;/strong&gt;: Used to train the model, i.e., set the parameters&lt;br&gt;
&lt;strong&gt;Dev&lt;/strong&gt;: Used during development, to inspect and choose &amp;ldquo;hyperparameters&amp;rdquo;&lt;br&gt;
&lt;strong&gt;Test&lt;/strong&gt;: In good experimental design, only allowed
to evaluate on test one time to avoid &amp;ldquo;cheating&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;i.i.d.&lt;/strong&gt;: Train and Test data are drawn from the same distribution. In reality, test isn’t always i.i.d.&lt;/p&gt;
&lt;h3 id=&#34;common-baselines&#34;&gt;Common Baselines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Guess at random&lt;/li&gt;
&lt;li&gt;State-of-the-art (SOTA)&lt;/li&gt;
&lt;li&gt;Task-specific Heuristics&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Most frequent class&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Skylines&amp;rdquo;, e.g. human performance, result under ideal conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Tasks &amp; Benchmarks</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-tasks-benchmarks/</link>
        <pubDate>Wed, 14 Sep 2022 10:33:24 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-tasks-benchmarks/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 14th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 14th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;task-driven-culture&#34;&gt;Task-Driven Culture&lt;/h2&gt;
&lt;p&gt;NLP research has tended to be organized around &amp;ldquo;tasks&amp;rdquo; with shared ideas. Instead of building a whole system themselves, they will justify the problem in relation to some commonly-agreed upon important &amp;ldquo;tasks&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;extrinsic--intrinsic-tasks&#34;&gt;Extrinsic &amp;amp; Intrinsic Tasks&lt;/h3&gt;
&lt;p&gt;Here, we take ACL Submission Topics as examples to make a classification.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extrinsic Tasks&lt;/strong&gt;: An &amp;ldquo;end user&amp;rdquo; might want to use&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dialog &amp;amp; Interactive Sys&lt;/li&gt;
&lt;li&gt;Info Retrieval &amp;amp; Text Mining&lt;/li&gt;
&lt;li&gt;Language Grounding to Vision, Robotics &amp;amp; Beyond&lt;/li&gt;
&lt;li&gt;Machine Translation &amp;amp; Multilinguality&lt;/li&gt;
&lt;li&gt;NLP Applications&lt;/li&gt;
&lt;li&gt;Question Answering&lt;/li&gt;
&lt;li&gt;Sementiment Analysis, Stylistic Analysis &amp;amp; Argument Mining&lt;/li&gt;
&lt;li&gt;Speech &amp;amp; Multimodality&lt;/li&gt;
&lt;li&gt;Summarization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Intrinsic Tasks&lt;/strong&gt;: Part of a bigger system&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generation&lt;/li&gt;
&lt;li&gt;ML for NLP&lt;/li&gt;
&lt;li&gt;Info Extraction&lt;/li&gt;
&lt;li&gt;Resources &amp;amp; Evaluation&lt;/li&gt;
&lt;li&gt;Discourses &amp;amp; Pragmatics&lt;/li&gt;
&lt;li&gt;Phonology, Morphology &amp;amp; Word Segmentation&lt;/li&gt;
&lt;li&gt;Syntax: Tagging, Chunking &amp;amp; Parsing&lt;/li&gt;
&lt;li&gt;Semantics: Lexical&lt;/li&gt;
&lt;li&gt;Sementics: Sentence-level Sementics, Textual Inference &amp;amp; Other Areas&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bakeoffs--leaderboards&#34;&gt;Bakeoffs &amp;amp; Leaderboards&lt;/h3&gt;
&lt;p&gt;Statistical Revolution uses the same inputs, and compares against the same &lt;strong&gt;ground truth&lt;/strong&gt; (standardized test sets) outputs, using the same metric. The filed places a lot of stock in &lt;strong&gt;empirical comparison of ideas&lt;/strong&gt; and &lt;strong&gt;applicability of tech&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DARPA&lt;/li&gt;
&lt;li&gt;SemEval&lt;/li&gt;
&lt;li&gt;SQuAD&lt;/li&gt;
&lt;li&gt;GLUE&lt;/li&gt;
&lt;li&gt;SuperGlue&lt;/li&gt;
&lt;li&gt;GEM&lt;/li&gt;
&lt;li&gt;DynaBench&lt;/li&gt;
&lt;li&gt;BigBench&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;major-downstream-tasks-and-evals&#34;&gt;Major &amp;ldquo;Downstream&amp;rdquo; Tasks and Evals&lt;/h2&gt;
&lt;h3 id=&#34;classification&#34;&gt;Classification&lt;/h3&gt;
&lt;h4 id=&#34;input--output&#34;&gt;Input &amp;amp; Output&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: a piece of text&lt;br&gt;
&lt;strong&gt;Output&lt;/strong&gt;: a label&lt;/p&gt;
&lt;h4 id=&#34;examples&#34;&gt;Examples&lt;/h4&gt;
&lt;p&gt;Sentiment prediction (e.g., for stock trading)&lt;br&gt;
Language detection (e.g., before machine translation)&lt;br&gt;
Relevance prediction (e.g., for retrieval or ad targeting)&lt;br&gt;
Intent classification (e.g., for goal-oriented dialog)&lt;/p&gt;
&lt;h4 id=&#34;standard-metrics&#34;&gt;Standard Metrics&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: How often is the label correct?&lt;br&gt;
&lt;strong&gt;Precision&lt;/strong&gt;: Probability that it is spam, given that the model says it is.&lt;br&gt;
&lt;strong&gt;Recall&lt;/strong&gt;: Probability that the model says its spam, given that it actually is spam.&lt;br&gt;
&lt;strong&gt;F-1 Score&lt;/strong&gt;: Harmonic Mean of Precision and Recall&lt;br&gt;
$$
F_{1}=2 \times \frac{\text { precision } \times \text { recall }}{\text { precision }+\text { recall }}
$$
&lt;strong&gt;AUC&lt;/strong&gt;: Area Under the True-Positive vs. False Positive Curve&lt;/p&gt;
&lt;h3 id=&#34;info-retrieval&#34;&gt;Info Retrieval&lt;/h3&gt;
&lt;h4 id=&#34;input--output-1&#34;&gt;Input &amp;amp; Output&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Query or Doc Collection&lt;br&gt;
&lt;strong&gt;Output&lt;/strong&gt;: Ranked List of Doc&lt;/p&gt;
&lt;h4 id=&#34;examples-1&#34;&gt;Examples&lt;/h4&gt;
&lt;p&gt;They have increasingly complex goals, which are interwoven with other NLP tasks (summarization, question answering).&lt;/p&gt;
&lt;h4 id=&#34;standard-metrics-1&#34;&gt;Standard Metrics&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Precision@K&lt;/strong&gt;: How many of the top K documents are relevant?&lt;br&gt;
&lt;strong&gt;Recall@K&lt;/strong&gt;: How many of the relevant documents occur in the top K?&lt;br&gt;
&lt;strong&gt;Mean Reciprocal Rank&lt;/strong&gt;: At what rank does the relevant document appear?&lt;br&gt;
&lt;strong&gt;Discounted Cumulative Gain (DCG)&lt;/strong&gt;: How much benefit with one more rank down the list?
$$
DCG_{p}=\sum_{i=1}^{p} \frac{rel_{i}}{\log _{2}(i+1)}
$$&lt;br&gt;
&lt;strong&gt;Area Under the Curve (AUC)&lt;/strong&gt;: Summary metrics for precision-recall curves
&lt;strong&gt;Averaged Precision (AP)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
AP=\sum_{n}\left(R_{n}-R_{n-1}\right) P_{n}
$$&lt;/p&gt;
&lt;h4 id=&#34;ground-truth&#34;&gt;Ground Truth&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Query Relevances (&amp;ldquo;qrels&amp;rdquo;)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Humans manually annotate documents for relevance, based on description of query&lt;/li&gt;
&lt;li&gt;Too expensive to estimate recall (requires labeling every document for every query)&lt;/li&gt;
&lt;li&gt;Not “ecologically valid”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;User Behavior/AB Testing&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the user click on links?&lt;/li&gt;
&lt;li&gt;Do they stay on the page after clicking?&lt;/li&gt;
&lt;li&gt;Do they go back and reword the query?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;questions-answering&#34;&gt;Questions Answering&lt;/h3&gt;
&lt;h4 id=&#34;variants-of-qa&#34;&gt;Variants of QA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Open Book&lt;/li&gt;
&lt;li&gt;Cloased Book&lt;/li&gt;
&lt;li&gt;QA over Databases&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;open-book-qa&#34;&gt;Open Book QA&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Question + Doc&lt;br&gt;
&lt;strong&gt;Output&lt;/strong&gt;: Answer / Highlighted Span&lt;/p&gt;
&lt;h4 id=&#34;closed-book-qa&#34;&gt;Closed Book QA&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Question
&lt;strong&gt;Output&lt;/strong&gt;: Answer&lt;br&gt;
&lt;strong&gt;Note&lt;/strong&gt;: Hard to verify answer source&lt;/p&gt;
&lt;h4 id=&#34;database-book-qa&#34;&gt;Database Book QA&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Question + Database&lt;br&gt;
&lt;strong&gt;Output&lt;/strong&gt;: Answer&lt;br&gt;
&lt;strong&gt;Note&lt;/strong&gt;: 1) Good for short, factoid questions 2) Requires formal query languages&lt;/p&gt;
&lt;h4 id=&#34;standard-metrics-2&#34;&gt;Standard Metrics&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: What % of questions were correct?&lt;br&gt;
&lt;strong&gt;Exact Match Accuracy&lt;/strong&gt;: % of times model’s answer was string identical to ground truth
&lt;strong&gt;Precision&lt;/strong&gt;: How many of the predicted tokens were correct?&lt;br&gt;
&lt;strong&gt;Recall&lt;/strong&gt;: How many of the correct tokens were predicted?&lt;br&gt;
&lt;strong&gt;F-1&lt;/strong&gt;: Harmonic mean of precision and recall&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;important-intrinsic-tasks-and-evals&#34;&gt;Important Intrinsic Tasks and Evals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tokenization&lt;/li&gt;
&lt;li&gt;Sentence Splitting&lt;/li&gt;
&lt;li&gt;Part-of-speech Tagging&lt;/li&gt;
&lt;li&gt;Morphological Analysis&lt;/li&gt;
&lt;li&gt;Named Entity Recognition&lt;/li&gt;
&lt;li&gt;Syntactic Parsing&lt;/li&gt;
&lt;li&gt;Coreference Resolution&lt;/li&gt;
&lt;li&gt;Other Annotations&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;current-debates&#34;&gt;Current Debates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Extrinsic vs. Intrinsic&lt;/li&gt;
&lt;li&gt;Scientific Validity&lt;/li&gt;
&lt;li&gt;Leaderboardism&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Intro to NLP</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/</link>
        <pubDate>Mon, 12 Sep 2022 22:06:54 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 12th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 12th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-nlp&#34;&gt;What is NLP?&lt;/h2&gt;
&lt;p&gt;The goal of this field is to get computers to perform useful tasks involving human language.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Search&lt;/li&gt;
&lt;li&gt;Auto Completion&lt;/li&gt;
&lt;li&gt;Info Retrieval&lt;/li&gt;
&lt;li&gt;Summarization&lt;/li&gt;
&lt;li&gt;Ad Targeting&lt;/li&gt;
&lt;li&gt;Recommendations&lt;/li&gt;
&lt;li&gt;Language Identification&lt;/li&gt;
&lt;li&gt;Language Translation&lt;/li&gt;
&lt;li&gt;Text Generation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-brief-history&#34;&gt;A Brief History&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/timeline.jpg&#34;
	width=&#34;995&#34;
	height=&#34;464&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/timeline_hu58ee0901517331e3436221a7560a45c9_287527_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/timeline_hu58ee0901517331e3436221a7560a45c9_287527_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;NLP History&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;214&#34;
		data-flex-basis=&#34;514px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;language-processing-pipeline&#34;&gt;Language Processing Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Phonology
&lt;ul&gt;
&lt;li&gt;Sounds&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Morphology
&lt;ul&gt;
&lt;li&gt;Analysis of the individual components of words&lt;/li&gt;
&lt;li&gt;e.g. suffixes and prefixes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lexicon
&lt;ul&gt;
&lt;li&gt;Words / Forms&lt;/li&gt;
&lt;li&gt;It captures the meanings of words.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Syntax
&lt;ul&gt;
&lt;li&gt;Sentences / Clauses&lt;/li&gt;
&lt;li&gt;Structural relationship or grammar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantics
&lt;ul&gt;
&lt;li&gt;Meanings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pragmatics
&lt;ul&gt;
&lt;li&gt;Language use&lt;/li&gt;
&lt;li&gt;Goals &amp;amp; purposes in a sequential level&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discourse
&lt;ul&gt;
&lt;li&gt;Context&lt;/li&gt;
&lt;li&gt;Expressions beyond sentences &amp;amp; paragraphs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://csci-1460-computational-linguistics.github.io/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CSCI1460 Computational Linguistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
