<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Machine Learning on Tony Feng</title>
        <link>https://tonyfpy.github.io/tags/machine-learning/</link>
        <description>Recent content in Machine Learning on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 07 Oct 2022 17:05:02 -0400</lastBuildDate><atom:link href="https://tonyfpy.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Machine Learning] Interview Questions - Part 2</title>
        <link>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-2/</link>
        <pubDate>Fri, 07 Oct 2022 17:05:02 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-2/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 1st, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 1st, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;The post contains a collection of questions for machine learning interview.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1) Why do ensembles typically have higher scores than individual models?&lt;/strong&gt;&lt;br&gt;
An ensemble is the combination of &lt;strong&gt;multiple models&lt;/strong&gt; to create a &lt;strong&gt;single prediction&lt;/strong&gt;. The key idea is that the errors of one model will be compensated by the right predictions of the other models and thus the score of the ensemble will be higher.&lt;/p&gt;
&lt;p&gt;We need diverse models for creating an ensemble.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using different ML algorithms.&lt;/li&gt;
&lt;li&gt;Using different subsets of the data for training. (Bagging)&lt;/li&gt;
&lt;li&gt;Giving a different weight to each of the samples of the training set. (Boosting)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Engineers need to find a balance between &lt;strong&gt;execution time&lt;/strong&gt; and &lt;strong&gt;accuracy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) What is an imbalanced dataset? Can you list some ways to deal with it?&lt;/strong&gt;&lt;br&gt;
An imbalanced dataset has different proportions of categories. There are different options to deal with imbalanced datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resampling the dataset: undersampling majority classes and oversampling minority classes&lt;/li&gt;
&lt;li&gt;Data Augmentation&lt;/li&gt;
&lt;li&gt;Cross Validation&lt;/li&gt;
&lt;li&gt;Choose a right algorithm, e.g., random forest&lt;/li&gt;
&lt;li&gt;Using appropriate metrics, e.g., F-score, Confusion Matrix, which describe the performance of the model better on an imbalanced dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3) What is data augmentation? Can you give some examples?&lt;/strong&gt;&lt;br&gt;
Data augmentation is a technique for generating new data by modifying existing data in such a way that the target is not changed, or it is changed in a known way.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4) Explain how the AUC - ROC curve works&lt;/strong&gt;&lt;br&gt;
ROC or Receiver Operating Characteristic curve represents a graphical representation of the contrast between &lt;strong&gt;true positive rates and the false positive rate&lt;/strong&gt; at various thresholds.&lt;/p&gt;
&lt;p&gt;AUC is known for Area Under the ROC curve. It calculates the two-dimensional area under the entire ROC curve ranging from 0 to 1, which means an excellent model will have AUC near 1, and hence it will show a good measure of &lt;strong&gt;separability&lt;/strong&gt;. AUC is &lt;strong&gt;scale-invariant&lt;/strong&gt;. It measures &lt;strong&gt;how well predictions are ranked&lt;/strong&gt;, rather than their absolute values. AUC is &lt;strong&gt;classification-threshold-invariant&lt;/strong&gt;. It measures the quality of the model&amp;rsquo;s predictions irrespective of what classification threshold is chosen.&lt;/p&gt;
&lt;p&gt;AUC is not preferable when we need to calibrate probability output. Further, AUC is not a useful metric when there are wide disparities in the cost of false negatives vs false positives, and it is difficult to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives even if that results in a significant increase of false negatives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5) What is Precision/Recall/F1-score?&lt;/strong&gt;&lt;br&gt;
Precision (positive predictive value) is the fraction of relevant instances among the retrieved instances.&lt;/p&gt;
&lt;p&gt;$$ Precision = \frac{TP}{TP + FP} $$&lt;/p&gt;
&lt;p&gt;Recall (sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.&lt;/p&gt;
&lt;p&gt;$$ Recall = \frac{TP}{TP + FN} $$&lt;/p&gt;
&lt;p&gt;F1-score is the weighted average of precision and recall. It considers both false positive and false negative into account.&lt;/p&gt;
&lt;p&gt;$$ F1 = \frac{2PR}{P + R} $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6) Define Learning Rate&lt;/strong&gt;&lt;br&gt;
The learning rate is a hyper-parameter used to govern the pace at which an algorithm updates or learns the values of a parameter estimate. In other words, it controls how much we are adjusting the weights of our network with respect the loss gradient.&lt;/p&gt;
&lt;p&gt;When training loss fluctuates, we may reduce the learning rate. Otherwise, SGD jumps too far and misses the area near local minima.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7) The differences between Batch Gradient Descent and Stochastic Gradient Descent&lt;/strong&gt;&lt;br&gt;
Batch Gradient Descent involves calculations over the &lt;strong&gt;whole training set&lt;/strong&gt; at each step as a result of which it is very slow on very large training data. Thus, it becomes very &lt;strong&gt;computationally expensive&lt;/strong&gt;. However, this is great for &lt;strong&gt;convex&lt;/strong&gt; or relatively smooth error manifolds. Also, Batch GD scales well with the number of features.&lt;/p&gt;
&lt;p&gt;Stochastic gradient descent (SGD) picks up a &lt;strong&gt;&amp;ldquo;random&amp;rdquo; instance&lt;/strong&gt; of training data at each step and then computes the gradient, making it much faster to reach the &lt;strong&gt;convergence&lt;/strong&gt;. Also, it can &lt;strong&gt;escape shallow local minima&lt;/strong&gt; more easily.&lt;/p&gt;
&lt;p&gt;Batch GD: Batch size = Size of training set&lt;br&gt;
Stochastic GD: Batch size = 1&lt;br&gt;
Mini-Batch GD: 1 &amp;lt; Batch size &amp;lt; Size of training set&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8) Epoch vs. Batch vs. Iteration&lt;/strong&gt;&lt;br&gt;
Epoch: It&amp;rsquo;s the number of times the whole training dataset is passed through the model. &lt;br&gt;
Batch: It&amp;rsquo;s the number of examples processed together in one pass.&lt;br&gt;
Iteration: It&amp;rsquo;s the number of batches required to complete one epoch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9) What is gradient vanishing? What is gradient explosion?&lt;/strong&gt;&lt;br&gt;
As we add more and more hidden layers, back propagation becomes less and less useful in passing information to the lower layers. &lt;strong&gt;A large change in the input will cause a small change in the output.&lt;/strong&gt; In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the networks. On the contrary, if the gradients get larger or even NaN as our backpropagation progresses, &lt;strong&gt;the error gradients accumulate&lt;/strong&gt; and end up with exploding gradients having big weight updates.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;sigmoid&lt;/strong&gt;, it saturates at zero for large negative values and at one for large positive values. The same applies to the &lt;strong&gt;Tanh&lt;/strong&gt; function that saturates at -1 and 1. &lt;strong&gt;ReLU&lt;/strong&gt; returns the input if the input value is positive, and it returns 0 if the input is negative.&lt;/p&gt;
&lt;p&gt;ReLu can solve gradient vanishing but may cause gradient explosion. &lt;strong&gt;The output of ReLU is unbounded in the positive domain by design.&lt;/strong&gt; This means that the output can, in some situations, continue to grow in size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10) What&amp;rsquo;s the difference between boosting and bagging?&lt;/strong&gt;&lt;br&gt;
Boosting and bagging are similar, in that they are both ensembling techniques, where &lt;strong&gt;multiple weak learners&lt;/strong&gt; (classifiers/regressors that are barely better than guessing) combine (through averaging or max vote) to form &lt;strong&gt;a single strong learner&lt;/strong&gt; that can make accurate predictions.&lt;/p&gt;
&lt;p&gt;Bagging means that you take &lt;strong&gt;bootstrap&lt;/strong&gt; samples (with replacement) of your dataset and each sample trains a (potentially) weak learner. Boosting, on the other hand, uses all data to train each learner, but instances that were misclassified by the previous learners are given &lt;strong&gt;more weight&lt;/strong&gt; so that &lt;strong&gt;subsequent&lt;/strong&gt; learners give more focus to them during training.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/andrewekhalel/MLQuestions&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ML Interview Questions collected by andrewekhalel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Interview Questions - Part 1</title>
        <link>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-1/</link>
        <pubDate>Sat, 01 Oct 2022 19:51:42 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-interview-questions-part-1/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 1st, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 1st, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;The post contains a collection of questions for machine learning interview.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1) Explain bias and variance, and trade-off between them&lt;/strong&gt;&lt;br&gt;
Bias is the &lt;strong&gt;difference&lt;/strong&gt; between the average prediction of our model and the ground truth.&lt;/p&gt;
&lt;p&gt;Variance refers to the &lt;strong&gt;variability&lt;/strong&gt; in the model prediction. In other words, it reflects the changes in the model when using different portions of the training dataset.&lt;/p&gt;
&lt;p&gt;Bias and variance are &lt;strong&gt;inversely connected&lt;/strong&gt;. An underfitting model has high bias and low variance, while an overfitting model has high variance and low bias. So we need to find the right balance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) What is gradient descent?&lt;/strong&gt;&lt;br&gt;
GD is an &lt;strong&gt;optimization&lt;/strong&gt; algorithm that finds the values of parameters (coefficients) of a loss function that minimizes a cost function.&lt;/p&gt;
&lt;p&gt;GD is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3) Difference between Loss Function, Cost Function&lt;/strong&gt;&lt;br&gt;
The Loss function is associated with &lt;strong&gt;single training example&lt;/strong&gt;, and the cost function is the average value of the loss function over the &lt;strong&gt;entire training dataset&lt;/strong&gt;. In ML, we usually try to optimize our cost
function rather than loss function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4) How to handle overfitting and underfitting&lt;/strong&gt; &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Handling Overfitting&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Handling Underfitting&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;More Training Data&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Removing Data Noise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Regularization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Reducing Regularization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Reducing Iterations&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Increasing Iterations&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Reducing Features&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Increasing Features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Increasing Learning Rate&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Reducing Learning Rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Other Strategies: Pruning, Dropout, etc.&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Increasing Model Complexity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;5) What is curse of dimensionality? How to prevent it?&lt;/strong&gt;&lt;br&gt;
CoD basically means that the error increases with the increase in the number of features. It leads to exponential increase in computational efforts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improving the ratio of observations over attributes&lt;/li&gt;
&lt;li&gt;Making distances in feature space more meaningful&lt;/li&gt;
&lt;li&gt;Removing features that have no correlation with the target distribution&lt;/li&gt;
&lt;li&gt;Removing or combining features that have redundant correlation with target distribution&lt;/li&gt;
&lt;li&gt;Extracting new features with a more direct correlation with target distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;6) What is regularization, and give some examples of common methods?&lt;/strong&gt;&lt;br&gt;
Regularization refers to techniques that are used to calibrate machine learning models so as to &lt;strong&gt;avoid the risk of overfitting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The obvious disadvantage of ridge is model interpretability. It will shrink the coefficients for least important features, &lt;strong&gt;only close to zero&lt;/strong&gt;. However, for lasso, it can force some of the coefficient estimates to &lt;strong&gt;be zero&lt;/strong&gt; when the weight is sufficiently large. Therefore, the lasso method also performs variable selection and is said to yield sparse models.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;L1 norm (Lasso)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;L2 norm (Ridge)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Penalizes the sum of absolute values of weights&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Penalizeshe the sum of squares of weights&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Sparse solution&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Non-sparse solution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Feature selection&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No feature selection&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Robust to outliers&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Not robust to outliers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Unable to learn complex data patterns&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Able to learn complex data patterns&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;7) Explain Principal Component Analysis (PCA)?&lt;/strong&gt;&lt;br&gt;
PCA is used for the &lt;strong&gt;dimensionality reduction&lt;/strong&gt;, which tries to find the lower-dimensional surface to project the high-dimensional data.&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://builtin.com/data-science/step-step-explanation-principal-component-analysis&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Common Steps&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standardizing the range of continuous initial variables&lt;/li&gt;
&lt;li&gt;Computing the covariance matrix to identify correlations&lt;/li&gt;
&lt;li&gt;Computing the eigenvectors and eigenvalues of the covariance matrix to identify the principal components&lt;/li&gt;
&lt;li&gt;Creating a feature vector to decide which principal components to keep&lt;/li&gt;
&lt;li&gt;Recasting the data along the principal components axes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;8) What is data normalization and why do we need it?&lt;/strong&gt;&lt;br&gt;
Data normalization is a &lt;strong&gt;preprocessing&lt;/strong&gt; step to &lt;strong&gt;re-scale values&lt;/strong&gt; to fit in a specific range to assure better convergence. In general, it boils down to subtracting the &lt;strong&gt;mean&lt;/strong&gt; of each data point and dividing by its &lt;strong&gt;standard deviation&lt;/strong&gt;. The data normalization makes all features weighted equally. Otherwise, some features with high magnitude will be weighted more in the cost function, while other features with lower values will be allocated less weights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9) What is the difference between training, validation set and test set?&lt;/strong&gt;&lt;br&gt;
The training dataset is used for &lt;strong&gt;fitting the model’s parameters&lt;/strong&gt;. However, the accuracy that we achieve on the training set is not reliable for predicting if the model will be accurate on new samples.&lt;/p&gt;
&lt;p&gt;The validation dataset is used to &lt;strong&gt;measure how well the model does&lt;/strong&gt; on examples that weren’t part of the training dataset and to &lt;strong&gt;provide information&lt;/strong&gt; for adjusting the model. The more evaluations, the more information is leaked. So we can end up overfitting to the validation data.&lt;/p&gt;
&lt;p&gt;The test dataset is used to measure how well the model does on previously unseen examples. It should &lt;strong&gt;only be used once&lt;/strong&gt; we have tuned the parameters using the validation set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10) What is cross-validation?&lt;/strong&gt;&lt;br&gt;
CV is a statistical &lt;strong&gt;resampling&lt;/strong&gt; technique that uses different parts of the dataset to train and test an ML algorithm on different iterations. The aim of CV provides the ability to estimate model performance on unseen data.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/andrewekhalel/MLQuestions&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ML Interview Questions collected by andrewekhalel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>An Intro to Kalman Filter</title>
        <link>https://tonyfpy.github.io/p/an-intro-to-kalman-filter/</link>
        <pubDate>Sun, 28 Aug 2022 22:57:10 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/an-intro-to-kalman-filter/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 23th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Aug 28th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;content&#34;&gt;Content&lt;/h2&gt;
&lt;div align=&#34;center&#34; &gt;
    &lt;img src=&#34;KF1_1.png&#34; width=100%/&gt;
&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;
    &lt;img src=&#34;KF1_2.png&#34; width=100%/&gt;
&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;
    &lt;img src=&#34;KF1_3.png&#34; width=100%/&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kalmanfilter.net/default.aspx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Online Kalmman Filter Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1wT4y19732?p=1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;目标追踪—计算机博士精讲卡尔曼滤波算法教程卡尔曼滤波&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Topic 4: K-Nearest Neighbours</title>
        <link>https://tonyfpy.github.io/p/machine-learning-topic-4-k-nearest-neighbours/</link>
        <pubDate>Wed, 18 May 2022 22:45:07 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-topic-4-k-nearest-neighbours/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on May 18th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on May 19th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;K-nearest neighbors (KNN)&lt;/strong&gt; is a simple, easy to understand machine learning algorithms. It examines the labels of a chosen number of data points surrounding a target data point, in order to make a prediction about the class that the data point falls into.&lt;/p&gt;
&lt;p&gt;It is a type of &lt;strong&gt;supervised&lt;/strong&gt; learning algorithm used for both &lt;strong&gt;regression&lt;/strong&gt; and &lt;strong&gt;classification&lt;/strong&gt;. 1) In classification, it tries to predict the class for the test data by a &lt;strong&gt;majority vote&lt;/strong&gt; from K amount of neighbours. 2) In the case of regression, the prediction is the &lt;strong&gt;mean&lt;/strong&gt; of the K selected training points.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;characteristics&#34;&gt;Characteristics&lt;/h2&gt;
&lt;h3 id=&#34;non-parametic&#34;&gt;Non-parametic&lt;/h3&gt;
&lt;p&gt;KNN is &lt;strong&gt;non-parametric&lt;/strong&gt;, which means there is &lt;strong&gt;no assumption&lt;/strong&gt; for underlying data distribution (i.e. the model structure is determined from the dataset).&lt;/p&gt;
&lt;h3 id=&#34;lazy&#34;&gt;Lazy&lt;/h3&gt;
&lt;p&gt;It is called &lt;strong&gt;Lazy algorithm&lt;/strong&gt; because it does not need any training data points for model generation. All training data is used in the testing phase. This makes training faster, but making the testing phase slower and costlier.&lt;/p&gt;
&lt;h3 id=&#34;similarity-measure&#34;&gt;Similarity Measure&lt;/h3&gt;
&lt;p&gt;KNN stores all the available cases and predicts the new data based on a &lt;strong&gt;similarity measure&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;knn-details&#34;&gt;KNN Details&lt;/h2&gt;
&lt;h3 id=&#34;steps&#34;&gt;Steps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Choosing a K and a distance metric&lt;/li&gt;
&lt;li&gt;Calculating the distance between a test example and each training examples&lt;/li&gt;
&lt;li&gt;Sorting the calculated distances&lt;/li&gt;
&lt;li&gt;Returning a prediction
&lt;ul&gt;
&lt;li&gt;Classification: Assigning the most frequent label of the top K neighbors&lt;/li&gt;
&lt;li&gt;Regression: Outputing the mean (average) of K-nearest neighbours&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;k-selection&#34;&gt;K Selection&lt;/h3&gt;
&lt;p&gt;There are no pre-defined statistical methods to find the most favorable value of K. Sometimes, we may apply a &lt;strong&gt;domain knowledge&lt;/strong&gt; to select a K. Additionally, we can derive &lt;strong&gt;a plot between error rate and K denoting values&lt;/strong&gt; in a defined range.&lt;/p&gt;
&lt;p&gt;A smaller K will lead to overfitting, while a larger K results in smooth decision boundaries or underfitting.&lt;/p&gt;
&lt;p&gt;In some cases, the algorithm faces a dilemma in majority vote. One solution, which will be discussed later, is to change to another decision rule. Besides, we can consider the &lt;strong&gt;parity&lt;/strong&gt; of the number of classes and the value of K.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the number of classes is even, then K should be odd;&lt;/li&gt;
&lt;li&gt;If the number of classed is odd, then K should be even.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distance-metric&#34;&gt;Distance Metric&lt;/h3&gt;
&lt;p&gt;There are various methods for calculating this distance, of which the most commonly known methods are Euclidian, Manhattan (for &lt;strong&gt;continuous&lt;/strong&gt; variables) and Hamming distance (for &lt;strong&gt;categorical&lt;/strong&gt; variables).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Euclidean Distance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ ED = \sqrt{\sum_{i=1}^{k}(x_{i}-y_{i})^{2}} $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Manhattan Distance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ MD = \sum_{i=1}^{k}|x_{i}-y_{i}| $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hamming Distance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$ HD = \sum_{i=1}^{k} M_i $$&lt;/p&gt;
&lt;p&gt;, where $ M_i = 0 $ if $ x_i = y_i $, else $ M_i = 1 $.&lt;/p&gt;
&lt;h3 id=&#34;decision-rules&#34;&gt;Decision Rules&lt;/h3&gt;
&lt;p&gt;When combining the class labels, the simplest method is to take the &lt;strong&gt;majority vote&lt;/strong&gt;, but this can be a problem if the nearest neighbors vary widely in their distance and the closest neighbors more reliably indicate the class of the object.&lt;/p&gt;
&lt;p&gt;To overcome the issue, we can assign weights to the nearest k points . The intuition behind &lt;strong&gt;weighted KNN&lt;/strong&gt; is to give more weight to the points which are nearby and less weight to the points which are farther away.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;h3 id=&#34;pros&#34;&gt;Pros&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;KNN is straightforward and &lt;strong&gt;easy to implement&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No &amp;ldquo;training&amp;rdquo; process&lt;/strong&gt; are needed  in the traditional sense.&lt;/li&gt;
&lt;li&gt;KNN doesn’t function with emphasis on assumptions.&lt;/li&gt;
&lt;li&gt;KNN can quickly adapt to new data.&lt;/li&gt;
&lt;li&gt;This algorithm can be used for both regression and classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cons&#34;&gt;Cons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;KNN algorithm is computationally expensive and slow.&lt;/li&gt;
&lt;li&gt;It needs a &lt;strong&gt;large memory storage&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The efficiency of KNN is inversely proportional to the number of features.&lt;/li&gt;
&lt;li&gt;It is highly susceptible to &lt;strong&gt;the curse of dimensionality&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;It cannot function properly with &lt;strong&gt;missing values&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outliers&lt;/strong&gt; are especially impactful for KNN.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;referrence&#34;&gt;Referrence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;K-Nearest Neighbor - Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;K-Nearest Neighbor(KNN) Algorithm for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.datadriveninvestor.com/k-nearest-neighbours-knn-a9f8ba09cb8b&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Intro to K-Nearest Neighbours (KNN) — Machine Learning 101&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.datadriveninvestor.com/k-nearest-neighbors-knn-algorithm-bd375d14eec7&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;K-Nearest Neighbors (KNN) Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.unite.ai/what-is-k-nearest-neighbors/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;What is a KNN (K-Nearest Neighbors)?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://mysweetindulgence.com/easy-writing/what-is-distance-weighted-knn/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;What is distance weighted KNN?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Topic 3: Decisoin Tree</title>
        <link>https://tonyfpy.github.io/p/machine-learning-topic-3-decisoin-tree/</link>
        <pubDate>Wed, 11 May 2022 20:51:10 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-topic-3-decisoin-tree/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on May 11th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on May 14th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;A decision tree is one of the supervised machine learning algorithms. Decision Tree algorithm belongs to the family of supervised learning algorithms. The goal is to create a model that predicts the value of a target variable by learning &lt;strong&gt;if-else conditions(decision rules)&lt;/strong&gt; inferred from the data features.&lt;/p&gt;
&lt;h3 id=&#34;types-of-decision-tree&#34;&gt;Types of Decision Tree&lt;/h3&gt;
&lt;p&gt;Decision Tree (DT) can be used for both &lt;strong&gt;regression&lt;/strong&gt; and &lt;strong&gt;classification&lt;/strong&gt; problems, but it is mostly used for classification problems. Based on the types of target variable, we have 1) &lt;strong&gt;categorical-variable&lt;/strong&gt; DT, and 2) &lt;strong&gt;continuous-variable&lt;/strong&gt; DT.&lt;/p&gt;
&lt;h3 id=&#34;non-linearity&#34;&gt;Non-linearity&lt;/h3&gt;
&lt;p&gt;Formally, a method is &lt;strong&gt;linear&lt;/strong&gt; if, for an input $ x \isin {\R}^n $ (with interecept term $ x_0=1 $), it only produces hypothesis functions $ h $ of the form:&lt;/p&gt;
&lt;p&gt;$$ h(x) = \theta^{T}x, \theta \isin {\R}^n $$&lt;/p&gt;
&lt;p&gt;Otherwise, it is &lt;strong&gt;non-linear&lt;/strong&gt;. Decision trees, on the other hand, can directly produce non-linear hypothesis functions without the need for first coming up with an appropriate feature mapping.&lt;/p&gt;
&lt;h3 id=&#34;region-selection&#34;&gt;Region Selection&lt;/h3&gt;
&lt;p&gt;In general, selecting optimal regions is intractable. Decision trees generate an approximate solution via &lt;strong&gt;greedy&lt;/strong&gt;, &lt;strong&gt;top-down&lt;/strong&gt;, &lt;strong&gt;recursive&lt;/strong&gt; partitioning.  Each node in the tree acts as a &lt;strong&gt;test case&lt;/strong&gt; for some &lt;strong&gt;attribute&lt;/strong&gt;, and each edge descending from that node corresponds to one of the possible answers to the test case.&lt;/p&gt;
&lt;p&gt;Formally, given a input space $ X $, a parent region $ R_p $, a feature index $ j $, and a threshold $ t \isin {\R}$, we obtain two child regions $R_1$  and $R_2$ as follows:&lt;/p&gt;
&lt;p&gt;$$ R_{1} = \{X \mid X_{j}&amp;lt;t, X \in R_{p} \} $$ $$ R_{2} = \{X \mid X_{j} \geq t, X \in R_{p} \} $$&lt;/p&gt;
&lt;p&gt;We can continue in such a manner until we a meet a &lt;strong&gt;pre-defined stop criterion&lt;/strong&gt;, and then predict the majority class at each leaf node.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;attribute-selective-measures&#34;&gt;Attribute Selective Measures&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;attribute selection measure (AMS)&lt;/strong&gt; is a technique used in the data mining process for data reduction. It is a &lt;strong&gt;heuristic&lt;/strong&gt; for choosing the splitting test that “best” separates a given data partition. The three main ASM techniques are 1) &lt;strong&gt;Information Gain&lt;/strong&gt;, 2) &lt;strong&gt;Gain Ratio&lt;/strong&gt;, and 3) &lt;strong&gt;Gini Index&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;information-gain&#34;&gt;Information Gain&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Entropy&lt;/strong&gt; is used to measure the level of &lt;strong&gt;impurity&lt;/strong&gt; in a group of examples. This concept comes from informatin theory. The higher the entropy, the more the information content. Now, consider a dataset with $n$ classes, then&lt;/p&gt;
&lt;p&gt;$$ E = -\sum_{i=1}^{n} p_{i} \log_{2} (p_{i}) $$&lt;/p&gt;
&lt;p&gt;, where $ p_i $ is the probability of randomly selecting an example in class $i$.&lt;/p&gt;
&lt;p&gt;We can define &lt;strong&gt;information gain (IG)&lt;/strong&gt; as a measure of how much information a feature provides about a class. It is the expected reduction in entropy caused by partitioning the examples according to a given attribute. Given a collection of dataset $ S $, we can calculate the $IG(S, A)$ of an attribute $A$ as:&lt;/p&gt;
&lt;p&gt;$$ IG(S, A) = E(S) - \sum_{v \in V} \frac{|S_{v}|}{|S|} E(S_{v}) $$&lt;/p&gt;
&lt;p&gt;, where $V$ is all possible values for attribute $A$ and $S_{v}$ is the subset of $S$ for which attribute $A$ has value $v$.&lt;/p&gt;
&lt;h3 id=&#34;gain-ratio&#34;&gt;Gain Ratio&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Gain Ratio&lt;/strong&gt; or &lt;strong&gt;Uncertainty Coefficient&lt;/strong&gt; is used to normalize the information gain of an attribute against how much entropy that attribute has. It attempts to lessen the bias of Information Gain on highly branched predictors by introducing a normalizing term called the &lt;strong&gt;Intrinsic Information (II)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$ II(S, A) = - \sum_{v \in V(A)} \frac{|S_{v}|}{|S|} \log_2 \frac{|s_{v}|}{|S|} $$&lt;/p&gt;
&lt;p&gt;Formula of Gain Ration is given by&lt;/p&gt;
&lt;p&gt;$$ GR(S, A) = \frac{IG(S, A)}{II(S, A)} $$&lt;/p&gt;
&lt;p&gt;From this formula, we can see that, with less value in attribute $ A $, $ II(S, A)$ is smaller and thus purity is higher. Informally, the formula of Intrinsic Information is the same as that of Entropy, both of which means the purity of attribute $ A $.&lt;/p&gt;
&lt;h3 id=&#34;gini-index&#34;&gt;Gini Index&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Gini Index&lt;/strong&gt;, also called &lt;strong&gt;Gini Impurity&lt;/strong&gt;, measures the degree or probability of a particular variable being &lt;strong&gt;wrongly&lt;/strong&gt; classified when it is randomly chosen. If we have $C$ total classes and $p_i$ is the probability of picking a datapoint with class $i$, then the Gini Impurity is calculated as&lt;/p&gt;
&lt;p&gt;$$ GI=\sum_{i=1}^{C} p_i(1-p_i) = 1 - \sum_{i=1}^{C} p_i^{2}$$&lt;/p&gt;
&lt;p&gt;For a data sample $ S $, we have an attribute $ A $ with a group of values $V$. Therefore,&lt;/p&gt;
&lt;p&gt;$$ GI(S, A) = \sum_{v \in V} \frac{|S_{v}|}{|S|} GI(S_{v}) $$&lt;/p&gt;
&lt;p&gt;The Gini Index varies between $0$ and $1$, where $0$ represents purity of the classification and $1$ denotes random distribution of elements among various classes. A Gini Index of $0.5$ shows that there is &lt;strong&gt;equal distribution&lt;/strong&gt; of elements across some classes.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;feature-selection-algorithms&#34;&gt;Feature Selection Algorithms&lt;/h2&gt;
&lt;h3 id=&#34;id3&#34;&gt;ID3&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;ID3&lt;/strong&gt; algorithm builds decision trees using a &lt;strong&gt;top-down greedy search&lt;/strong&gt; approach through the space of possible branches with no backtracking.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps of ID3&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Begin with the original datset as the root node and calculte its entropy.&lt;/li&gt;
&lt;li&gt;For each attribute/feature:
&lt;ul&gt;
&lt;li&gt;Calculate entropy for all its categorical values.&lt;/li&gt;
&lt;li&gt;Calculate information gain for the feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Find the feature with &lt;strong&gt;maximum information gain&lt;/strong&gt; and split to produce subsets.&lt;/li&gt;
&lt;li&gt;Continues to recur on each subset until we get the desired tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ID3 has no pruning strategy and is easy to &lt;strong&gt;overfit&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The IG criterion has a preference for features with a number of possible values.&lt;/li&gt;
&lt;li&gt;It can only be used to deal with &lt;strong&gt;discretely&lt;/strong&gt; distributed features;&lt;/li&gt;
&lt;li&gt;It does not consider missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;c45&#34;&gt;C4.5&lt;/h3&gt;
&lt;p&gt;Compared with the shortcomings of ID3, C4.5 has the following improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C4.5 overcomes is the shortcoming of ID3&amp;rsquo;s emphasis on the number of feature by using the &lt;strong&gt;Gain Ratio&lt;/strong&gt; as the goodness function to split the dataset.&lt;/li&gt;
&lt;li&gt;C4.5 is able to handle both &lt;strong&gt;continuous&lt;/strong&gt; and &lt;strong&gt;discrete&lt;/strong&gt; attributes.&lt;/li&gt;
&lt;li&gt;C4.5 has a &lt;strong&gt;pruning&lt;/strong&gt; strategy, which replaces the helpless branches with leaf nodes after the tree is created.&lt;/li&gt;
&lt;li&gt;C4.5 takes &lt;strong&gt;missing values&lt;/strong&gt; into account.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C4.5 uses a &lt;strong&gt;polytree&lt;/strong&gt; which is less efficient than a binary tree.&lt;/li&gt;
&lt;li&gt;C4.5 can only be used for &lt;strong&gt;classification&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;C4.5 is &lt;strong&gt;computaionally expensive&lt;/strong&gt; both in time and space.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cart&#34;&gt;CART&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Classification And Regression Trees (CART)&lt;/strong&gt; is a recursive partitioning method, builds classification and regression trees for predicting &lt;strong&gt;continuous dependent variables (regression)&lt;/strong&gt; and &lt;strong&gt;categorical predictor variables (classification)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a class=&#34;link&#34; href=&#34;http://rollingdeep.github.io/2020/11/02/CART%E5%88%86%E7%B1%BB%E6%A0%91%E4%BE%8B%E5%AD%90%E8%AE%B2%E8%A7%A3/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;classification algorithm&lt;/a&gt; for building a decision tree is based on &lt;strong&gt;Gini impurity&lt;/strong&gt; as splitting criterion, while the &lt;a class=&#34;link&#34; href=&#34;http://rollingdeep.github.io/2020/11/02/CART%E5%9B%9E%E5%BD%92%E6%A0%91%E4%BE%8B%E5%AD%90%E8%AE%B2%E8%A7%A3/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;regression tree&lt;/a&gt;  uses &lt;strong&gt;square error&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;tricks-in-decision-tree-construction&#34;&gt;Tricks in Decision Tree Construction&lt;/h2&gt;
&lt;h3 id=&#34;pruning&#34;&gt;Pruning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Pre-pruning&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The size of nodes is smaller than the threshold.&lt;/li&gt;
&lt;li&gt;The depth of the tree is deeper than the threshold.&lt;/li&gt;
&lt;li&gt;All the attributes have been checked.&lt;/li&gt;
&lt;li&gt;Accuracy on the validation set becomes lower after splitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Post-pruning&lt;/strong&gt; (&lt;a class=&#34;link&#34; href=&#34;https://wizardforcel.gitbooks.io/dm-algo-top10/content/cart.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Readings&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reduced-Error Pruning&lt;/li&gt;
&lt;li&gt;Pessimistic Error Pruning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;continuous-variables&#34;&gt;Continuous Variables&lt;/h3&gt;
&lt;p&gt;Since some attributes has a continuous property, we dealt with the continuous values according to the following manner. Suppose the attribute $ A $ has $ a_1, a_2, &amp;hellip; , a_n $, then $ v $ equals $ n − 1 $. The values are sorted in an ascending order to calculate each threshold.&lt;/p&gt;
&lt;p&gt;$$ threshold(k)=\frac{a_{k}+a_{k+1}}{2} $$&lt;/p&gt;
&lt;p&gt;, where $ a_{k}&amp;lt;a_{k+1} $ and $ 1 \leq k \leq n-1 $. The optimal split will be found when it achieves the maximum purity.&lt;/p&gt;
&lt;h3 id=&#34;missing-values&#34;&gt;Missing Values&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Case 1&lt;/strong&gt;:
When an attribute has missing values, we calulate the ASM without considering the data with missing values. Then, we normalize the ASM based on the proportion of data with missing values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2&lt;/strong&gt;:
We find a split criterion, but some data samples don&amp;rsquo;t have the value of that attribute. We distribute those data into sub-nodes based on the ratio of the size of those sub-nodes created by normal data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 3&lt;/strong&gt;:
When the testing data has missing values, let each abnormal data go through every branch and see which branch results in the highest probability.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Decision Tree Algorithm, Explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://aman.ai/cs229/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stanford CS229 ML Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.xoriant.com/blog/product-engineering/decision-trees-machine-learning-algorithm.html#:~:text=Introduction%20Decision%20Trees%20are%20a,namely%20decision%20nodes%20and%20leaves.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Decision Trees for Classification: A Machine Learning Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hack Earth = Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsai.net/p/programming/decision-trees-explained-with-a-practical-example-fe47872d3b53&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Decision Trees Explained With a Practical Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.tutorialspoint.com/what-is-attribute-selection-measures#:~:text=An%20attribute%20selection%20measure%20is,training%20tuples%20into%20single%20classes.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;What is Attribute Selection Measures?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.section.io/engineering-education/entropy-information-gain-machine-learning/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Entropy and Information Gain to Build Decision Trees in Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Topic 2: Logistic Regression</title>
        <link>https://tonyfpy.github.io/p/machine-learning-topic-2-logistic-regression/</link>
        <pubDate>Fri, 06 May 2022 21:25:32 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-topic-2-logistic-regression/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on May 6th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on May 7th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Logistic regression&lt;/strong&gt; is a process of modeling the probability of a discrete outcome given an input variable. The most common logistic regression models a &lt;strong&gt;binary&lt;/strong&gt; outcome. It’s an extension of the linear regression model for classification problems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hypothesis-representation&#34;&gt;Hypothesis Representation&lt;/h2&gt;
&lt;h3 id=&#34;logistic-function&#34;&gt;Logistic Function&lt;/h3&gt;
&lt;p&gt;Since $\theta^{T}x$ could be $ \ll 0 $ and $ \gg 1 $, which is not very natural for a binary classification problem with labels 0 or 1. Instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1.&lt;/p&gt;
&lt;p&gt;$$ g(z) = \frac{1}{1+e^{-z}} $$&lt;/p&gt;
&lt;p&gt;The deravative of the &lt;strong&gt;sigmoid&lt;/strong&gt; functin is:&lt;/p&gt;
&lt;p&gt;$$ \frac{\partial}{\partial z} g(z) = \frac{1}{(1+e^{-z})} \cdot(1-\frac{1}{(1+e^{-z})}) = g(z)\cdot(1-g(z)) $$&lt;/p&gt;
&lt;p&gt;From the plot of this function, we could see that &lt;strong&gt;$ g(z) $ starts off really close to 0, then rises and asymptotes towards 1&lt;/strong&gt;. Since we want our hypothesis to output values between 0 and 1, we could combine $ g(z) $ and $ h_{\theta}(x) $:&lt;/p&gt;
&lt;p&gt;$$ h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}} $$&lt;/p&gt;
&lt;p&gt;If $ h_{\theta}(x) &amp;gt;= 0.5 $, we could know $ y = 1 $, and vice versa.&lt;/p&gt;
&lt;h3 id=&#34;estimated-probability&#34;&gt;Estimated Probability&lt;/h3&gt;
&lt;p&gt;In the logistic regression model, the ouput of $ h_{\theta}(x) $ represents the estimated probability of $ y = 1 $ or $ y = 0 $. Thus, for a single training example $ (x^{(i)},y^{(i)}) $, we have:&lt;/p&gt;
&lt;p&gt;$$ P(y^{(i)}=1 \mid x^{(i)} ; \theta)=h_{\theta}(x^{(i)}) $$
$$ P(y^{(i)}=0 \mid x^{(i)} ; \theta)=1-h_{\theta}(x^{(i)}) $$&lt;/p&gt;
&lt;p&gt;Those can be written more compactly as a single equation as follows:&lt;/p&gt;
&lt;p&gt;$$ P(y^{(i)} \mid x^{(i)} ; \theta)=(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}} $$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cost-function&#34;&gt;Cost Function&lt;/h2&gt;
&lt;h3 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h3&gt;
&lt;p&gt;Assuming that the $m$ training examples were generated independently, we can express the likelihood of the parameters $L(\theta)$ as:&lt;/p&gt;
&lt;p&gt;$$
L(\theta) =\prod_{i=1}^{m} P(y^{(i)} \mid x^{(i)} ; \theta) =\prod_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
$$&lt;/p&gt;
&lt;p&gt;We can make the calculation simpler by applying logarithm:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta) =\log L(\theta) =\sum_{i=1}^{m} (y^{(i)} \log (h(x^{(i)})) + (1-y^{(i)}) \log (1-h(x^{(i)})))
$$&lt;/p&gt;
&lt;h3 id=&#34;cost-function-formation&#34;&gt;Cost Function Formation&lt;/h3&gt;
&lt;p&gt;Usually, in machine learning, we want to convert the &amp;ldquo;acsent&amp;rdquo; problem into a &amp;ldquo;descent&amp;rdquo; problem. Therefore,&lt;/p&gt;
&lt;p&gt;$$ J(\theta) = - \ell(\theta) = - \sum_{i=1}^{m} (y^{(i)} \log (h(x^{(i)})) + (1-y^{(i)}) \log (1-h(x^{(i)}))) $$&lt;/p&gt;
&lt;h3 id=&#34;update-rules&#34;&gt;Update Rules&lt;/h3&gt;
&lt;p&gt;Now, let&amp;rsquo;s take derivatives to derive the gradient descent rule:&lt;/p&gt;
&lt;p&gt;$$
\frac{\delta}{\delta \theta_{j}} J(\theta)=- \sum_{i=1}^{m}(y^{(i)} \frac{1}{h_{\theta}(x^{(i)})} \frac{\delta}{\delta \theta_{j}} h_{\theta}(x^{(i)})-(1-y^{(i)}) \frac{1}{1-h_{\theta}(x^{(i)})} \frac{\delta}{\delta \theta_{j}} h_{\theta}(x^{(i)}))
$$&lt;/p&gt;
&lt;p&gt;Becasue $ h_{\theta}(x) = g(\theta^{T}x) $, we have:&lt;/p&gt;
&lt;p&gt;$$
\frac{\delta}{\delta \theta_{j}} J(\theta)=- \sum_{i=1}^{m}(y^{(i)} \frac{1}{g(\theta^{\mathrm{T}} x^{(i)})}-(1-y^{(i)}) \frac{1}{1-g(\theta^{\mathrm{T}} x^{(i)})}) \frac{\delta}{\delta \theta_{j}} g(\theta^{\mathrm{T}} x^{(i)})
$$&lt;/p&gt;
&lt;p&gt;Previously, we have $ \frac{\partial}{\partial z} g(z) = g(z)\cdot(1-g(z)) $ and $ \frac{\partial}{\partial \theta_{j}} \theta^{T} x^{(i)} = x_{j}^{(i)} $. So,&lt;/p&gt;
&lt;p&gt;$$
\frac{\delta}{\delta \theta_{j}} J(\theta)=-\sum_{i=1}^{m}(y^{(i)} \frac{1}{g(z)}-(1-y^{(i)}) \frac{1}{1-g(z)}) g(z)(1-g(z)) x_{j}^{(i)}
$$&lt;/p&gt;
&lt;p&gt;Finally, we have&lt;/p&gt;
&lt;p&gt;$$
\frac{\delta}{\delta \theta_{j}} J(\theta)=(h_{\theta}(x^{(i)})-y) x_{j}^{(i)}
$$&lt;/p&gt;
&lt;p&gt;Thus, the gradient descent rule is&lt;/p&gt;
&lt;p&gt;$$ \theta_{j}\gets \theta_{j}-\alpha \cdot \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}) x_{j}^{(i)} $$&lt;/p&gt;
&lt;p&gt;If we compare this to the &lt;strong&gt;LMS update rule&lt;/strong&gt;, we see that it looks identical; but this is not the same algorithm, because  $ h_{\theta}\left(x^{(i)}\right) $ is now defined as a non-linear function of $ \theta^{T} x^{(i)} $.&lt;/p&gt;
&lt;p&gt;It might be a little surprising that we end up with the same update rule for a rather different algorithm and learning problem. This is actually not a coincidence, and is in fact a general property of a much bigger class of algorithms called &lt;strong&gt;generalized linear models&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://aman.ai/cs229/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stanford CS229 ML Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://christophm.github.io/interpretable-ml-book/logistic.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Interpretable ML - Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scruel.gitee.io/ml-andrewng-notes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Coursera ML Andrew Ng Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Topic 1: Linear Regression</title>
        <link>https://tonyfpy.github.io/p/machine-learning-topic-1-linear-regression/</link>
        <pubDate>Mon, 02 May 2022 23:09:54 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-topic-1-linear-regression/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on May 2nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on May 5th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt; is studied as a model for understanding the relationship between input and output numerical variables. It is classified as a supervised learning problem, whose goal is to learn a function/hypothesis $ h: X \rarr Y $ so that $ h(x) $ is a “good” predictor for the corresponding value of $ y $.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$ x^{(i)} $ denotes the input variables or features, and $ y^{(i)} $ denotes the output variable that we’re trying to predict, where the superscript $ i $ denotes the $ i^{th} $ training sample.&lt;/li&gt;
&lt;li&gt;A pair $ (x^{(i)}, y^{(i)}) $ denotes a single training example.&lt;/li&gt;
&lt;li&gt;$ x^{(i)}_j $ represents the $ j^{th} $ feature of the $ i^{th} $ training sample.&lt;/li&gt;
&lt;li&gt;$ m $ denotes &lt;strong&gt;the number of training examples&lt;/strong&gt;, while $ n $ denotes &lt;strong&gt;the number of features&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hypothesis-function&#34;&gt;Hypothesis Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$ h_{\theta}(x) $ denotes the hypothesis function, which is a linear function of the features $ x $ and $ x_0=1 $.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cost-function&#34;&gt;Cost Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$ J_{\theta} $ denotes the cost function that we want to minimize by finding the optimal set of parameters $ \theta $.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hypothesis-function-1&#34;&gt;Hypothesis Function&lt;/h2&gt;
&lt;h3 id=&#34;initial-representation&#34;&gt;Initial Representation&lt;/h3&gt;
&lt;p&gt;Given a set of data, let&amp;rsquo;s say we want to approximate $ y $ as a linear function of $ x $:&lt;/p&gt;
&lt;p&gt;$$ h_{\theta}(x) = {\theta}_0 + {\theta}_1x_1 + {\theta}_2x_2 + &amp;hellip; + {\theta}_nx_n $$&lt;/p&gt;
&lt;p&gt;, where the $ {\theta}_j $&amp;rsquo;s are the parameters (also called &lt;strong&gt;weights&lt;/strong&gt;) parameterizing the space of linear functions mapping from $ X $ to $ Y $.&lt;/p&gt;
&lt;h3 id=&#34;representation-improvement&#34;&gt;Representation Improvement&lt;/h3&gt;
&lt;p&gt;To make it simplied, we introduce an &lt;strong&gt;intercept term&lt;/strong&gt; $ x_0=1 $ such that:&lt;/p&gt;
&lt;p&gt;$$ h(x) =  \sum_{j=0}^{n} {\theta}_jx_j = {\theta}^Tx $$&lt;/p&gt;
&lt;p&gt;, where on the right-hand side above we are viewing $ {\theta} $ and $ x $ both as vectors.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cost-function-1&#34;&gt;Cost Function&lt;/h2&gt;
&lt;h3 id=&#34;error-interpretation&#34;&gt;Error Interpretation&lt;/h3&gt;
&lt;p&gt;The hypothesis may not capture unmodeled effects or random noise, so we can say:&lt;/p&gt;
&lt;p&gt;$$ y^{(i)} = {\theta}^Tx^{(i)} + {\epsilon}^{(i)} $$&lt;/p&gt;
&lt;p&gt;Let’s further assume that $ {\epsilon}^{(i)} $ follows a &lt;strong&gt;Gaussian distribution (Normal distribution)&lt;/strong&gt; with mean $ {\mu}=0 $ and variance $ {\sigma}^2 $. This could be expressed as $ {\epsilon}^{(i)} {\sim} N(0, {\sigma}^2)$. This implies that the probability density of $ {\epsilon}^{(i)} $ follows a Gaussian density function as follows:&lt;/p&gt;
&lt;p&gt;$$ P({\epsilon}^{(i)}) = \frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{({\epsilon}^{(i)})^2}{2 \sigma^{2}}) $$&lt;/p&gt;
&lt;p&gt;Another assumption we’re going to make is that the $ {\epsilon}^{(i)} $ error terms are &lt;strong&gt;IID&lt;/strong&gt;, which in statistics stands for &lt;strong&gt;independently and identically distributed&lt;/strong&gt;. Under these set of assumptions, the probability density of $ y^{(i)} $ given $ x^{(i)} $ and parameterized by $ {\theta} $:&lt;/p&gt;
&lt;p&gt;$$ P(y^{(i)} \mid x^{(i)} ; \theta)=\frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}}) $$&lt;/p&gt;
&lt;p&gt;In other words, given $ x^{(i)} $ and $ {\theta} $, the random variable $ y^{(i)} $ is sampled from a Gaussian Distribution $ N({\theta}^Tx^{(i)}, {\sigma}^{2}) $.&lt;/p&gt;
&lt;h3 id=&#34;likelihood-function&#34;&gt;Likelihood Function&lt;/h3&gt;
&lt;p&gt;Given the input features matrix (design matrix) $ X $ (All $ x^{(i)} $ and $ {\theta} $), the probability distribution of $ \vec{y} $ (All $ y^{(i)} $’s) is given by $ P(\vec{y}∣X;{\theta}) $. The likelikhood of the parameters $ L({\theta}) $ is defined as:&lt;/p&gt;
&lt;p&gt;$$ L(\theta) =P(\vec{y} \mid X ; \theta) =\prod_{i=1}^{m} P(y^{(i)} \mid x^{(i)} ; \theta) $$&lt;/p&gt;
&lt;p&gt;Now, we add details into the above equation,
$$
L(\theta)=\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}})
$$&lt;/p&gt;
&lt;p&gt;The principal of &lt;strong&gt;Maximum Likelihood Estimation (MLE)&lt;/strong&gt; states that we should choose $ {\theta} $ so as to make the data as high probability as possible. The derivations will be a bit simpler if we instead maximize the log likelihood $ {\ell(\theta)} $:&lt;/p&gt;
&lt;p&gt;$$
{\ell(\theta)}=ln(\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}}))
$$&lt;/p&gt;
&lt;p&gt;According to the product rule of logarithms,&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta)=\sum_{i=1}^{m} ln \frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}})
$$&lt;/p&gt;
&lt;p&gt;Again, the log product rule shows us:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta)=\sum_{i=1}^{m} \left[ln \frac{1}{\sqrt{2 \pi} \sigma} + ln(exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}})\right]
$$&lt;/p&gt;
&lt;p&gt;This could be expanded as:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta)= m {\cdot} ln \frac{1}{\sqrt{2 \pi} \sigma} - \frac{1}{\sigma^{2}} {\cdot} \frac{1}{2} {\cdot} \sum_{i=1}^{m} (y^{(i)}-\theta^{T} x^{(i)})^{2}
$$&lt;/p&gt;
&lt;h3 id=&#34;cost-function-formation&#34;&gt;Cost Function Formation&lt;/h3&gt;
&lt;p&gt;From the probability interpretation above, we could regard $ \ell(\theta) $ as:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta)= C_1 - C_2 {\cdot} \frac{1}{2} {\cdot} \sum_{i=1}^{m} (y^{(i)}-\theta^{T} x^{(i)})^{2}
$$&lt;/p&gt;
&lt;p&gt;, where $ C_1 $ and $ C_2 $ are constant. Hence, maximizing $ \ell(\theta) $ is equivalent to minimizing $ J(\theta) $:&lt;/p&gt;
&lt;p&gt;$$
J(\theta)= \frac{1}{2} {\cdot} \sum_{i=1}^{m} (y^{(i)}-\theta^{T} x^{(i)})^{2}
$$&lt;/p&gt;
&lt;p&gt;, where $ 1/2 $ is a constant that helps cancel 2 in derivative of the function.&lt;/p&gt;
&lt;p&gt;The formation above shows that choosing the value of $ \theta $ to minimize the &lt;strong&gt;least squares error cost function&lt;/strong&gt;, is equivalent to finding the &lt;strong&gt;MLE&lt;/strong&gt; for the parameters $ \theta $ under the set of assumptions that the error terms $ {\epsilon}^{(i)} $ are &lt;strong&gt;Gaussian&lt;/strong&gt; and &lt;strong&gt;IID&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cost-function-minimization&#34;&gt;Cost Function Minimization&lt;/h2&gt;
&lt;h3 id=&#34;least-mean-square&#34;&gt;Least Mean Square&lt;/h3&gt;
&lt;p&gt;To find a set of parameters, one way is to find the optimum that minimizes the cost function. Therefore, we need $ \nabla_{\theta} J(\theta)= 0 $. More specifically, $ \frac{\partial J}{\partial \theta_{1}} = 0, \frac{\partial J}{\partial \theta_{2}} = 0, \frac{\partial J}{\partial \theta_{3}} = 0, &amp;hellip;, \frac{\partial J}{\partial \theta_{n}} = 0 $.&lt;/p&gt;
&lt;p&gt;However, this may generate $ n $ equations and is computationally expensive if there are many features in the dataset. We need to find other ways.&lt;/p&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h3&gt;
&lt;p&gt;We can start with some “initial guess” for $ {\theta} $, and that repeatedly changes $ {\theta} $ to make $ J(\theta) $ smaller, until hopefully we converge to a value of $ {\theta} $ that minimizes $ J(\theta) $. For $ j = 0, 1, 2, &amp;hellip;, n $, we &lt;strong&gt;simultaneously&lt;/strong&gt; update the $ {\theta} $ and this process will be kept until convergence:&lt;/p&gt;
&lt;p&gt;$$ \theta _{j} \gets \theta _{j} - \alpha \cdot \frac{\partial J}{\partial \theta _{j}}  $$&lt;/p&gt;
&lt;p&gt;Here, $ \alpha$ is the learning rate, a hyperparameter to control the searching speed. Now, we put all the equations together and we coud see:&lt;/p&gt;
&lt;p&gt;$$ \theta_{j}\gets \theta_{j}-\alpha \cdot \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}) x_{j}^{(i)} $$&lt;/p&gt;
&lt;p&gt;This is the &lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt; strategy, because we update the parameters according to the gradient of the error with respect to a single training example only. Beside, there are other strategies, such as &lt;strong&gt;Batch GD&lt;/strong&gt;, &lt;strong&gt;Mini-batch GD&lt;/strong&gt;, etc, which will be discussed in my future posts.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;normal-equation&#34;&gt;Normal Equation&lt;/h2&gt;
&lt;p&gt;In this method, we will minimize $ J(\theta) $ by explicitly taking its derivatives with respect to the $ {\theta}_j $’s, and setting them to zero.&lt;/p&gt;
&lt;p&gt;Given a training set, define the design matrix $ X $ to be the $ m×n $, and let $ y $ to be the $ m×1 $ vector containing all the target values from the training set. We can obtain:&lt;/p&gt;
&lt;p&gt;$$
J(\theta)= \frac{1}{2} {\cdot} \sum_{i=1}^{m} (y^{(i)}-\theta^{T} x^{(i)})^{2} = \frac{1}{2} || X\theta-y || ^2 = \frac{1}{2} (X \theta-y)^{T}(X \theta-y)
$$&lt;/p&gt;
&lt;p&gt;Now, we expand the equation:&lt;/p&gt;
&lt;p&gt;$$
J(\theta)= \frac{1}{2} (\theta^{T} X^{T} X \theta-\theta^{T} X^{T} y-y^{T} X \theta + y^{T} y)
$$&lt;/p&gt;
&lt;p&gt;Since $ \theta^{T} X^{T} y $ and $ y^{T} X \theta $ are scalars, so they are equivalent. We can now simplify the equation as:&lt;/p&gt;
&lt;p&gt;$$
J(\theta)= \frac{1}{2} (\theta^{T} X^{T} X \theta-2\theta^{T} X^{T} y + y^{T} y)
$$&lt;/p&gt;
&lt;p&gt;Accoding to the rules of derivative of matrix, $ \frac{d B^{T} A B}{d B}=(A+A^{\mathrm{T}}) B $ and $ \frac{d B^{T} A}{d B}=A $, we can find its derivatives with respect to $ \theta $:&lt;/p&gt;
&lt;p&gt;$$
\nabla_{\theta} J(\theta)= \frac{1}{2} ((X^TX + (X^TX)^T)\theta - 2X^Ty) = \frac{1}{2} (2X^TX\theta - 2X^Ty)
$$&lt;/p&gt;
&lt;p&gt;Now, we set $ \nabla_{\theta} J(\theta) $ to zero:&lt;/p&gt;
&lt;p&gt;$$
X^TX\theta = X^Ty
$$&lt;/p&gt;
&lt;p&gt;Thus, the value of $ \theta $ that minimizes $ J(\theta) $ is given in closed form by the equation:&lt;/p&gt;
&lt;p&gt;$$
\theta=(X^{T} X)^{-1} X^{T} y
$$&lt;/p&gt;
&lt;p&gt;This approach requires the matrix to be &lt;strong&gt;inversible&lt;/strong&gt;. Usually, if some features have linear relations or the size of training samples is smaller than the number of features, $ X^TX $ is not inversible.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://aman.ai/cs229/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stanford CS229 ML Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.seldon.io/machine-learning-regression-explained#:~:text=Regression%20is%20a%20technique%20for,used%20to%20predict%20continuous%20outcomes.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Machine Learning Regression Explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://machinelearningmastery.com/linear-regression-for-machine-learning/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Linear Regression for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Logistic Regression — Detailed Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/swlh/ml-fundamentals-what-is-cost-function-d396129cc611&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ML Fundamentals: What Is Cost Function?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scruel.gitee.io/ml-andrewng-notes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Coursera ML Andrew Ng Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
