<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Machine Learning on Tony Feng</title>
        <link>https://tonyfpy.github.io/tags/machine-learning/</link>
        <description>Recent content in Machine Learning on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 18 May 2022 22:45:07 +0800</lastBuildDate><atom:link href="https://tonyfpy.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Machine Learning] Topic 4: K-Nearest Neighbours</title>
        <link>https://tonyfpy.github.io/p/machine-learning-topic-4-k-nearest-neighbours/</link>
        <pubDate>Wed, 18 May 2022 22:45:07 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-topic-4-k-nearest-neighbours/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on May 18th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on May 18th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;K-nearest neighbors (KNN)&lt;/strong&gt; is a type of &lt;strong&gt;supervised&lt;/strong&gt; learning algorithm used for both &lt;strong&gt;regression&lt;/strong&gt; and &lt;strong&gt;classification&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In classification, it tries to predict the correct class for the test data by a &lt;strong&gt;majority vote&lt;/strong&gt; from K amount of neighbours to classify.&lt;/p&gt;
&lt;p&gt;In the case of regression, the value is the &lt;strong&gt;mean&lt;/strong&gt; of the K selected training points.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;referrence&#34;&gt;Referrence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;K-Nearest Neighbor - Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;K-Nearest Neighbor(KNN) Algorithm for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.datadriveninvestor.com/k-nearest-neighbours-knn-a9f8ba09cb8b&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Intro to K-Nearest Neighbours (KNN) — Machine Learning 101&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Topic 3: Decisoin Tree</title>
        <link>https://tonyfpy.github.io/p/machine-learning-topic-3-decisoin-tree/</link>
        <pubDate>Wed, 11 May 2022 20:51:10 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-topic-3-decisoin-tree/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on May 11th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on May 14th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;A decision tree is one of the supervised machine learning algorithms. Decision Tree algorithm belongs to the family of supervised learning algorithms. The goal is to create a model that predicts the value of a target variable by learning &lt;strong&gt;if-else conditions(decision rules)&lt;/strong&gt; inferred from the data features.&lt;/p&gt;
&lt;h3 id=&#34;types-of-decision-tree&#34;&gt;Types of Decision Tree&lt;/h3&gt;
&lt;p&gt;Decision Tree (DT) can be used for both &lt;strong&gt;regression&lt;/strong&gt; and &lt;strong&gt;classification&lt;/strong&gt; problems, but it is mostly used for classification problems. Based on the types of target variable, we have 1) &lt;strong&gt;categorical-variable&lt;/strong&gt; DT, and 2) &lt;strong&gt;continuous-variable&lt;/strong&gt; DT.&lt;/p&gt;
&lt;h3 id=&#34;non-linearity&#34;&gt;Non-linearity&lt;/h3&gt;
&lt;p&gt;Formally, a method is &lt;strong&gt;linear&lt;/strong&gt; if, for an input $ x \isin {\R}^n $ (with interecept term $ x_0=1 $), it only produces hypothesis functions $ h $ of the form:&lt;/p&gt;
&lt;p&gt;$$ h(x) = \theta^{T}x, \theta \isin {\R}^n $$&lt;/p&gt;
&lt;p&gt;Otherwise, it is &lt;strong&gt;non-linear&lt;/strong&gt;. Decision trees, on the other hand, can directly produce non-linear hypothesis functions without the need for first coming up with an appropriate feature mapping.&lt;/p&gt;
&lt;h3 id=&#34;region-selection&#34;&gt;Region Selection&lt;/h3&gt;
&lt;p&gt;In general, selecting optimal regions is intractable. Decision trees generate an approximate solution via &lt;strong&gt;greedy&lt;/strong&gt;, &lt;strong&gt;top-down&lt;/strong&gt;, &lt;strong&gt;recursive&lt;/strong&gt; partitioning.  Each node in the tree acts as a &lt;strong&gt;test case&lt;/strong&gt; for some &lt;strong&gt;attribute&lt;/strong&gt;, and each edge descending from that node corresponds to one of the possible answers to the test case.&lt;/p&gt;
&lt;p&gt;Formally, given a input space $ X $, a parent region $ R_p $, a feature index $ j $, and a threshold $ t \isin {\R}$, we obtain two child regions $R_1$  and $R_2$ as follows:&lt;/p&gt;
&lt;p&gt;$$ R_{1} = \{X \mid X_{j}&amp;lt;t, X \in R_{p} \} $$ $$ R_{2} = \{X \mid X_{j} \geq t, X \in R_{p} \} $$&lt;/p&gt;
&lt;p&gt;We can continue in such a manner until we a meet a &lt;strong&gt;pre-defined stop criterion&lt;/strong&gt;, and then predict the majority class at each leaf node.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;attribute-selective-measures&#34;&gt;Attribute Selective Measures&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;attribute selection measure (AMS)&lt;/strong&gt; is a technique used in the data mining process for data reduction. It is a &lt;strong&gt;heuristic&lt;/strong&gt; for choosing the splitting test that “best” separates a given data partition. The three main ASM techniques are 1) &lt;strong&gt;Information Gain&lt;/strong&gt;, 2) &lt;strong&gt;Gain Ratio&lt;/strong&gt;, and 3) &lt;strong&gt;Gini Index&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;information-gain&#34;&gt;Information Gain&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Entropy&lt;/strong&gt; is used to measure the level of &lt;strong&gt;impurity&lt;/strong&gt; in a group of examples. This concept comes from informatin theory. The higher the entropy, the more the information content. Now, consider a dataset with $n$ classes, then&lt;/p&gt;
&lt;p&gt;$$ E = -\sum_{i=1}^{n} p_{i} \log_{2} (p_{i}) $$&lt;/p&gt;
&lt;p&gt;, where $ p_i $ is the probability of randomly selecting an example in class $i$.&lt;/p&gt;
&lt;p&gt;We can define &lt;strong&gt;information gain (IG)&lt;/strong&gt; as a measure of how much information a feature provides about a class. It is the expected reduction in entropy caused by partitioning the examples according to a given attribute. Given a collection of dataset $ S $, we can calculate the $IG(S, A)$ of an attribute $A$ as:&lt;/p&gt;
&lt;p&gt;$$ IG(S, A) = E(S) - \sum_{v \in V} \frac{|S_{v}|}{|S|} E(S_{v}) $$&lt;/p&gt;
&lt;p&gt;, where $V$ is all possible values for attribute $A$ and $S_{v}$ is the subset of $S$ for which attribute $A$ has value $v$.&lt;/p&gt;
&lt;h3 id=&#34;gain-ratio&#34;&gt;Gain Ratio&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Gain Ratio&lt;/strong&gt; or &lt;strong&gt;Uncertainty Coefficient&lt;/strong&gt; is used to normalize the information gain of an attribute against how much entropy that attribute has. It attempts to lessen the bias of Information Gain on highly branched predictors by introducing a normalizing term called the &lt;strong&gt;Intrinsic Information (II)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$ II(S, A) = - \sum_{v \in V(A)} \frac{|S_{v}|}{|S|} \log_2 \frac{|s_{v}|}{|S|} $$&lt;/p&gt;
&lt;p&gt;Formula of Gain Ration is given by&lt;/p&gt;
&lt;p&gt;$$ GR(S, A) = \frac{IG(S, A)}{II(S, A)} $$&lt;/p&gt;
&lt;p&gt;From this formula, we can see that, with less value in attribute $ A $, $ II(S, A)$ is smaller and thus purity is higher. Informally, the formula of Intrinsic Information is the same as that of Entropy, both of which means the purity of attribute $ A $.&lt;/p&gt;
&lt;h3 id=&#34;gini-index&#34;&gt;Gini Index&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Gini Index&lt;/strong&gt;, also called &lt;strong&gt;Gini Impurity&lt;/strong&gt;, measures the degree or probability of a particular variable being &lt;strong&gt;wrongly&lt;/strong&gt; classified when it is randomly chosen. If we have $C$ total classes and $p_i$ is the probability of picking a datapoint with class $i$, then the Gini Impurity is calculated as&lt;/p&gt;
&lt;p&gt;$$ GI=\sum_{i=1}^{C} p_i(1-p_i) = 1 - \sum_{i=1}^{C} p_i^{2}$$&lt;/p&gt;
&lt;p&gt;For a data sample $ S $, we have an attribute $ A $ with a group of values $V$. Therefore,&lt;/p&gt;
&lt;p&gt;$$ GI(S, A) = \sum_{v \in V} \frac{|S_{v}|}{|S|} GI(S_{v}) $$&lt;/p&gt;
&lt;p&gt;The Gini Index varies between $0$ and $1$, where $0$ represents purity of the classification and $1$ denotes random distribution of elements among various classes. A Gini Index of $0.5$ shows that there is &lt;strong&gt;equal distribution&lt;/strong&gt; of elements across some classes.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;feature-selection-algorithms&#34;&gt;Feature Selection Algorithms&lt;/h2&gt;
&lt;h3 id=&#34;id3&#34;&gt;ID3&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;ID3&lt;/strong&gt; algorithm builds decision trees using a &lt;strong&gt;top-down greedy search&lt;/strong&gt; approach through the space of possible branches with no backtracking.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps of ID3&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Begin with the original datset as the root node and calculte its entropy.&lt;/li&gt;
&lt;li&gt;For each attribute/feature:
&lt;ul&gt;
&lt;li&gt;Calculate entropy for all its categorical values.&lt;/li&gt;
&lt;li&gt;Calculate information gain for the feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Find the feature with &lt;strong&gt;maximum information gain&lt;/strong&gt; and split to produce subsets.&lt;/li&gt;
&lt;li&gt;Continues to recur on each subset until we get the desired tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ID3 has no pruning strategy and is easy to &lt;strong&gt;overfit&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The IG criterion has a preference for features with a number of possible values.&lt;/li&gt;
&lt;li&gt;It can only be used to deal with &lt;strong&gt;discretely&lt;/strong&gt; distributed features;&lt;/li&gt;
&lt;li&gt;It does not consider missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;c45&#34;&gt;C4.5&lt;/h3&gt;
&lt;p&gt;Compared with the shortcomings of ID3, C4.5 has the following improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C4.5 overcomes is the shortcoming of ID3&amp;rsquo;s emphasis on the number of feature by using the &lt;strong&gt;Gain Ratio&lt;/strong&gt; as the goodness function to split the dataset.&lt;/li&gt;
&lt;li&gt;C4.5 is able to handle both &lt;strong&gt;continuous&lt;/strong&gt; and &lt;strong&gt;discrete&lt;/strong&gt; attributes.&lt;/li&gt;
&lt;li&gt;C4.5 has a &lt;strong&gt;pruning&lt;/strong&gt; strategy, which replaces the helpless branches with leaf nodes after the tree is created.&lt;/li&gt;
&lt;li&gt;C4.5 takes &lt;strong&gt;missing values&lt;/strong&gt; into account.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C4.5 uses a &lt;strong&gt;polytree&lt;/strong&gt; which is less efficient than a binary tree.&lt;/li&gt;
&lt;li&gt;C4.5 can only be used for &lt;strong&gt;classification&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;C4.5 is &lt;strong&gt;computaionally expensive&lt;/strong&gt; both in time and space.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cart&#34;&gt;CART&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Classification And Regression Trees (CART)&lt;/strong&gt; is a recursive partitioning method, builds classification and regression trees for predicting &lt;strong&gt;continuous dependent variables (regression)&lt;/strong&gt; and &lt;strong&gt;categorical predictor variables (classification)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a class=&#34;link&#34; href=&#34;http://rollingdeep.github.io/2020/11/02/CART%E5%88%86%E7%B1%BB%E6%A0%91%E4%BE%8B%E5%AD%90%E8%AE%B2%E8%A7%A3/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;classification algorithm&lt;/a&gt; for building a decision tree is based on &lt;strong&gt;Gini impurity&lt;/strong&gt; as splitting criterion, while the &lt;a class=&#34;link&#34; href=&#34;http://rollingdeep.github.io/2020/11/02/CART%E5%9B%9E%E5%BD%92%E6%A0%91%E4%BE%8B%E5%AD%90%E8%AE%B2%E8%A7%A3/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;regression tree&lt;/a&gt;  uses &lt;strong&gt;square error&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;tricks-in-decision-tree-construction&#34;&gt;Tricks in Decision Tree Construction&lt;/h2&gt;
&lt;h3 id=&#34;pruning&#34;&gt;Pruning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Pre-pruning&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The size of nodes is smaller than the threshold.&lt;/li&gt;
&lt;li&gt;The depth of the tree is deeper than the threshold.&lt;/li&gt;
&lt;li&gt;All the attributes have been checked.&lt;/li&gt;
&lt;li&gt;Accuracy on the validation set becomes lower after splitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Post-pruning&lt;/strong&gt; (&lt;a class=&#34;link&#34; href=&#34;https://wizardforcel.gitbooks.io/dm-algo-top10/content/cart.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Readings&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reduced-Error Pruning&lt;/li&gt;
&lt;li&gt;Pessimistic Error Pruning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;continuous-variables&#34;&gt;Continuous Variables&lt;/h3&gt;
&lt;p&gt;Since some attributes has a continuous property, we dealt with the continuous values according to the following manner. Suppose the attribute $ A $ has $ a_1, a_2, &amp;hellip; , a_n $, then $ v $ equals $ n − 1 $. The values are sorted in an ascending order to calculate each threshold.&lt;/p&gt;
&lt;p&gt;$$ threshold(k)=\frac{a_{k}+a_{k+1}}{2} $$&lt;/p&gt;
&lt;p&gt;, where $ a_{k}&amp;lt;a_{k+1} $ and $ 1 \leq k \leq n-1 $. The optimal split will be found when it achieves the maximum purity.&lt;/p&gt;
&lt;h3 id=&#34;missing-values&#34;&gt;Missing Values&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Case 1&lt;/strong&gt;:
When an attribute has missing values, we calulate the ASM without considering the data with missing values. Then, we normalize the ASM based on the proportion of data with missing values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2&lt;/strong&gt;:
We find a split criterion, but some data samples don&amp;rsquo;t have the value of that attribute. We distribute those data into sub-nodes based on the ratio of the size of those sub-nodes created by normal data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 3&lt;/strong&gt;:
When the testing data has missing values, let each abnormal data go through every branch and see which branch results in the highest probability.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Decision Tree Algorithm, Explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://aman.ai/cs229/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stanford CS229 ML Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.xoriant.com/blog/product-engineering/decision-trees-machine-learning-algorithm.html#:~:text=Introduction%20Decision%20Trees%20are%20a,namely%20decision%20nodes%20and%20leaves.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Decision Trees for Classification: A Machine Learning Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hack Earth = Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsai.net/p/programming/decision-trees-explained-with-a-practical-example-fe47872d3b53&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Decision Trees Explained With a Practical Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.tutorialspoint.com/what-is-attribute-selection-measures#:~:text=An%20attribute%20selection%20measure%20is,training%20tuples%20into%20single%20classes.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;What is Attribute Selection Measures?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.section.io/engineering-education/entropy-information-gain-machine-learning/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Entropy and Information Gain to Build Decision Trees in Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Topic 2: Logistic Regression</title>
        <link>https://tonyfpy.github.io/p/machine-learning-topic-2-logistic-regression/</link>
        <pubDate>Fri, 06 May 2022 21:25:32 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-topic-2-logistic-regression/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on May 6th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on May 7th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Logistic regression&lt;/strong&gt; is a process of modeling the probability of a discrete outcome given an input variable. The most common logistic regression models a &lt;strong&gt;binary&lt;/strong&gt; outcome. It’s an extension of the linear regression model for classification problems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hypothesis-representation&#34;&gt;Hypothesis Representation&lt;/h2&gt;
&lt;h3 id=&#34;logistic-function&#34;&gt;Logistic Function&lt;/h3&gt;
&lt;p&gt;Since $\theta^{T}x$ could be $ \ll 0 $ and $ \gg 1 $, which is not very natural for a binary classification problem with labels 0 or 1. Instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1.&lt;/p&gt;
&lt;p&gt;$$ g(z) = \frac{1}{1+e^{-z}} $$&lt;/p&gt;
&lt;p&gt;The deravative of the &lt;strong&gt;sigmoid&lt;/strong&gt; functin is:&lt;/p&gt;
&lt;p&gt;$$ \frac{\partial}{\partial z} g(z) = \frac{1}{(1+e^{-z})} \cdot(1-\frac{1}{(1+e^{-z})}) = g(z)\cdot(1-g(z)) $$&lt;/p&gt;
&lt;p&gt;From the plot of this function, we could see that &lt;strong&gt;$ g(z) $ starts off really close to 0, then rises and asymptotes towards 1&lt;/strong&gt;. Since we want our hypothesis to output values between 0 and 1, we could combine $ g(z) $ and $ h_{\theta}(x) $:&lt;/p&gt;
&lt;p&gt;$$ h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}} $$&lt;/p&gt;
&lt;p&gt;If $ h_{\theta}(x) &amp;gt;= 0.5 $, we could know $ y = 1 $, and vice versa.&lt;/p&gt;
&lt;h3 id=&#34;estimated-probability&#34;&gt;Estimated Probability&lt;/h3&gt;
&lt;p&gt;In the logistic regression model, the ouput of $ h_{\theta}(x) $ represents the estimated probability of $ y = 1 $ or $ y = 0 $. Thus, for a single training example $ (x^{(i)},y^{(i)}) $, we have:&lt;/p&gt;
&lt;p&gt;$$ P(y^{(i)}=1 \mid x^{(i)} ; \theta)=h_{\theta}(x^{(i)}) $$
$$ P(y^{(i)}=0 \mid x^{(i)} ; \theta)=1-h_{\theta}(x^{(i)}) $$&lt;/p&gt;
&lt;p&gt;Those can be written more compactly as a single equation as follows:&lt;/p&gt;
&lt;p&gt;$$ P(y^{(i)} \mid x^{(i)} ; \theta)=(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}} $$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cost-function&#34;&gt;Cost Function&lt;/h2&gt;
&lt;h3 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h3&gt;
&lt;p&gt;Assuming that the $m$ training examples were generated independently, we can express the likelihood of the parameters $L(\theta)$ as:&lt;/p&gt;
&lt;p&gt;$$
L(\theta) =\prod_{i=1}^{m} P(y^{(i)} \mid x^{(i)} ; \theta) =\prod_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
$$&lt;/p&gt;
&lt;p&gt;We can make the calculation simpler by applying logarithm:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta) =\log L(\theta) =\sum_{i=1}^{m} (y^{(i)} \log (h(x^{(i)})) + (1-y^{(i)}) \log (1-h(x^{(i)})))
$$&lt;/p&gt;
&lt;h3 id=&#34;cost-function-formation&#34;&gt;Cost Function Formation&lt;/h3&gt;
&lt;p&gt;Usually, in machine learning, we want to convert the &amp;ldquo;acsent&amp;rdquo; problem into a &amp;ldquo;descent&amp;rdquo; problem. Therefore,&lt;/p&gt;
&lt;p&gt;$$ J(\theta) = - \ell(\theta) = - \sum_{i=1}^{m} (y^{(i)} \log (h(x^{(i)})) + (1-y^{(i)}) \log (1-h(x^{(i)}))) $$&lt;/p&gt;
&lt;h3 id=&#34;update-rules&#34;&gt;Update Rules&lt;/h3&gt;
&lt;p&gt;Now, let&amp;rsquo;s take derivatives to derive the gradient descent rule:&lt;/p&gt;
&lt;p&gt;$$
\frac{\delta}{\delta \theta_{j}} J(\theta)=- \sum_{i=1}^{m}(y^{(i)} \frac{1}{h_{\theta}(x^{(i)})} \frac{\delta}{\delta \theta_{j}} h_{\theta}(x^{(i)})-(1-y^{(i)}) \frac{1}{1-h_{\theta}(x^{(i)})} \frac{\delta}{\delta \theta_{j}} h_{\theta}(x^{(i)}))
$$&lt;/p&gt;
&lt;p&gt;Becasue $ h_{\theta}(x) = g(\theta^{T}x) $, we have:&lt;/p&gt;
&lt;p&gt;$$
\frac{\delta}{\delta \theta_{j}} J(\theta)=- \sum_{i=1}^{m}(y^{(i)} \frac{1}{g(\theta^{\mathrm{T}} x^{(i)})}-(1-y^{(i)}) \frac{1}{1-g(\theta^{\mathrm{T}} x^{(i)})}) \frac{\delta}{\delta \theta_{j}} g(\theta^{\mathrm{T}} x^{(i)})
$$&lt;/p&gt;
&lt;p&gt;Previously, we have $ \frac{\partial}{\partial z} g(z) = g(z)\cdot(1-g(z)) $ and $ \frac{\partial}{\partial \theta_{j}} \theta^{T} x^{(i)} = x_{j}^{(i)} $. So,&lt;/p&gt;
&lt;p&gt;$$
\frac{\delta}{\delta \theta_{j}} J(\theta)=-\sum_{i=1}^{m}(y^{(i)} \frac{1}{g(z)}-(1-y^{(i)}) \frac{1}{1-g(z)}) g(z)(1-g(z)) x_{j}^{(i)}
$$&lt;/p&gt;
&lt;p&gt;Finally, we have&lt;/p&gt;
&lt;p&gt;$$
\frac{\delta}{\delta \theta_{j}} J(\theta)=(h_{\theta}(x^{(i)})-y) x_{j}^{(i)}
$$&lt;/p&gt;
&lt;p&gt;Thus, the gradient descent rule is&lt;/p&gt;
&lt;p&gt;$$ \theta_{j}\gets \theta_{j}-\alpha \cdot \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}) x_{j}^{(i)} $$&lt;/p&gt;
&lt;p&gt;If we compare this to the &lt;strong&gt;LMS update rule&lt;/strong&gt;, we see that it looks identical; but this is not the same algorithm, because  $ h_{\theta}\left(x^{(i)}\right) $ is now defined as a non-linear function of $ \theta^{T} x^{(i)} $.&lt;/p&gt;
&lt;p&gt;It might be a little surprising that we end up with the same update rule for a rather different algorithm and learning problem. This is actually not a coincidence, and is in fact a general property of a much bigger class of algorithms called &lt;strong&gt;generalized linear models&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://aman.ai/cs229/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stanford CS229 ML Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://christophm.github.io/interpretable-ml-book/logistic.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Interpretable ML - Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scruel.gitee.io/ml-andrewng-notes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Coursera ML Andrew Ng Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Machine Learning] Topic 1: Linear Regression</title>
        <link>https://tonyfpy.github.io/p/machine-learning-topic-1-linear-regression/</link>
        <pubDate>Mon, 02 May 2022 23:09:54 +0800</pubDate>
        
        <guid>https://tonyfpy.github.io/p/machine-learning-topic-1-linear-regression/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on May 2nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on May 5th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt; is studied as a model for understanding the relationship between input and output numerical variables. It is classified as a supervised learning problem, whose goal is to learn a function/hypothesis $ h: X \rarr Y $ so that $ h(x) $ is a “good” predictor for the corresponding value of $ y $.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$ x^{(i)} $ denotes the input variables or features, and $ y^{(i)} $ denotes the output variable that we’re trying to predict, where the superscript $ i $ denotes the $ i^{th} $ training sample.&lt;/li&gt;
&lt;li&gt;A pair $ (x^{(i)}, y^{(i)}) $ denotes a single training example.&lt;/li&gt;
&lt;li&gt;$ x^{(i)}_j $ represents the $ j^{th} $ feature of the $ i^{th} $ training sample.&lt;/li&gt;
&lt;li&gt;$ m $ denotes &lt;strong&gt;the number of training examples&lt;/strong&gt;, while $ n $ denotes &lt;strong&gt;the number of features&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hypothesis-function&#34;&gt;Hypothesis Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$ h_{\theta}(x) $ denotes the hypothesis function, which is a linear function of the features $ x $ and $ x_0=1 $.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cost-function&#34;&gt;Cost Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$ J_{\theta} $ denotes the cost function that we want to minimize by finding the optimal set of parameters $ \theta $.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hypothesis-function-1&#34;&gt;Hypothesis Function&lt;/h2&gt;
&lt;h3 id=&#34;initial-representation&#34;&gt;Initial Representation&lt;/h3&gt;
&lt;p&gt;Given a set of data, let&amp;rsquo;s say we want to approximate $ y $ as a linear function of $ x $:&lt;/p&gt;
&lt;p&gt;$$ h_{\theta}(x) = {\theta}_0 + {\theta}_1x_1 + {\theta}_2x_2 + &amp;hellip; + {\theta}_nx_n $$&lt;/p&gt;
&lt;p&gt;, where the $ {\theta}_j $&amp;rsquo;s are the parameters (also called &lt;strong&gt;weights&lt;/strong&gt;) parameterizing the space of linear functions mapping from $ X $ to $ Y $.&lt;/p&gt;
&lt;h3 id=&#34;representation-improvement&#34;&gt;Representation Improvement&lt;/h3&gt;
&lt;p&gt;To make it simplied, we introduce an &lt;strong&gt;intercept term&lt;/strong&gt; $ x_0=1 $ such that:&lt;/p&gt;
&lt;p&gt;$$ h(x) =  \sum_{j=0}^{n} {\theta}_jx_j = {\theta}^Tx $$&lt;/p&gt;
&lt;p&gt;, where on the right-hand side above we are viewing $ {\theta} $ and $ x $ both as vectors.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cost-function-1&#34;&gt;Cost Function&lt;/h2&gt;
&lt;h3 id=&#34;error-interpretation&#34;&gt;Error Interpretation&lt;/h3&gt;
&lt;p&gt;The hypothesis may not capture unmodeled effects or random noise, so we can say:&lt;/p&gt;
&lt;p&gt;$$ y^{(i)} = {\theta}^Tx^{(i)} + {\epsilon}^{(i)} $$&lt;/p&gt;
&lt;p&gt;Let’s further assume that $ {\epsilon}^{(i)} $ follows a &lt;strong&gt;Gaussian distribution (Normal distribution)&lt;/strong&gt; with mean $ {\mu}=0 $ and variance $ {\sigma}^2 $. This could be expressed as $ {\epsilon}^{(i)} {\sim} N(0, {\sigma}^2)$. This implies that the probability density of $ {\epsilon}^{(i)} $ follows a Gaussian density function as follows:&lt;/p&gt;
&lt;p&gt;$$ P({\epsilon}^{(i)}) = \frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{({\epsilon}^{(i)})^2}{2 \sigma^{2}}) $$&lt;/p&gt;
&lt;p&gt;Another assumption we’re going to make is that the $ {\epsilon}^{(i)} $ error terms are &lt;strong&gt;IID&lt;/strong&gt;, which in statistics stands for &lt;strong&gt;independently and identically distributed&lt;/strong&gt;. Under these set of assumptions, the probability density of $ y^{(i)} $ given $ x^{(i)} $ and parameterized by $ {\theta} $:&lt;/p&gt;
&lt;p&gt;$$ P(y^{(i)} \mid x^{(i)} ; \theta)=\frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}}) $$&lt;/p&gt;
&lt;p&gt;In other words, given $ x^{(i)} $ and $ {\theta} $, the random variable $ y^{(i)} $ is sampled from a Gaussian Distribution $ N({\theta}^Tx^{(i)}, {\sigma}^{2}) $.&lt;/p&gt;
&lt;h3 id=&#34;likelihood-function&#34;&gt;Likelihood Function&lt;/h3&gt;
&lt;p&gt;Given the input features matrix (design matrix) $ X $ (All $ x^{(i)} $ and $ {\theta} $), the probability distribution of $ \vec{y} $ (All $ y^{(i)} $’s) is given by $ P(\vec{y}∣X;{\theta}) $. The likelikhood of the parameters $ L({\theta}) $ is defined as:&lt;/p&gt;
&lt;p&gt;$$ L(\theta) =P(\vec{y} \mid X ; \theta) =\prod_{i=1}^{m} P(y^{(i)} \mid x^{(i)} ; \theta) $$&lt;/p&gt;
&lt;p&gt;Now, we add details into the above equation,
$$
L(\theta)=\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}})
$$&lt;/p&gt;
&lt;p&gt;The principal of &lt;strong&gt;Maximum Likelihood Estimation (MLE)&lt;/strong&gt; states that we should choose $ {\theta} $ so as to make the data as high probability as possible. The derivations will be a bit simpler if we instead maximize the log likelihood $ {\ell(\theta)} $:&lt;/p&gt;
&lt;p&gt;$$
{\ell(\theta)}=ln(\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}}))
$$&lt;/p&gt;
&lt;p&gt;According to the product rule of logarithms,&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta)=\sum_{i=1}^{m} ln \frac{1}{\sqrt{2 \pi} \sigma} exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}})
$$&lt;/p&gt;
&lt;p&gt;Again, the log product rule shows us:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta)=\sum_{i=1}^{m} \left[ln \frac{1}{\sqrt{2 \pi} \sigma} + ln(exp (-\frac{(y^{(i)}-\theta^{T} x^{(i)})^{2}}{2 \sigma^{2}})\right]
$$&lt;/p&gt;
&lt;p&gt;This could be expanded as:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta)= m {\cdot} ln \frac{1}{\sqrt{2 \pi} \sigma} - \frac{1}{\sigma^{2}} {\cdot} \frac{1}{2} {\cdot} \sum_{i=1}^{m} (y^{(i)}-\theta^{T} x^{(i)})^{2}
$$&lt;/p&gt;
&lt;h3 id=&#34;cost-function-formation&#34;&gt;Cost Function Formation&lt;/h3&gt;
&lt;p&gt;From the probability interpretation above, we could regard $ \ell(\theta) $ as:&lt;/p&gt;
&lt;p&gt;$$
\ell(\theta)= C_1 - C_2 {\cdot} \frac{1}{2} {\cdot} \sum_{i=1}^{m} (y^{(i)}-\theta^{T} x^{(i)})^{2}
$$&lt;/p&gt;
&lt;p&gt;, where $ C_1 $ and $ C_2 $ are constant. Hence, maximizing $ \ell(\theta) $ is equivalent to minimizing $ J(\theta) $:&lt;/p&gt;
&lt;p&gt;$$
J(\theta)= \frac{1}{2} {\cdot} \sum_{i=1}^{m} (y^{(i)}-\theta^{T} x^{(i)})^{2}
$$&lt;/p&gt;
&lt;p&gt;, where $ 1/2 $ is a constant that helps cancel 2 in derivative of the function.&lt;/p&gt;
&lt;p&gt;The formation above shows that choosing the value of $ \theta $ to minimize the &lt;strong&gt;least squares error cost function&lt;/strong&gt;, is equivalent to finding the &lt;strong&gt;MLE&lt;/strong&gt; for the parameters $ \theta $ under the set of assumptions that the error terms $ {\epsilon}^{(i)} $ are &lt;strong&gt;Gaussian&lt;/strong&gt; and &lt;strong&gt;IID&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cost-function-minimization&#34;&gt;Cost Function Minimization&lt;/h2&gt;
&lt;h3 id=&#34;least-mean-square&#34;&gt;Least Mean Square&lt;/h3&gt;
&lt;p&gt;To find a set of parameters, one way is to find the optimum that minimizes the cost function. Therefore, we need $ \nabla_{\theta} J(\theta)= 0 $. More specifically, $ \frac{\partial J}{\partial \theta_{1}} = 0, \frac{\partial J}{\partial \theta_{2}} = 0, \frac{\partial J}{\partial \theta_{3}} = 0, &amp;hellip;, \frac{\partial J}{\partial \theta_{n}} = 0 $.&lt;/p&gt;
&lt;p&gt;However, this may generate $ n $ equations and is computationally expensive if there are many features in the dataset. We need to find other ways.&lt;/p&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h3&gt;
&lt;p&gt;We can start with some “initial guess” for $ {\theta} $, and that repeatedly changes $ {\theta} $ to make $ J(\theta) $ smaller, until hopefully we converge to a value of $ {\theta} $ that minimizes $ J(\theta) $. For $ j = 0, 1, 2, &amp;hellip;, n $, we &lt;strong&gt;simultaneously&lt;/strong&gt; update the $ {\theta} $ and this process will be kept until convergence:&lt;/p&gt;
&lt;p&gt;$$ \theta _{j} \gets \theta _{j} - \alpha \cdot \frac{\partial J}{\partial \theta _{j}}  $$&lt;/p&gt;
&lt;p&gt;Here, $ \alpha$ is the learning rate, a hyperparameter to control the searching speed. Now, we put all the equations together and we coud see:&lt;/p&gt;
&lt;p&gt;$$ \theta_{j}\gets \theta_{j}-\alpha \cdot \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}) x_{j}^{(i)} $$&lt;/p&gt;
&lt;p&gt;This is the &lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt; strategy, because we update the parameters according to the gradient of the error with respect to a single training example only. Beside, there are other strategies, such as &lt;strong&gt;Batch GD&lt;/strong&gt;, &lt;strong&gt;Mini-batch GD&lt;/strong&gt;, etc, which will be discussed in my future posts.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;normal-equation&#34;&gt;Normal Equation&lt;/h2&gt;
&lt;p&gt;In this method, we will minimize $ J(\theta) $ by explicitly taking its derivatives with respect to the $ {\theta}_j $’s, and setting them to zero.&lt;/p&gt;
&lt;p&gt;Given a training set, define the design matrix $ X $ to be the $ m×n $, and let $ y $ to be the $ m×1 $ vector containing all the target values from the training set. We can obtain:&lt;/p&gt;
&lt;p&gt;$$
J(\theta)= \frac{1}{2} {\cdot} \sum_{i=1}^{m} (y^{(i)}-\theta^{T} x^{(i)})^{2} = \frac{1}{2} || X\theta-y || ^2 = \frac{1}{2} (X \theta-y)^{T}(X \theta-y)
$$&lt;/p&gt;
&lt;p&gt;Now, we expand the equation:&lt;/p&gt;
&lt;p&gt;$$
J(\theta)= \frac{1}{2} (\theta^{T} X^{T} X \theta-\theta^{T} X^{T} y-y^{T} X \theta + y^{T} y)
$$&lt;/p&gt;
&lt;p&gt;Since $ \theta^{T} X^{T} y $ and $ y^{T} X \theta $ are scalars, so they are equivalent. We can now simplify the equation as:&lt;/p&gt;
&lt;p&gt;$$
J(\theta)= \frac{1}{2} (\theta^{T} X^{T} X \theta-2\theta^{T} X^{T} y + y^{T} y)
$$&lt;/p&gt;
&lt;p&gt;Accoding to the rules of derivative of matrix, $ \frac{d B^{T} A B}{d B}=(A+A^{\mathrm{T}}) B $ and $ \frac{d B^{T} A}{d B}=A $, we can find its derivatives with respect to $ \theta $:&lt;/p&gt;
&lt;p&gt;$$
\nabla_{\theta} J(\theta)= \frac{1}{2} ((X^TX + (X^TX)^T)\theta - 2X^Ty) = \frac{1}{2} (2X^TX\theta - 2X^Ty)
$$&lt;/p&gt;
&lt;p&gt;Now, we set $ \nabla_{\theta} J(\theta) $ to zero:&lt;/p&gt;
&lt;p&gt;$$
X^TX\theta = X^Ty
$$&lt;/p&gt;
&lt;p&gt;Thus, the value of $ \theta $ that minimizes $ J(\theta) $ is given in closed form by the equation:&lt;/p&gt;
&lt;p&gt;$$
\theta=(X^{T} X)^{-1} X^{T} y
$$&lt;/p&gt;
&lt;p&gt;This approach requires the matrix to be &lt;strong&gt;inversible&lt;/strong&gt;. Usually, if some features have linear relations or the size of training samples is smaller than the number of features, $ X^TX $ is not inversible.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://aman.ai/cs229/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Stanford CS229 ML Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.seldon.io/machine-learning-regression-explained#:~:text=Regression%20is%20a%20technique%20for,used%20to%20predict%20continuous%20outcomes.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Machine Learning Regression Explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://machinelearningmastery.com/linear-regression-for-machine-learning/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Linear Regression for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Logistic Regression — Detailed Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/swlh/ml-fundamentals-what-is-cost-function-d396129cc611&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ML Fundamentals: What Is Cost Function?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scruel.gitee.io/ml-andrewng-notes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Coursera ML Andrew Ng Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
