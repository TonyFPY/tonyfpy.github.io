<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Authored by Tony Feng
Created on May 11th, 2022
Last Modified on May 14th, 2022
 Intro A decision tree is one of the supervised machine learning algorithms. Decision Tree algorithm belongs to the family of supervised learning algorithms. The goal is to create a model that predicts the value of a target variable by learning if-else conditions(decision rules) inferred from the data features.
Types of Decision Tree Decision Tree (DT) can be used for both regression and classification problems, but it is mostly used for classification problems.'><title>[Machine Learning] Topic 3: Decisoin Tree</title>

<link rel='canonical' href='https://tonyfpy.github.io/p/machine-learning-topic-3-decisoin-tree/'>

<link rel="stylesheet" href="/scss/style.min.eb00024a265e968b72ca9ba79297ca591e365639164dcdbf985ae5694a6c4a3f.css"><meta property='og:title' content='[Machine Learning] Topic 3: Decisoin Tree'>
<meta property='og:description' content='Authored by Tony Feng
Created on May 11th, 2022
Last Modified on May 14th, 2022
 Intro A decision tree is one of the supervised machine learning algorithms. Decision Tree algorithm belongs to the family of supervised learning algorithms. The goal is to create a model that predicts the value of a target variable by learning if-else conditions(decision rules) inferred from the data features.
Types of Decision Tree Decision Tree (DT) can be used for both regression and classification problems, but it is mostly used for classification problems.'>
<meta property='og:url' content='https://tonyfpy.github.io/p/machine-learning-topic-3-decisoin-tree/'>
<meta property='og:site_name' content='Tony Feng'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='Machine Learning' /><meta property='article:tag' content='Python' /><meta property='article:published_time' content='2022-05-11T20:51:10&#43;08:00'/><meta property='article:modified_time' content='2022-05-18T09:16:45-04:00'/>
<meta name="twitter:title" content="[Machine Learning] Topic 3: Decisoin Tree">
<meta name="twitter:description" content="Authored by Tony Feng
Created on May 11th, 2022
Last Modified on May 14th, 2022
 Intro A decision tree is one of the supervised machine learning algorithms. Decision Tree algorithm belongs to the family of supervised learning algorithms. The goal is to create a model that predicts the value of a target variable by learning if-else conditions(decision rules) inferred from the data features.
Types of Decision Tree Decision Tree (DT) can be used for both regression and classification problems, but it is mostly used for classification problems.">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/Brown_hu1b43bb4664aba7c4309526f1ef45ab91_1767_300x0_resize_q75_box.jpeg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Tony Feng</a></h1>
            <h2 class="site-description">M.Sc. in Computer Science</h2>
            <h2 class="site-description">Brown University</h2>
        </div>
    </header><ol class="social-menu">
            
                <li>
                    <a 
                        href='https://tonyfpy.github.io/about/'
                        target="_blank"
                        title="E-mail"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="#ffffff" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <rect x="3" y="5" width="18" height="14" rx="2" />
  <polyline points="3 7 12 13 21 7" />
</svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://github.com/TonyFPY'
                        target="_blank"
                        title="GitHub"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://www.linkedin.com/in/pinyuanfeng'
                        target="_blank"
                        title="Linkedin"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="#ffffff" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <rect x="4" y="4" width="16" height="16" rx="2" />
  <line x1="8" y1="11" x2="8" y2="16" />
  <line x1="8" y1="8" x2="8" y2="8.01" />
  <line x1="12" y1="16" x2="12" y2="11" />
  <path d="M16 16v-3a2 2 0 0 0 -4 0" />
</svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/projects/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="7 8 3 12 7 16" />
  <polyline points="17 8 21 12 17 16" />
  <line x1="14" y1="4" x2="10" y2="20" />
</svg>


                
                <span>Projects</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/machine-learning/" style="background-color: #3399FF; color: #fff;">
                Machine Learning
            </a>
        
            <a href="/categories/python/" style="background-color: #0099CC; color: #fff;">
                Python
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/machine-learning-topic-3-decisoin-tree/">[Machine Learning] Topic 3: Decisoin Tree</a>
        </h2>
    
        
    </div>

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">May 11, 2022</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    7 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>
</header>

    <section class="article-content">
    
    
    <blockquote>
<p>Authored by Tony Feng</p>
<p>Created on May 11th, 2022</p>
<p>Last Modified on May 14th, 2022</p>
</blockquote>
<h2 id="intro">Intro</h2>
<p>A decision tree is one of the supervised machine learning algorithms. Decision Tree algorithm belongs to the family of supervised learning algorithms. The goal is to create a model that predicts the value of a target variable by learning <strong>if-else conditions(decision rules)</strong> inferred from the data features.</p>
<h3 id="types-of-decision-tree">Types of Decision Tree</h3>
<p>Decision Tree (DT) can be used for both <strong>regression</strong> and <strong>classification</strong> problems, but it is mostly used for classification problems. Based on the types of target variable, we have 1) <strong>categorical-variable</strong> DT, and 2) <strong>continuous-variable</strong> DT.</p>
<h3 id="non-linearity">Non-linearity</h3>
<p>Formally, a method is <strong>linear</strong> if, for an input $ x \isin {\R}^n $ (with interecept term $ x_0=1 $), it only produces hypothesis functions $ h $ of the form:</p>
<p>$$ h(x) = \theta^{T}x, \theta \isin {\R}^n $$</p>
<p>Otherwise, it is <strong>non-linear</strong>. Decision trees, on the other hand, can directly produce non-linear hypothesis functions without the need for first coming up with an appropriate feature mapping.</p>
<h3 id="region-selection">Region Selection</h3>
<p>In general, selecting optimal regions is intractable. Decision trees generate an approximate solution via <strong>greedy</strong>, <strong>top-down</strong>, <strong>recursive</strong> partitioning.  Each node in the tree acts as a <strong>test case</strong> for some <strong>attribute</strong>, and each edge descending from that node corresponds to one of the possible answers to the test case.</p>
<p>Formally, given a input space $ X $, a parent region $ R_p $, a feature index $ j $, and a threshold $ t \isin {\R}$, we obtain two child regions $R_1$  and $R_2$ as follows:</p>
<p>$$ R_{1} = \{X \mid X_{j}&lt;t, X \in R_{p} \} $$ $$ R_{2} = \{X \mid X_{j} \geq t, X \in R_{p} \} $$</p>
<p>We can continue in such a manner until we a meet a <strong>pre-defined stop criterion</strong>, and then predict the majority class at each leaf node.</p>
<hr>
<h2 id="attribute-selective-measures">Attribute Selective Measures</h2>
<p>An <strong>attribute selection measure (AMS)</strong> is a technique used in the data mining process for data reduction. It is a <strong>heuristic</strong> for choosing the splitting test that “best” separates a given data partition. The three main ASM techniques are 1) <strong>Information Gain</strong>, 2) <strong>Gain Ratio</strong>, and 3) <strong>Gini Index</strong>.</p>
<h3 id="information-gain">Information Gain</h3>
<p><strong>Entropy</strong> is used to measure the level of <strong>impurity</strong> in a group of examples. This concept comes from informatin theory. The higher the entropy, the more the information content. Now, consider a dataset with $n$ classes, then</p>
<p>$$ E = -\sum_{i=1}^{n} p_{i} \log_{2} (p_{i}) $$</p>
<p>, where $ p_i $ is the probability of randomly selecting an example in class $i$.</p>
<p>We can define <strong>information gain (IG)</strong> as a measure of how much information a feature provides about a class. It is the expected reduction in entropy caused by partitioning the examples according to a given attribute. Given a collection of dataset $ S $, we can calculate the $IG(S, A)$ of an attribute $A$ as:</p>
<p>$$ IG(S, A) = E(S) - \sum_{v \in V} \frac{|S_{v}|}{|S|} E(S_{v}) $$</p>
<p>, where $V$ is all possible values for attribute $A$ and $S_{v}$ is the subset of $S$ for which attribute $A$ has value $v$.</p>
<h3 id="gain-ratio">Gain Ratio</h3>
<p><strong>Gain Ratio</strong> or <strong>Uncertainty Coefficient</strong> is used to normalize the information gain of an attribute against how much entropy that attribute has. It attempts to lessen the bias of Information Gain on highly branched predictors by introducing a normalizing term called the <strong>Intrinsic Information (II)</strong>.</p>
<p>$$ II(S, A) = - \sum_{v \in V(A)} \frac{|S_{v}|}{|S|} \log_2 \frac{|s_{v}|}{|S|} $$</p>
<p>Formula of Gain Ration is given by</p>
<p>$$ GR(S, A) = \frac{IG(S, A)}{II(S, A)} $$</p>
<p>From this formula, we can see that, with less value in attribute $ A $, $ II(S, A)$ is smaller and thus purity is higher. Informally, the formula of Intrinsic Information is the same as that of Entropy, both of which means the purity of attribute $ A $.</p>
<h3 id="gini-index">Gini Index</h3>
<p><strong>Gini Index</strong>, also called <strong>Gini Impurity</strong>, measures the degree or probability of a particular variable being <strong>wrongly</strong> classified when it is randomly chosen. If we have $C$ total classes and $p_i$ is the probability of picking a datapoint with class $i$, then the Gini Impurity is calculated as</p>
<p>$$ GI=\sum_{i=1}^{C} p_i(1-p_i) = 1 - \sum_{i=1}^{C} p_i^{2}$$</p>
<p>For a data sample $ S $, we have an attribute $ A $ with a group of values $V$. Therefore,</p>
<p>$$ GI(S, A) = \sum_{v \in V} \frac{|S_{v}|}{|S|} GI(S_{v}) $$</p>
<p>The Gini Index varies between $0$ and $1$, where $0$ represents purity of the classification and $1$ denotes random distribution of elements among various classes. A Gini Index of $0.5$ shows that there is <strong>equal distribution</strong> of elements across some classes.</p>
<hr>
<h2 id="feature-selection-algorithms">Feature Selection Algorithms</h2>
<h3 id="id3">ID3</h3>
<p>The <strong>ID3</strong> algorithm builds decision trees using a <strong>top-down greedy search</strong> approach through the space of possible branches with no backtracking.</p>
<p><strong>Steps of ID3</strong></p>
<ul>
<li>Begin with the original datset as the root node and calculte its entropy.</li>
<li>For each attribute/feature:
<ul>
<li>Calculate entropy for all its categorical values.</li>
<li>Calculate information gain for the feature.</li>
</ul>
</li>
<li>Find the feature with <strong>maximum information gain</strong> and split to produce subsets.</li>
<li>Continues to recur on each subset until we get the desired tree.</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>ID3 has no pruning strategy and is easy to <strong>overfit</strong>.</li>
<li>The IG criterion has a preference for features with a number of possible values.</li>
<li>It can only be used to deal with <strong>discretely</strong> distributed features;</li>
<li>It does not consider missing values.</li>
</ul>
<h3 id="c45">C4.5</h3>
<p>Compared with the shortcomings of ID3, C4.5 has the following improvements:</p>
<ul>
<li>C4.5 overcomes is the shortcoming of ID3&rsquo;s emphasis on the number of feature by using the <strong>Gain Ratio</strong> as the goodness function to split the dataset.</li>
<li>C4.5 is able to handle both <strong>continuous</strong> and <strong>discrete</strong> attributes.</li>
<li>C4.5 has a <strong>pruning</strong> strategy, which replaces the helpless branches with leaf nodes after the tree is created.</li>
<li>C4.5 takes <strong>missing values</strong> into account.</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>C4.5 uses a <strong>polytree</strong> which is less efficient than a binary tree.</li>
<li>C4.5 can only be used for <strong>classification</strong>.</li>
<li>C4.5 is <strong>computaionally expensive</strong> both in time and space.</li>
</ul>
<h3 id="cart">CART</h3>
<p><strong>Classification And Regression Trees (CART)</strong> is a recursive partitioning method, builds classification and regression trees for predicting <strong>continuous dependent variables (regression)</strong> and <strong>categorical predictor variables (classification)</strong>.</p>
<p>The <a class="link" href="http://rollingdeep.github.io/2020/11/02/CART%E5%88%86%E7%B1%BB%E6%A0%91%E4%BE%8B%E5%AD%90%E8%AE%B2%E8%A7%A3/"  target="_blank" rel="noopener"
    >classification algorithm</a> for building a decision tree is based on <strong>Gini impurity</strong> as splitting criterion, while the <a class="link" href="http://rollingdeep.github.io/2020/11/02/CART%E5%9B%9E%E5%BD%92%E6%A0%91%E4%BE%8B%E5%AD%90%E8%AE%B2%E8%A7%A3/"  target="_blank" rel="noopener"
    >regression tree</a>  uses <strong>square error</strong>.</p>
<hr>
<h2 id="tricks-in-decision-tree-construction">Tricks in Decision Tree Construction</h2>
<h3 id="pruning">Pruning</h3>
<p><strong>Pre-pruning</strong>:</p>
<ul>
<li>The size of nodes is smaller than the threshold.</li>
<li>The depth of the tree is deeper than the threshold.</li>
<li>All the attributes have been checked.</li>
<li>Accuracy on the validation set becomes lower after splitting.</li>
</ul>
<p><strong>Post-pruning</strong> (<a class="link" href="https://wizardforcel.gitbooks.io/dm-algo-top10/content/cart.html"  target="_blank" rel="noopener"
    >Readings</a>):</p>
<ul>
<li>Reduced-Error Pruning</li>
<li>Pessimistic Error Pruning</li>
</ul>
<h3 id="continuous-variables">Continuous Variables</h3>
<p>Since some attributes has a continuous property, we dealt with the continuous values according to the following manner. Suppose the attribute $ A $ has $ a_1, a_2, &hellip; , a_n $, then $ v $ equals $ n − 1 $. The values are sorted in an ascending order to calculate each threshold.</p>
<p>$$ threshold(k)=\frac{a_{k}+a_{k+1}}{2} $$</p>
<p>, where $ a_{k}&lt;a_{k+1} $ and $ 1 \leq k \leq n-1 $. The optimal split will be found when it achieves the maximum purity.</p>
<h3 id="missing-values">Missing Values</h3>
<p><strong>Case 1</strong>:
When an attribute has missing values, we calulate the ASM without considering the data with missing values. Then, we normalize the ASM based on the proportion of data with missing values.</p>
<p><strong>Case 2</strong>:
We find a split criterion, but some data samples don&rsquo;t have the value of that attribute. We distribute those data into sub-nodes based on the ratio of the size of those sub-nodes created by normal data.</p>
<p><strong>Case 3</strong>:
When the testing data has missing values, let each abnormal data go through every branch and see which branch results in the highest probability.</p>
<hr>
<h2 id="reference">Reference</h2>
<ul>
<li><a class="link" href="https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html"  target="_blank" rel="noopener"
    >Decision Tree Algorithm, Explained</a></li>
<li><a class="link" href="https://aman.ai/cs229/"  target="_blank" rel="noopener"
    >Stanford CS229 ML Notes</a></li>
<li><a class="link" href="https://www.xoriant.com/blog/product-engineering/decision-trees-machine-learning-algorithm.html#:~:text=Introduction%20Decision%20Trees%20are%20a,namely%20decision%20nodes%20and%20leaves."  target="_blank" rel="noopener"
    >Decision Trees for Classification: A Machine Learning Algorithm</a></li>
<li><a class="link" href="https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/"  target="_blank" rel="noopener"
    >Hack Earth = Decision Tree</a></li>
<li><a class="link" href="https://towardsai.net/p/programming/decision-trees-explained-with-a-practical-example-fe47872d3b53"  target="_blank" rel="noopener"
    >Decision Trees Explained With a Practical Example</a></li>
<li><a class="link" href="https://www.tutorialspoint.com/what-is-attribute-selection-measures#:~:text=An%20attribute%20selection%20measure%20is,training%20tuples%20into%20single%20classes."  target="_blank" rel="noopener"
    >What is Attribute Selection Measures?</a></li>
<li><a class="link" href="https://www.section.io/engineering-education/entropy-information-gain-machine-learning/"  target="_blank" rel="noopener"
    >Entropy and Information Gain to Build Decision Trees in Machine Learning</a></li>
</ul>
<hr>

</section>


    <footer class="article-footer">
    

    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>MIT License</span>
    </section>
    <section class="article-lastmod">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <span>
            Last updated on May 18, 2022 09:16 EDT
        </span>
    </section></footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    

    
     
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2022 Tony Feng
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.11.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#intro">Intro</a>
      <ol>
        <li><a href="#types-of-decision-tree">Types of Decision Tree</a></li>
        <li><a href="#non-linearity">Non-linearity</a></li>
        <li><a href="#region-selection">Region Selection</a></li>
      </ol>
    </li>
    <li><a href="#attribute-selective-measures">Attribute Selective Measures</a>
      <ol>
        <li><a href="#information-gain">Information Gain</a></li>
        <li><a href="#gain-ratio">Gain Ratio</a></li>
        <li><a href="#gini-index">Gini Index</a></li>
      </ol>
    </li>
    <li><a href="#feature-selection-algorithms">Feature Selection Algorithms</a>
      <ol>
        <li><a href="#id3">ID3</a></li>
        <li><a href="#c45">C4.5</a></li>
        <li><a href="#cart">CART</a></li>
      </ol>
    </li>
    <li><a href="#tricks-in-decision-tree-construction">Tricks in Decision Tree Construction</a>
      <ol>
        <li><a href="#pruning">Pruning</a></li>
        <li><a href="#continuous-variables">Continuous Variables</a></li>
        <li><a href="#missing-values">Missing Values</a></li>
      </ol>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
