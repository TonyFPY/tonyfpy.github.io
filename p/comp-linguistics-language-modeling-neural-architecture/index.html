<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Authored by Tony Feng
Created on Oct 18th, 2022
Last Modified on Oct 20th, 2022
 Intro This sereis of posts contains a summary of materials and readings from the course CSCI 1460 Computational Linguistics that I&amp;rsquo;ve taken @ Brown University. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.
 Multi-layer Perceptron  MLP doesn’t readily support long, sequential inputs MLP doesn’t consider encoding word order, essentially a BOW model.'><title>[Comp Linguistics] Language Modeling - Neural Architecture</title>

<link rel='canonical' href='https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/'>

<link rel="stylesheet" href="/scss/style.min.188690048e9cccc35dbaed0d37520e9b3eea8ddf3a7499913462277d73fe65cb.css"><meta property='og:title' content='[Comp Linguistics] Language Modeling - Neural Architecture'>
<meta property='og:description' content='Authored by Tony Feng
Created on Oct 18th, 2022
Last Modified on Oct 20th, 2022
 Intro This sereis of posts contains a summary of materials and readings from the course CSCI 1460 Computational Linguistics that I&amp;rsquo;ve taken @ Brown University. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.
 Multi-layer Perceptron  MLP doesn’t readily support long, sequential inputs MLP doesn’t consider encoding word order, essentially a BOW model.'>
<meta property='og:url' content='https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/'>
<meta property='og:site_name' content='Tony Feng'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='Natural Language Processing' /><meta property='article:published_time' content='2022-10-18T14:32:35-04:00'/><meta property='article:modified_time' content='2022-10-20T16:21:19-04:00'/>
<meta name="twitter:title" content="[Comp Linguistics] Language Modeling - Neural Architecture">
<meta name="twitter:description" content="Authored by Tony Feng
Created on Oct 18th, 2022
Last Modified on Oct 20th, 2022
 Intro This sereis of posts contains a summary of materials and readings from the course CSCI 1460 Computational Linguistics that I&amp;rsquo;ve taken @ Brown University. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.
 Multi-layer Perceptron  MLP doesn’t readily support long, sequential inputs MLP doesn’t consider encoding word order, essentially a BOW model.">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/Brown_hu1b43bb4664aba7c4309526f1ef45ab91_1767_300x0_resize_q75_box.jpeg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Tony Feng</a></h1>
            <h2 class="site-description">M.Sc. in Computer Science</h2>
            <h2 class="site-description">Brown University</h2>
        </div>
    </header><ol class="social-menu">
            
                <li>
                    <a 
                        href='https://tonyfpy.github.io/about/'
                        target="_blank"
                        title="E-mail"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="#ffffff" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <rect x="3" y="5" width="18" height="14" rx="2" />
  <polyline points="3 7 12 13 21 7" />
</svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://github.com/TonyFPY'
                        target="_blank"
                        title="GitHub"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://www.linkedin.com/in/pinyuanfeng'
                        target="_blank"
                        title="Linkedin"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="#ffffff" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <rect x="4" y="4" width="16" height="16" rx="2" />
  <line x1="8" y1="11" x2="8" y2="16" />
  <line x1="8" y1="8" x2="8" y2="8.01" />
  <line x1="12" y1="16" x2="12" y2="11" />
  <path d="M16 16v-3a2 2 0 0 0 -4 0" />
</svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/research/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-book" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M3 19a9 9 0 0 1 9 0a9 9 0 0 1 9 0" />
  <path d="M3 6a9 9 0 0 1 9 0a9 9 0 0 1 9 0" />
  <line x1="3" y1="6" x2="3" y2="19" />
  <line x1="12" y1="6" x2="12" y2="19" />
  <line x1="21" y1="6" x2="21" y2="19" />
</svg>
                
                <span>Research</span>
            </a>
        </li>
        
        

        <li >
            <a href='/projects/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="7 8 3 12 7 16" />
  <polyline points="17 8 21 12 17 16" />
  <line x1="14" y1="4" x2="10" y2="20" />
</svg>


                
                <span>Projects</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/natural-language-processing/" style="background-color: #D7B59C; color: #fff;">
                Natural Language Processing
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/comp-linguistics-language-modeling-neural-architecture/">[Comp Linguistics] Language Modeling - Neural Architecture</a>
        </h2>
    
        
    </div>

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Oct 18, 2022</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    4 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>
</header>

    <section class="article-content">
    
    
    <blockquote>
<p>Authored by Tony Feng</p>
<p>Created on Oct 18th, 2022</p>
<p>Last Modified on Oct 20th, 2022</p>
</blockquote>
<h2 id="intro">Intro</h2>
<p>This sereis of posts contains a summary of materials and readings from the course <strong>CSCI 1460 Computational Linguistics</strong> that I&rsquo;ve taken @ <strong>Brown University</strong>. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &ldquo;Notes&rdquo; (what I&rsquo;ve learnt) for study and review only.</p>
<hr>
<h2 id="multi-layer-perceptron">Multi-layer Perceptron</h2>
<ul>
<li>MLP doesn’t readily support long, sequential inputs</li>
<li>MLP doesn’t consider encoding word order, essentially a BOW model.</li>
<li>Inputs either become muddy (adding everything together, i.e., &ldquo;bag-of-vectors&rdquo;) or too large (concatenating everything)</li>
<li>&ldquo;bag-of-vectors&rdquo; classifiers are common and often work well for
<strong>basic</strong> applications</li>
</ul>
<hr>
<h2 id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h2>
<h3 id="architecture">Architecture</h3>
<p>The basic idea is generation of word $i+1$ depends on word $i$ plus &ldquo;memory&rdquo; of words generated up to $i$.</p>
<p><img src="/p/comp-linguistics-language-modeling-neural-architecture/view1.jpg"
	width="714"
	height="628"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/view1_hu2f49a735a03e56027346c1b27d8044b1_34408_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/view1_hu2f49a735a03e56027346c1b27d8044b1_34408_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
		alt="&ldquo;Unrolled&rdquo;"
	
	
		class="gallery-image" 
		data-flex-grow="113"
		data-flex-basis="272px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/view2.jpg"
	width="358"
	height="534"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/view2_hu74f30de8a34c66e2143b7e317a6fab4e_11034_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/view2_hu74f30de8a34c66e2143b7e317a6fab4e_11034_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
		alt="&ldquo;Recurrent / Recursive&rdquo;"
	
	
		class="gallery-image" 
		data-flex-grow="67"
		data-flex-basis="160px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/view3.jpg"
	width="568"
	height="506"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/view3_hu428cbdb151055c717b37789e2c1a1e84_8961_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/view3_hu428cbdb151055c717b37789e2c1a1e84_8961_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
		alt="&ldquo;A single step of recurrent / recursive&rdquo;"
	
	
		class="gallery-image" 
		data-flex-grow="112"
		data-flex-basis="269px"
	
></p>
<p>$$h_{t}=g\left(U h_{t-1}+W x_{t}\right)$$
$$y_{t}=f\left(V h_{t}\right)$$</p>
<h3 id="inference">Inference</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">function</span> <span class="n">ForwardRNN</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">sequence</span> <span class="n">y</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_0</span> <span class="o">&lt;-</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="o">&lt;-</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="n">do</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_i</span> <span class="o">&lt;-</span> <span class="n">g</span><span class="p">(</span><span class="n">U</span> <span class="o">*</span> <span class="n">h_</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">W</span> <span class="o">*</span> <span class="n">x_i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_i</span> <span class="o">&lt;-</span> <span class="n">f</span><span class="p">(</span><span class="n">V</span> <span class="o">*</span> <span class="n">h_i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">y</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="training-considerations">Training Considerations</h3>
<ul>
<li>In practice, using unrolled and padded to a fixed length is better for batching.</li>
<li>When producing word $i$, predict based on the <strong>real</strong> $i-1$, not the
<strong>predicted</strong> $i-1$ (which is likely wrong).</li>
</ul>
<hr>
<h2 id="long-short-term-memory-lstm">Long-Short Term Memory (LSTM)</h2>
<h3 id="motivation">Motivation</h3>
<ul>
<li>RNNs struggle with <strong>long range dependencies</strong>.</li>
<li><strong>Vanishing gradients</strong> makes it hard to update early hidden states
for long sequences.</li>
</ul>
<h3 id="architecture-1">Architecture</h3>
<p><strong>Overview</strong></p>
<ul>
<li>Introducing a <strong>&ldquo;gating&rdquo; mechanisms</strong> to manage the hidden state/
memory</li>
<li>Passing through &ldquo;gates&rdquo;
<ul>
<li>&ldquo;Forget&rdquo; gate removes information no longer needed</li>
<li>&ldquo;Add&rdquo; gate adds new information likely to be useful in the
future</li>
<li>&ldquo;Output&rdquo; gate</li>
</ul>
</li>
<li>Adding explicit previous &ldquo;context&rdquo; in addition to prior hidden state</li>
</ul>
<p><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/lstm.jpg"
	width="1400"
	height="580"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/lstm_hu041f8b1d561fb25df604ac2817f56ef3_57530_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/lstm_hu041f8b1d561fb25df604ac2817f56ef3_57530_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
		alt="LSTM Computation Graph"
	
	
		class="gallery-image" 
		data-flex-grow="241"
		data-flex-basis="579px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
></p>
<p><strong>Gate</strong></p>
<ul>
<li>Learn some mask (i.e., vector) via backpropagation</li>
<li>Apply the mask(i.e., elementwise multiplication) to some hidden state</li>
</ul>
<p><strong>Computation</strong></p>
<ul>
<li>Compute &ldquo;current&rdquo; state, &ldquo;add&rdquo; gate, &ldquo;forget&rdquo; gate, and &ldquo;output&rdquo; gate from previous hidden state and current input.
<ul>
<li>g = current state</li>
<li>f = forget gate</li>
<li>i = add gate</li>
<li>o = output gate (controls what to output and retains the relavant info right now.)</li>
<li>k = intermediate output (context after &ldquo;forgetting&rdquo;)</li>
<li>j = intermediate output (info to be added to the context)</li>
<li>c = updated context</li>
<li>h = updated hidden state</li>
</ul>
</li>
<li>Combine those things using Hadamard product.</li>
<li>Update context and hidden state for next iteration.</li>
</ul>
<p>$$g=\tanh \left(U_{g} h_{t-1}+W_{g} x_{t}\right) $$
$$f=\operatorname{sigmoid}\left(U_{f} h_{t-1}+W_{f} x_{t}\right) $$
$$i_{t}=\operatorname{sigmoid}\left(U_{i} h_{t-1}+W_{i} x_{t}\right) $$
$$o_{t}=\operatorname{sigmoid}\left(U_{o} h_{t-1}+W_{o} x_{t}\right) $$
$$k_{t}=f_{t} \odot c_{t-1} $$
$$j_{t}=i_{t} \odot g_{t} $$
$$c_{t}=j_{t}+k_{t} $$
$$h_{t}=o_{t} \odot \tanh \left(c_{t}\right) $$</p>
<hr>
<h2 id="transformer">Transformer</h2>
<h3 id="architecture-2">Architecture</h3>
<p>Representation of the word depends on (slightly less contextualized) representation of other words.</p>
<p><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/transformer.jpg"
	width="1270"
	height="540"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/transformer_hub8fb5f921e84dda4eb3c5a79434a7612_34993_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/transformer_hub8fb5f921e84dda4eb3c5a79434a7612_34993_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
		alt="Transformer"
	
	
		class="gallery-image" 
		data-flex-grow="235"
		data-flex-basis="564px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
></p>
<h3 id="self-attention">Self Attention</h3>
<p>The idea is to learn a <strong>distribution/weighted combination</strong> of hidden states that inform this hidden state.</p>
<p>Each word has three roles at each timestep and we learn three weight matrices $(Q,K,V)$ to cast each word into each role. The <strong>dot product</strong> of key and Query produces a weight and the next layer receives the weighted combination of values.</p>
<ul>
<li><strong>Query</strong>: The word as the current focus</li>
<li><strong>Key</strong>: The word as a context word</li>
<li><strong>Value</strong>: The word as part of the output</li>
</ul>
<p><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/self-attention.jpg"
	width="1352"
	height="1222"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/self-attention_hudff2f91419a60808ee0972ddc4b5d5f0_95851_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/self-attention_hudff2f91419a60808ee0972ddc4b5d5f0_95851_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
		alt="Self-attention"
	
	
		class="gallery-image" 
		data-flex-grow="110"
		data-flex-basis="265px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
></p>
<p>Multi-threaded self-attention is repeating the attention process multiple times. Each KQV set can focus on a slightly different aspect of the representation.</p>
<h3 id="blocks">Blocks</h3>
<p><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/TB.jpg"
	width="1690"
	height="914"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/TB_huc391e28b06d8b613b934126bcf6e40b9_84008_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/TB_huc391e28b06d8b613b934126bcf6e40b9_84008_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
		alt="A Transformer Block"
	
	
		class="gallery-image" 
		data-flex-grow="184"
		data-flex-basis="443px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
></p>
<p><strong>Residual Connection</strong></p>
<p>It adds input to output to help with training/vanishing gradients.</p>
<p><strong>Layer Normalize</strong></p>
<p>It has the same idea of Z-score normalization.</p>
<p><strong>Feedforward Layer</strong></p>
<p>It is a simple perceptron-style layer to combine everything togethrt.</p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>Transformers aren’t actually aware of the order in which words occur, becasue they are a bag of words essentially.</p>
<p><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/PE.jpg"
	width="1224"
	height="628"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/PE_hud8e377890faa05347da7d0f37a3f43ec_76411_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/PE_hud8e377890faa05347da7d0f37a3f43ec_76411_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
		alt="A Simple Positional Encoding"
	
	
		class="gallery-image" 
		data-flex-grow="194"
		data-flex-basis="467px"
	
><img src="/p/comp-linguistics-language-modeling-neural-architecture/white.jpg"
	width="285"
	height="340"
	srcset="/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, /p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="83"
		data-flex-basis="201px"
	
></p>
<p>Positional encodings add an embedding representation of the <strong>absolute position</strong> to the input word embedding. However,</p>
<ul>
<li>Not the same as relative/order information in language</li>
<li>Less supervision for later positions</li>
<li>Hard to deal with recursive human language</li>
</ul>
<h3 id="why-is-it-a-big-deal">why is it a big deal?</h3>
<ul>
<li>Attention
<ul>
<li>has minimal inductive bias</li>
<li>can learn arbitrary graph structure</li>
</ul>
</li>
<li>Multiheadedness
<ul>
<li>Each &ldquo;head&rdquo; focuses on a different subspace of the input</li>
</ul>
</li>
<li>Scalability
<ul>
<li>At layer N, no dependency between timesteps, so it can be trained in parallel</li>
<li>Faster training = bigger models + more data</li>
<li>Allows for massive pretraining</li>
</ul>
</li>
</ul>
<hr>

</section>


    <footer class="article-footer">
    

    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>MIT License</span>
    </section>
    <section class="article-lastmod">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <span>
            Last updated on Oct 20, 2022 16:21 EDT
        </span>
    </section></footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    

    
     
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2022 - 
        
        2023 Tony Feng
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.11.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#intro">Intro</a></li>
    <li><a href="#multi-layer-perceptron">Multi-layer Perceptron</a></li>
    <li><a href="#recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</a>
      <ol>
        <li><a href="#architecture">Architecture</a></li>
        <li><a href="#inference">Inference</a></li>
        <li><a href="#training-considerations">Training Considerations</a></li>
      </ol>
    </li>
    <li><a href="#long-short-term-memory-lstm">Long-Short Term Memory (LSTM)</a>
      <ol>
        <li><a href="#motivation">Motivation</a></li>
        <li><a href="#architecture-1">Architecture</a></li>
      </ol>
    </li>
    <li><a href="#transformer">Transformer</a>
      <ol>
        <li><a href="#architecture-2">Architecture</a></li>
        <li><a href="#self-attention">Self Attention</a></li>
        <li><a href="#blocks">Blocks</a></li>
        <li><a href="#positional-encoding">Positional Encoding</a></li>
        <li><a href="#why-is-it-a-big-deal">why is it a big deal?</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
