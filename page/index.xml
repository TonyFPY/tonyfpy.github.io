<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Pages on Tony Feng</title>
        <link>https://tonyfpy.github.io/page/</link>
        <description>Recent content in Pages on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 13 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://tonyfpy.github.io/page/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Research</title>
        <link>https://tonyfpy.github.io/research/</link>
        <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
        
        <guid>https://tonyfpy.github.io/research/</guid>
        <description>&lt;p&gt;My main interest lies in the exploration of &lt;strong&gt;interactive intelligent systems&lt;/strong&gt; to shape a better user experience. Feel free to contact me for further discussion and collaboration.&lt;/p&gt;
&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;
&lt;h3 id=&#34;social-ar&#34;&gt;Social AR&lt;/h3&gt;
&lt;img src=&#34;MUAR.jpg&#34; align=&#34;left&#34; width=&#34;285px&#34; style=&#34;padding-right: 15px&#34;/&gt;
&lt;p&gt;Social AR is developed to support real-time multiuser interaction for exploration of cultural heritage. In this research, we believe Social AR that projects cultural heritage objects projected onto the real environment can be a better experience in terms of depth of viewing. The work was carried out at the &lt;a class=&#34;link&#34; href=&#34;https://www.nottingham.edu.cn/en/research-centres/nvidia-joint-lab/about.aspx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;NVIDIA Joint-Lab on Mixed Reality, NVIDIA Technology Centre&lt;/strong&gt;&lt;/a&gt;, supported by NVIDIA AI Technology Center (Singapore).
&lt;br clear=&#34;left&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advisors&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://www.nottingham.ac.uk/news/expertiseguide/computer-science-/professor-eugene-chng.aspx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Prof. Eugene Ch&amp;rsquo;ng&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Timeline&lt;/strong&gt;: Jun. 2021 - Aug. 2022&lt;br&gt;
&lt;strong&gt;Key Words&lt;/strong&gt;: Augmented Reality, Multi-user Interaction, Digital Heritage, Communication&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/abs/10.1145/3582266&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Paper - &lt;em&gt;ACM Journal on Computing and Cultural Heritage&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;you-draw-i-guess&#34;&gt;You Draw, I Guess&lt;/h3&gt;
&lt;img src=&#34;DRAW.jpg&#34; align=&#34;left&#34; width=&#34;285px&#34; style=&#34;padding-right: 15px&#34;/&gt;
&lt;p&gt;This &lt;strong&gt;undergraduate final-year project&lt;/strong&gt; is about exploring the interpretability of AI-driven sketch recognition through &lt;strong&gt;human-AI interaction&lt;/strong&gt; based on a lightweight interactive system – &lt;strong&gt;DRAW&lt;/strong&gt; (&lt;strong&gt;D&lt;/strong&gt;oodle &lt;strong&gt;R&lt;/strong&gt;ecognition in an &lt;strong&gt;A&lt;/strong&gt;utomated &lt;strong&gt;W&lt;/strong&gt;ay). A preliminary field study has been conducted to answer the research question: &amp;ldquo;Can humans interpret AI’s inference and draw machine-recognizable sketches through the gameplay?&amp;rdquo; According to the data analysis, the answer to this research question is positive, but not a 100% &amp;ldquo;Yes&amp;rdquo;.This project paves a way for future studies to explore the human understanding of AI&amp;rsquo;s decison making.
&lt;br clear=&#34;left&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advisors&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://www.nottingham.ac.uk/research/groups/mixedrealitylab/people/steve.benford&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Prof. Steve Benford&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Timeline&lt;/strong&gt;: Oct. 2020 - May. 2021&lt;br&gt;
&lt;strong&gt;Key Words&lt;/strong&gt;: Human-AI Interaction, Mobile Intelligence, Free-hand Sketch Recognition&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/d/1Du9rS4CPxiEfjSCOa6ugb35AOIAivygW/view?usp=sharing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Undergraduate Thesis&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/d/1ST01TDxZDLSsceFvIgxTbJ9qoEXcriIG/view?usp=sharing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Presentation&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;deeprelics-dataset&#34;&gt;DeepRelics Dataset&lt;/h3&gt;
&lt;img src=&#34;ICAART.jpg&#34; align=&#34;left&#34; width=&#34;285px&#34; style=&#34;padding-right: 15px&#34;/&gt;
&lt;p&gt;Cultural heritage presents both challenges and opportunities for the adoption of deep learning in &lt;strong&gt;3D digitalization&lt;/strong&gt;. In this research, we attempted to reduce the need for manual labour by &lt;strong&gt;automated data generation&lt;/strong&gt; via close-range photogrammetry models with a view for future cultural heritage activities, such as digital identification, sorting and asset management.
&lt;br clear=&#34;left&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advisors&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://www.nottingham.ac.uk/news/expertiseguide/computer-science-/professor-eugene-chng.aspx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Prof. Eugene Ch&amp;rsquo;ng&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Timeline&lt;/strong&gt;: Apr. 2020 - Jun. 2020&lt;br&gt;
&lt;strong&gt;Key Words&lt;/strong&gt;: Data Augmentation, Object Detection, Digital Heritage, Photogrammetry&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://www.researchgate.net/publication/349209603_Balancing_Performance_and_Effort_in_Deep_Learning_via_the_Fusion_of_Real_and_Synthetic_Cultural_Heritage_Photogrammetry_Training_Sets&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Paper - &lt;em&gt;The 13th International Conference on Agents and Artificial Intelligence&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;technology-patents&#34;&gt;Technology Patents&lt;/h2&gt;
&lt;h3 id=&#34;automobile-ble-localization&#34;&gt;Automobile BLE Localization&lt;/h3&gt;
&lt;img src=&#34;BLE.jpg&#34; align=&#34;left&#34; width=&#34;285px&#34; style=&#34;padding-right: 15px&#34;/&gt;
&lt;p&gt;This project was part of the work of my internship at &lt;a class=&#34;link&#34; href=&#34;https://www.nio.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;NIO&lt;/strong&gt;&lt;/a&gt;. It&amp;rsquo;s about the optimization of BLE positioning for &lt;strong&gt;&amp;ldquo;Passive Entry Passive Start (PEPS)&amp;rdquo; strategy&lt;/strong&gt;.  The algorithm was deployed in phone key and key fob of NIO car in order to shape a comfortable ingress-egress passenger experience. Due to NIO&amp;rsquo;s Confidentiality Agreement, the details will not be shown.
&lt;br clear=&#34;left&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Timeline&lt;/strong&gt;: Nov. 2021 - Apr. 2022&lt;br&gt;
&lt;strong&gt;Key Words&lt;/strong&gt;: Bluetooth Low Energy, Signal Localization, Optimization, Smart Key&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: Patent (Aceepted)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;projects&#34;&gt;Projects&lt;/h2&gt;
&lt;h3 id=&#34;traffic-scene-understanding&#34;&gt;Traffic Scene Understanding&lt;/h3&gt;
&lt;img src=&#34;BDD.jpg&#34; align=&#34;left&#34; width=&#34;285px&#34; style=&#34;padding-right: 15px&#34;/&gt;
&lt;p&gt;This research project was conducted at &lt;a class=&#34;link&#34; href=&#34;https://deepdrive.berkeley.edu/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;Berkeley DeepDrive&lt;/strong&gt;&lt;/a&gt;, where I worked as a student research assistant in the Summer of 2020. The work aims to define a framework that enables the scene understanding of the driving environment by a perception system. My contribution was to evaluate an inflated 3D convolutional neural network model (I3D) on &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;TITAN dataset&lt;/strong&gt;&lt;/a&gt; for identifying vehicle and pedestrian behavior, and compare the result with the scene understanding solution proposed by our team.
&lt;br clear=&#34;left&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advisors&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://path.berkeley.edu/ching-yao-chan&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Prof. Ching-Yao Chan&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;https://scholar.google.com/citations?user=QPf13fYAAAAJ&amp;amp;hl=en&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Dr. Pin Wang&lt;/a&gt;, &lt;a class=&#34;link&#34; href=&#34;https://path.berkeley.edu/yu-ke-li&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Dr. Yuke Li&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Timeline&lt;/strong&gt;: Jun. 2020 - Sept. 2020&lt;br&gt;
&lt;strong&gt;Key Words&lt;/strong&gt;: Action Detection, Trajectory Prediction, Spatial-temporal Learning&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/d/1EYFa-eiq-y-TT-wJG6en9Wds9xO7q8QV/view?usp=sharing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Internship Presentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>About</title>
        <link>https://tonyfpy.github.io/about/</link>
        <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
        
        <guid>https://tonyfpy.github.io/about/</guid>
        <description>&lt;img src=&#34;tony.jpg&#34; align=&#34;right&#34; width=&#34;285px&#34; style=&#34;padding-left: 20px&#34;/&gt;
Hi, I am so glad to see you here and thank you for your interests in learning more about me.
&lt;p&gt;Currently, I am pursuing a Master of Science degree @ &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.brown.edu/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Brown University&lt;/a&gt;&lt;/strong&gt; (one of the eight private &lt;strong&gt;Ivy League&lt;/strong&gt; universities) with a concentration in Computer Science. Previously, I completed my undergraduate study @ &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.nottingham.ac.uk/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;University of Nottingham&lt;/a&gt;&lt;/strong&gt; (&lt;strong&gt;top 100&lt;/strong&gt; in QS Global Ranking) with &lt;strong&gt;first-class honors&lt;/strong&gt; in Computer Science with Artificial Intelligence.&lt;/p&gt;
&lt;p&gt;I have experiences in software, data, AI, as well as user research. My main interest lies in the exploration of &lt;strong&gt;Human-AI Interaction&lt;/strong&gt; with cutting-edge technologies to shape a better user experience. My goal is to build &lt;strong&gt;intelligent interactive systems&lt;/strong&gt; that are fit for the strengths and limitations of human cognition, perception and behavior.&lt;/p&gt;
&lt;p&gt;I love to work with people from different backgrounds to solve real-world problems with interdisciplinary knowledge. I believe this process can always boost my creativity, critical thinking, and knowledge transfer.
&lt;br clear=&#34;right&#34;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDUCATION&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;M.Sc. Computer Science @ &lt;a class=&#34;link&#34; href=&#34;https://www.brown.edu/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;Brown University&lt;/strong&gt;&lt;/a&gt;, &lt;em&gt;09/2022 - Present&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;B.Sc. Computer Science with AI @ &lt;a class=&#34;link&#34; href=&#34;https://www.nottingham.ac.uk/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;University of Nottingham&lt;/strong&gt;&lt;/a&gt;, &lt;em&gt;09/2017 - 08/2021&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;WORK EXPERIENCES&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Research &amp;amp; Development Engineer Intern (Automobile Software) @ &lt;a class=&#34;link&#34; href=&#34;https://www.nio.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;NIO&lt;/strong&gt;&lt;/a&gt;, &lt;em&gt;11/2021 - 08/2022&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Software Engineer Intern (Video Streaming) @ &lt;a class=&#34;link&#34; href=&#34;https://www.juphoon.com/en/about/aboutus.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;Juphoon System Software&lt;/strong&gt;&lt;/a&gt;, &lt;em&gt;07/2018 - 09/2018&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;RESEARCH EXPERIENCES&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Student Research Assistant @ &lt;a class=&#34;link&#34; href=&#34;https://www.nottingham.ac.uk/research/groups/mixedrealitylab/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;Mixed Reality Lab&lt;/strong&gt;&lt;/a&gt; (University of Nottingham), &lt;em&gt;06/2021 - 011/2021&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Student Research Assistant @ &lt;a class=&#34;link&#34; href=&#34;https://www.nottingham.edu.cn/en/research-centres/nvidia-joint-lab/about.aspx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;NVIDIA Joint-Lab on Mixed Reality&lt;/strong&gt;&lt;/a&gt; (University of Nottingham Ningbo China), &lt;em&gt;09/2020 - 05/2021&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Student Research Assistant @ &lt;a class=&#34;link&#34; href=&#34;https://deepdrive.berkeley.edu/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;Berkeley DeepDrive&lt;/strong&gt;&lt;/a&gt; (University of California, Berkeley), &lt;em&gt;06/2020 - 09/2020&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;CONTACT&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;fengpinyuan@gmail.com&#34; &gt;My Email&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;center&gt;Brown University&lt;/center&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/about/Brown_1.jpg&#34;
	width=&#34;375&#34;
	height=&#34;500&#34;
	srcset=&#34;https://tonyfpy.github.io/about/Brown_1_huf1886a3aa72b687d4b1b4845d86166ad_52134_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/about/Brown_1_huf1886a3aa72b687d4b1b4845d86166ad_52134_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;75&#34;
		data-flex-basis=&#34;180px&#34;
	
&gt;  &lt;img src=&#34;https://tonyfpy.github.io/about/Brown_2.jpg&#34;
	width=&#34;500&#34;
	height=&#34;375&#34;
	srcset=&#34;https://tonyfpy.github.io/about/Brown_2_hubdba236f0db678155a5e5e61aea52b5e_66565_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/about/Brown_2_hubdba236f0db678155a5e5e61aea52b5e_66565_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt; &lt;img src=&#34;https://tonyfpy.github.io/about/Brown_4.jpg&#34;
	width=&#34;375&#34;
	height=&#34;500&#34;
	srcset=&#34;https://tonyfpy.github.io/about/Brown_4_hu434b30b316422a1b1c0cb46228f0e0d6_27294_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/about/Brown_4_hu434b30b316422a1b1c0cb46228f0e0d6_27294_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;75&#34;
		data-flex-basis=&#34;180px&#34;
	
&gt;  &lt;img src=&#34;https://tonyfpy.github.io/about/Brown_3.jpg&#34;
	width=&#34;500&#34;
	height=&#34;375&#34;
	srcset=&#34;https://tonyfpy.github.io/about/Brown_3_hu6b20be9db27d563c4ad207a12189909a_67302_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/about/Brown_3_hu6b20be9db27d563c4ad207a12189909a_67302_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;center&gt;University of Nottingham&lt;/center&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/about/UoN_1.jpg&#34;
	width=&#34;1000&#34;
	height=&#34;750&#34;
	srcset=&#34;https://tonyfpy.github.io/about/UoN_1_hu070637a17326c7dba29e4caaea7b23a1_87704_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/about/UoN_1_hu070637a17326c7dba29e4caaea7b23a1_87704_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;  &lt;img src=&#34;https://tonyfpy.github.io/about/UoN_4.jpg&#34;
	width=&#34;1000&#34;
	height=&#34;750&#34;
	srcset=&#34;https://tonyfpy.github.io/about/UoN_4_hu006f7d380fecf1e3c476b79b75753a95_79539_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/about/UoN_4_hu006f7d380fecf1e3c476b79b75753a95_79539_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt; &lt;img src=&#34;https://tonyfpy.github.io/about/UoN_3.jpg&#34;
	width=&#34;1000&#34;
	height=&#34;750&#34;
	srcset=&#34;https://tonyfpy.github.io/about/UoN_3_hu1563c90af76baeec87afada92aebcb22_85558_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/about/UoN_3_hu1563c90af76baeec87afada92aebcb22_85558_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;  &lt;img src=&#34;https://tonyfpy.github.io/about/UoN_2.jpg&#34;
	width=&#34;1000&#34;
	height=&#34;750&#34;
	srcset=&#34;https://tonyfpy.github.io/about/UoN_2_hu50691e145aed3669f51b9d19abd22da3_135115_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/about/UoN_2_hu50691e145aed3669f51b9d19abd22da3_135115_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Projects</title>
        <link>https://tonyfpy.github.io/projects/</link>
        <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
        
        <guid>https://tonyfpy.github.io/projects/</guid>
        <description>&lt;h2 id=&#34;interactive-audio-adventure&#34;&gt;Interactive Audio Adventure&lt;/h2&gt;
&lt;img src=&#34;GRP.jpeg&#34; align=&#34;left&#34; width=&#34;285px&#34; style=&#34;padding-right: 15px&#34;/&gt;
&lt;p&gt;The Software Engineering Group Project is about developing a story-based interactive audio game for people with vision loss. With the collaboration with our industry sponsor &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.ibm.com/watson/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;IBM&lt;/a&gt;&lt;/strong&gt;, our game is powered by IBM Watson Assistant (WA), Text-to-Speech (T2S) and Speech-To-Text (S2T), making the game experience engaging and joyful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Java&lt;/strong&gt; is our optimal choice to apply MVC design pattern, making the project maintainable and extendable.&lt;/p&gt;
&lt;br clear=&#34;left&#34;/&gt;
&lt;h2 id=&#34;face-matching&#34;&gt;Face Matching&lt;/h2&gt;
&lt;img src=&#34;CV.jpeg&#34; align=&#34;right&#34; width=&#34;320px&#34;/&gt;
&lt;p&gt;In this project, traditional approaches and a deep learning approach for face matching are evaluated based on a public facial dataset.&lt;/p&gt;
&lt;p&gt;The main steps to match the facial images consist of: 1) Facial Localisation 2) Facial Feature Extraction 3) Facial Classification. The implementations are based on &lt;strong&gt;MATLAB&lt;/strong&gt; and &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt;
&lt;br clear=&#34;right&#34;/&gt;
&lt;h2 id=&#34;cell-nuclei-identification&#34;&gt;Cell Nuclei Identification&lt;/h2&gt;
&lt;img src=&#34;IIP.png&#34; align=&#34;left&#34; width=&#34;300px&#34; style=&#34;padding-right: 15px&#34;/&gt; 
&lt;p&gt;This project is about identify the cell nuclei from a set of confocal microscope images implemented based on &lt;strong&gt;MATLAB&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A GUI is designed to allow user interaction and make comparisons between different techniques.&lt;/p&gt;
&lt;br clear=&#34;left&#34;/&gt;
&lt;h2 id=&#34;maze-solver&#34;&gt;Maze Solver&lt;/h2&gt;
&lt;img src=&#34;FAI.png&#34; align=&#34;right&#34; width=&#34;300px&#34;/&gt; 
&lt;p&gt;The project is about solving the maze using A*, DFS, Greedy algorithm respectively.&lt;/p&gt;
&lt;p&gt;The project is implemented in &lt;strong&gt;MATLAB&lt;/strong&gt;, where a GUI is implemented to visualize the searching process. The red lines represents the solutions attempted by the algorithm, while the black lines means the final route.&lt;/p&gt;
&lt;br clear=&#34;right&#34;/&gt;
&lt;h2 id=&#34;route-planning-and-optimization&#34;&gt;Route Planning and Optimization&lt;/h2&gt;
&lt;img src=&#34;AIM.png&#34; align=&#34;left&#34; width=&#34;280px&#34; style=&#34;padding-right: 15px&#34;/&gt;
&lt;p&gt;The project aims to design a solution to the Postal Worker Problem (PWP). The objective is to find a route which ensures that all postal deliveries are made exactly once such that the total tour length is minimised.&lt;/p&gt;
&lt;p&gt;In the project, &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://link.springer.com/chapter/10.1007/978-3-642-29124-1_12&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;HyFlex&lt;/a&gt;&lt;/strong&gt;, a &lt;strong&gt;Java-based&lt;/strong&gt; software framework for the development of cross-domain search methodologies, is utilized to implement and test the iterative general purpose search methods.&lt;/p&gt;
&lt;br clear=&#34;left&#34;/&gt;
&lt;h2 id=&#34;frogger&#34;&gt;Frogger&lt;/h2&gt;
&lt;img src=&#34;SWM.png&#34; align=&#34;right&#34; width=&#34;280px&#34;/&gt;
&lt;p&gt;This project is about maintaining and extending a re-implementation of a classic retro game Frogger.&lt;/p&gt;
&lt;p&gt;For maintenance part, I optimize the file structure, apply Java properties and apply the design patterns to make the program maintainable.&lt;/p&gt;
&lt;p&gt;In additin to maintenance, I extend the game by adding different game elements, beautifying the UI, re-designing the game flow to improve the interacive game experience .&lt;/p&gt;
&lt;p&gt;You could watch this &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=456PU1yNJ3w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;video&lt;/a&gt;&lt;/strong&gt; to see how the game is refactored.&lt;/p&gt;
&lt;br clear=&#34;right&#34;/&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>https://tonyfpy.github.io/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://tonyfpy.github.io/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Search</title>
        <link>https://tonyfpy.github.io/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://tonyfpy.github.io/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
