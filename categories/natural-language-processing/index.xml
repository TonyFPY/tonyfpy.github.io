<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Natural Language Processing on Tony Feng</title>
        <link>https://tonyfpy.github.io/categories/natural-language-processing/</link>
        <description>Recent content in Natural Language Processing on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language><atom:link href="https://tonyfpy.github.io/categories/natural-language-processing/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Comp Linguistics] Part of Speech Tagging</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/</link>
        <pubDate>Thu, 10 Nov 2022 14:26:20 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Nov 10th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Nov 10th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-of-speech-tags&#34;&gt;Part of Speech Tags&lt;/h2&gt;
&lt;h3 id=&#34;sequence-labeling-tasks&#34;&gt;Sequence Labeling Tasks&lt;/h3&gt;
&lt;p&gt;In linguistics, tags are &lt;strong&gt;descriptive&lt;/strong&gt; to help us understand the constraints on grammar and meaning. In NLP, tags are used as &lt;strong&gt;features&lt;/strong&gt;, which provide information that is important for understanding sentence, anticipating next words, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;POS Tagging&lt;/strong&gt;: NOUN, VERB, ADJ, &amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Named Entity Recognition (NER)&lt;/strong&gt;: PERSON, LOCATION, ORG, &amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Given words, predict tags&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ambiguity&#34;&gt;Ambiguity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One word may have more than one possible tag.&lt;/li&gt;
&lt;li&gt;It is usually easy to tell from context.&lt;/li&gt;
&lt;li&gt;Most frequent class baseline: when ambiguous, guess the most likely tag given the training data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/ambiguity.jpg&#34;
	width=&#34;1334&#34;
	height=&#34;514&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/ambiguity_hue872b508020aa00334981c3ef6cf28ee_39071_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/ambiguity_hue872b508020aa00334981c3ef6cf28ee_39071_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;259&#34;
		data-flex-basis=&#34;622px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;markov-models&#34;&gt;Markov Models&lt;/h2&gt;
&lt;h3 id=&#34;markov-chains&#34;&gt;Markov Chains&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Markov Assumption: state $t+1$ depends only on state $t$.
&lt;ul&gt;
&lt;li&gt;$P(t+1 \mid t_0, &amp;hellip;,t)=P(t+1 \mid t)$&lt;/li&gt;
&lt;li&gt;Similar to a bigram language model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Markov Chain is a simple model based on the assumption.&lt;/li&gt;
&lt;li&gt;Formal Def
&lt;ul&gt;
&lt;li&gt;$Q=\{ q_{1}, q_{2}, &amp;hellip;, q_{N}\}$: a set of states&lt;/li&gt;
&lt;li&gt;$A=\{a_{i,j}\}$: a transition probability matrix&lt;/li&gt;
&lt;li&gt;$\pi=\{\pi_{1}, \pi_{2}, \ldots \pi_{N}\}$: an initial probability distribution over states&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white_hu36e283984737934d1af0bcfe1c0c61db_1764_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white_hu36e283984737934d1af0bcfe1c0c61db_1764_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;55&#34;
		data-flex-basis=&#34;132px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/MC.jpg&#34;
	width=&#34;1290&#34;
	height=&#34;700&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/MC_hu40eb4476e9f9fb4b29e5fd37d772dfa8_106102_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/MC_hu40eb4476e9f9fb4b29e5fd37d772dfa8_106102_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;184&#34;
		data-flex-basis=&#34;442px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white_hu36e283984737934d1af0bcfe1c0c61db_1764_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white_hu36e283984737934d1af0bcfe1c0c61db_1764_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;55&#34;
		data-flex-basis=&#34;132px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;hidden-markov-models&#34;&gt;Hidden Markov Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hidden or Latent factors are not directly observed, but they can
nonetheless influence events which are observed.
&lt;ul&gt;
&lt;li&gt;Observed = func(Latent)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two Assumptions in HMM
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Markov&lt;/strong&gt;: $P(q_{t+1} \mid q_{0} \ldots q_{t})=P(q_{t+1} \mid q_{t})$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output Independence&lt;/strong&gt;: $P(o_t \mid o_0 \ldots o_n, q_0&amp;hellip;q_n)=P(o_t \mid q_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Formal Def
&lt;ul&gt;
&lt;li&gt;$Q=\{ q_{1}, q_{2}, &amp;hellip;, q_{N}\}$: a set of states&lt;/li&gt;
&lt;li&gt;$A=\{a_{i,j}\}$: a transition probability matrix&lt;/li&gt;
&lt;li&gt;$\pi=\{\pi_{1}, \pi_{2}, \ldots \pi_{N}\}$: an initial&lt;/li&gt;
&lt;li&gt;$O=\{o_{1}, o_{2}, &amp;hellip;, o_{N}\}$: a set of observations&lt;/li&gt;
&lt;li&gt;$B=\{b_{i,j}\}$: an emission probability matrix specifying probability of observation $o_j$ given state $q_i$.&lt;/li&gt;
&lt;li&gt;$\hat{q} = \text{argmax}_{Q} P(Q \mid O)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To find $\hat{q}$, we can convert the problem into $\hat{q} = \text{argmax}_{Q} P(O \mid Q) P(Q)$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Markov (&lt;strong&gt;Emission&lt;/strong&gt;): $P(Q)\approx \prod_{i=1}^{n} P\left(q_{i} \mid q_{i-1}\right)$&lt;/li&gt;
&lt;li&gt;Output Independence (&lt;strong&gt;Transition&lt;/strong&gt;): $P(O \mid Q)\approx \prod_{i=1}^{n} P\left(o_{i} \mid q_{i}\right)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white_hu36e283984737934d1af0bcfe1c0c61db_1764_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white_hu36e283984737934d1af0bcfe1c0c61db_1764_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;55&#34;
		data-flex-basis=&#34;132px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/HMM.jpg&#34;
	width=&#34;1288&#34;
	height=&#34;712&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/HMM_hu49b520efb2e95a6ee537b02ec5b1b836_171435_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/HMM_hu49b520efb2e95a6ee537b02ec5b1b836_171435_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;434px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white_hu36e283984737934d1af0bcfe1c0c61db_1764_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/white_hu36e283984737934d1af0bcfe1c0c61db_1764_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;55&#34;
		data-flex-basis=&#34;132px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;In language, &lt;strong&gt;POS is latent&lt;/strong&gt;, it is not observed. So, we can use an HMM to find the most likely POS sequence for a sentence. We can estimated by manually tagging a corpus and then counting up &lt;strong&gt;occurrences&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;viterbi-decoding&#34;&gt;Viterbi Decoding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Viterbi Algorithm is used to maximize $P(o_i \mid q_i)P(q_i|q_{i-1})$.&lt;/li&gt;
&lt;li&gt;It is a dynamic programming algorithm for &lt;strong&gt;decoding&lt;/strong&gt; (finding the most likely output sequece).&lt;/li&gt;
&lt;li&gt;Formal Def&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$M[j][t]={max}_{i}^{n} M[i][t-1]*A[i][j]*B[j][t]$$&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- Maintain $M$: a matrix with # of states x # of outputs
- $M[i][j]$ represents the probability of being in state $i$ after seeing outputs $0…j$
- $j$ is the current state (tag)
- $i$ is the previous state (tag)
- $t$ is current observation (word)
- $A$ is transition
- $B$ is emission
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conditional-random-fields&#34;&gt;Conditional Random Fields&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;HMMs struggle with unknown words.&lt;/li&gt;
&lt;li&gt;It is hard to incorporate all these features into an HMM.&lt;/li&gt;
&lt;li&gt;Using discriminative (log-linear) models to model the condition distribution directly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/CRF.jpg&#34;
	width=&#34;1292&#34;
	height=&#34;448&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/CRF_hu49b520efb2e95a6ee537b02ec5b1b836_75238_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-part-of-speech-tagging/CRF_hu49b520efb2e95a6ee537b02ec5b1b836_75238_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;288&#34;
		data-flex-basis=&#34;692px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Neural Machine Translation</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-neural-machine-translation/</link>
        <pubDate>Thu, 03 Nov 2022 14:33:24 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-neural-machine-translation/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Nov 3rd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Nov 10th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mt-evaluation&#34;&gt;MT Evaluation&lt;/h2&gt;
&lt;h3 id=&#34;human-evalution&#34;&gt;Human Evalution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Explicit ratings for fluency and faithfulness&lt;/li&gt;
&lt;li&gt;Most reliable, but expensive to collect&lt;/li&gt;
&lt;li&gt;New collections are needed for each system&lt;/li&gt;
&lt;li&gt;Can’t be “hill climbed”&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;automatic-evaluation&#34;&gt;Automatic Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NLP prefers automatic eval for standardization and optimization.&lt;/li&gt;
&lt;li&gt;Popular metrics for MT: BLEU, ESIM, BLEURT.&lt;/li&gt;
&lt;li&gt;However, once the system is sufficiently good, metrics stop correlating with human judgements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bleu&#34;&gt;BLEU&lt;/h3&gt;
&lt;p&gt;$$
BLEU=BP \times \exp \left(\frac{1}{N} \sum_{n=1}^{N} \log p_{n}\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assume we have an MT output (&lt;strong&gt;Candidate&lt;/strong&gt;) and are comparing against multiple human-generated translations (&lt;strong&gt;Reference&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Intuition: We should reward models for producing translations that
contains lots of the same words/phrases as the references.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
p_{n}=\frac{\sum_{c \in \text{cand}} \sum_{ngm \in c} {count_{clip}}  (ngm)}{\sum_{c^{\prime} \in \text{cand}} \sum_{ngm^{\prime} \in c^{\prime}} \operatorname{count}\left(ngm^{\prime}\right)}
$$&lt;/p&gt;
&lt;p&gt;$$
BP = 1 \text{ if } c&amp;gt;r \text{ else } e^{(1-r) / c}  \text { if } c \leq r
$$&lt;/p&gt;
&lt;p&gt;, where $BP$ is brevity precision and $p_n$ is weighted n-gram precision.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;neural-mt&#34;&gt;Neural MT&lt;/h2&gt;
&lt;h3 id=&#34;encoder-decoder-model&#34;&gt;Encoder-Decoder Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It refers to &amp;ldquo;sequence to sequence (seq2seq)&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Intuition
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Conditional&amp;rdquo; text generation/language modeling.&lt;/li&gt;
&lt;li&gt;The output is dependent on some input.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Examples
&lt;ul&gt;
&lt;li&gt;RNN Encoder-Decoder&lt;/li&gt;
&lt;li&gt;Transformer Encoder-Decoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Many other models are inspired by this structures
&lt;ul&gt;
&lt;li&gt;Encoder-Decoder: Original Transformer Model (Vaswani et al, 2017)&lt;/li&gt;
&lt;li&gt;Encoder-only: BERT and variants (ALBERT, DistilBERT, RoBERTa)&lt;/li&gt;
&lt;li&gt;Decoder-only (i.e., auto-regressive): GPT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;multilingual-lm&#34;&gt;Multilingual LM&lt;/h2&gt;
&lt;h3 id=&#34;cross-lingual-transfer&#34;&gt;Cross Lingual Transfer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Goal: train on one language but work in all languages&lt;/li&gt;
&lt;li&gt;Intuition: if the model learns a good representation, it should be able to map the training it receives in one language to any other language.&lt;/li&gt;
&lt;li&gt;Requirements: unlabeled, monolingual data&lt;/li&gt;
&lt;li&gt;Better transfer for languages that are more &lt;strong&gt;typologivally&lt;/strong&gt; similar and more &lt;strong&gt;syntactically&lt;/strong&gt; similar.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;common-multilingual-models&#34;&gt;Common Multilingual Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;mBERT&lt;/li&gt;
&lt;li&gt;XLM-RoBERTa&lt;/li&gt;
&lt;li&gt;mGPT&lt;/li&gt;
&lt;li&gt;BLOOM&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/55365040&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;NLP度量指标BELU真的完美么？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_31584157/article/details/77709454&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;机器翻译自动评估-BLEU算法详解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://coladrill.github.io/2018/10/20/%E6%B5%85%E8%B0%88BLEU%E8%AF%84%E5%88%86/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;浅谈BLEU评分&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://easyai.tech/ai-definition/encoder-decoder-seq2seq/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Encoder-Decoder 和 Seq2Seq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://coladrill.github.io/2018/09/05/Attention%E6%9C%BA%E5%88%B6/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Attention机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://imzhanghao.com/2021/09/01/attention-mechanism/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Attention机制的基本思想与实现原理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Statistical Machine Translation</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-statistical-machine-translation/</link>
        <pubDate>Tue, 01 Nov 2022 14:34:47 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-statistical-machine-translation/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Nov 1st, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Nov 10th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;language-diversity-and-challenges&#34;&gt;Language Diversity and Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perfect translation is not possible&lt;/li&gt;
&lt;li&gt;Morphology
&lt;ul&gt;
&lt;li&gt;Isolating vs. polysynthetic&lt;/li&gt;
&lt;li&gt;Agglutinative vs. fusional languages&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Syntax&lt;/li&gt;
&lt;li&gt;Agument structure and marking
&lt;ul&gt;
&lt;li&gt;Possession&lt;/li&gt;
&lt;li&gt;Motion, manner&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Referential Density&lt;/li&gt;
&lt;li&gt;Lexicon: Every language has both polysemy and redundancy&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;noisy-channel-models-for-smt&#34;&gt;Noisy Channel Models for SMT&lt;/h2&gt;
&lt;h3 id=&#34;noisy-channel-model&#34;&gt;Noisy Channel Model&lt;/h3&gt;
&lt;p&gt;Consider a message sent across a noisy channel. To determine the message, you need to correct for the noise that was added.&lt;/p&gt;
&lt;p&gt;$$
P(\operatorname{tgt} \mid \operatorname{src}) \propto P(\operatorname{src} \mid \operatorname{tgt}) P(\operatorname{tgt})
$$&lt;/p&gt;
&lt;p&gt;,where $P(\operatorname{src} \mid \operatorname{tgt})$ is &lt;strong&gt;translation model&lt;/strong&gt; and $P(\operatorname{tgt})$ is &lt;em&gt;&lt;strong&gt;language model&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;translation-model&#34;&gt;Translation Model&lt;/h3&gt;
&lt;h4 id=&#34;word-alignment&#34;&gt;Word Alignment&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Bitext / Bilingual Parallel Corpora&lt;/strong&gt; is a corpus that contains translated pairs of texts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Document-level or Sentence level&lt;/li&gt;
&lt;li&gt;Word-level alignment is rare&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ibm-model-1&#34;&gt;IBM Model 1&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;IBM Model 1&lt;/strong&gt; is the 1st (simplest) automatic unsupervised alignment model from IBM.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose length $L$ for source sentence&lt;/li&gt;
&lt;li&gt;Choose an alignment $A = a_1 … a_L$&lt;/li&gt;
&lt;li&gt;Generate target position $t_i$ by translating whatever source phrase is
aligned to position $i$ in the target&lt;/li&gt;
&lt;li&gt;Find the alignment that makes each observed source word most likely
given its aligned target word&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dilemma:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We want alignments so that we can figure out the probabilities of
phrase translations&lt;/li&gt;
&lt;li&gt;To estimate those alignments, we need phrase translation probabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;em-algorithm&#34;&gt;EM Algorithm&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=7e65vXZEv5Q&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Expectation Maximization Algorithm&lt;/a&gt;&lt;/strong&gt; is an optimization algorithm for generative models. It works for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $P(a)$, then $P(b)$ can be computed.&lt;/li&gt;
&lt;li&gt;If $P(b)$, the $P(a)$ can be computed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Basic idea:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Randomly guess what $P(a)$ is&lt;/li&gt;
&lt;li&gt;Use it to compute $P(b)$&lt;/li&gt;
&lt;li&gt;Then, with your newly computed $P(b)$, recompute $P(a)$&lt;/li&gt;
&lt;li&gt;Iterate until convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;language-model&#34;&gt;Language Model&lt;/h3&gt;
&lt;h4 id=&#34;greedy-search&#34;&gt;Greedy Search&lt;/h4&gt;
&lt;p&gt;Finding best translation (e.g., arg-maxing $P(s|t)P(t)$) is just a search
problem!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Greedy Search (Just for intuition)&lt;/li&gt;
&lt;li&gt;Beam Search (more common)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;beam-search&#34;&gt;Beam Search&lt;/h4&gt;
&lt;p&gt;Finding the most optimal sequence is computationally hard, since the search space increases exponentially. Beam Search considers the &lt;strong&gt;top K&lt;/strong&gt; at each time step.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At first time step, choose k most likely words to be the initial
&amp;ldquo;hypotheses&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Theb, the top $K$ best hypotheses are carried forward, (i.e., softmax over the whole vocab given each hypothesis, so $K*V$ computations)&lt;/li&gt;
&lt;li&gt;When &amp;lt;/s&amp;gt; is generated, the generation is output.&lt;/li&gt;
&lt;li&gt;Continue until beam is empty&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heuristic and still not guaranteed to find best solution.&lt;/li&gt;
&lt;li&gt;Hard to deal with long range dependencies.&lt;/li&gt;
&lt;li&gt;Slow and memory intensive.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, we often apply sampling &lt;strong&gt;$K$ continuations&lt;/strong&gt;, rather than choosing the top $K$, which is controled by a &lt;strong&gt;temperature&lt;/strong&gt; parameter.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Higher temperature = more oversampling of lower probability
continuations&lt;/li&gt;
&lt;li&gt;A low temperature (below 1) makes the model more confident.&lt;/li&gt;
&lt;li&gt;A high temperature (above 1) makes the model less confident.&lt;/li&gt;
&lt;li&gt;Intuition: &amp;ldquo;flatter&amp;rdquo; distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-statistical-machine-translation/softmax_temperature.jpg&#34;
	width=&#34;1207&#34;
	height=&#34;282&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-statistical-machine-translation/softmax_temperature_hu682d865c054f4cd4e325158e801c2f26_26789_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-statistical-machine-translation/softmax_temperature_hu682d865c054f4cd4e325158e801c2f26_26789_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;softmax with temperature&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;428&#34;
		data-flex-basis=&#34;1027px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;full-phrase-based-mt-system&#34;&gt;Full Phrase-Based MT System&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-statistical-machine-translation/PBMT.jpg&#34;
	width=&#34;738&#34;
	height=&#34;288&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-statistical-machine-translation/PBMT_hu55f186897c0d8e3ee66e5171f85b8615_35761_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-statistical-machine-translation/PBMT_hu55f186897c0d8e3ee66e5171f85b8615_35761_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;256&#34;
		data-flex-basis=&#34;615px&#34;
	
&gt;
$$
\operatorname{cost}(E, F)=\prod_{i \in S} \phi\left(\overline{f_{i}}, \overline{e_{i}}\right) d\left(a_{i}-b_{i-1}\right) P(E)
$$&lt;/p&gt;
&lt;p&gt;The cost is the product of all positions in the partial sentence, including translation probability, distortion probability, and language model probability.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Reference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=7e65vXZEv5Q&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Expectation Maximization Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/blog/how-to-generate&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;How to generate text: using different decoding methods for language generation with Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.kasimte.com/2020/02/14/how-does-temperature-affect-softmax-in-machine-learning.html#:~:text=What%20is%20softmax%20with%20temperature,makes%20the%20model%20less%20confident.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;How does temperature affect softmax in machine learning?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Language Modeling - Pretraining</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/</link>
        <pubDate>Thu, 20 Oct 2022 14:28:33 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 20th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 25th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-pretraining&#34;&gt;What is pretraining?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Train on some &lt;strong&gt;&amp;ldquo;general&amp;rdquo;&lt;/strong&gt; task that encourages good representations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transfer&lt;/strong&gt; these representations with little updating to other tasks&lt;/li&gt;
&lt;li&gt;No need to train embeddings anew for every task&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;contextualized-word-representations&#34;&gt;Contextualized Word Representations&lt;/h2&gt;
&lt;h3 id=&#34;basic-idea&#34;&gt;Basic Idea&lt;/h3&gt;
&lt;p&gt;Traditional word embedding methods (e.g., word2vec) produce typelevel representations, while contextualized word embeddings (e.g., BERT, ELMo) produce token-level representations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/SC.jpg&#34;
	width=&#34;1186&#34;
	height=&#34;530&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/SC_hu52f3c2037d1b7b578ab83e07beae61bd_53097_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/SC_hu52f3c2037d1b7b578ab83e07beae61bd_53097_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Static VS. Contextualized&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;223&#34;
		data-flex-basis=&#34;537px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;advantages&#34;&gt;Advantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It can capture word sense&lt;/li&gt;
&lt;li&gt;It can capture syntactic and semantic context&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;disadvantages&#34;&gt;Disadvantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Variable-length sentence representations&lt;/li&gt;
&lt;li&gt;No clear &amp;ldquo;lexicon&amp;rdquo;, which (traditionally) linguistics likes to have&lt;/li&gt;
&lt;li&gt;More training data needed&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elmo&#34;&gt;ELMo&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/ELMo.jpg&#34;
	width=&#34;756&#34;
	height=&#34;452&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/ELMo_hu09aa66db2e32ffe6d47638c662814a54_35780_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/ELMo_hu09aa66db2e32ffe6d47638c662814a54_35780_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;ELMo&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;167&#34;
		data-flex-basis=&#34;401px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Architecture: Two-layer BiLSTM&lt;/li&gt;
&lt;li&gt;Layer 0 = pretrained static embeddings (Glove)&lt;/li&gt;
&lt;li&gt;Trained on vanilla language modeling task&lt;/li&gt;
&lt;li&gt;Finetuned by learning a simple linear combination of the learned
embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bert&#34;&gt;BERT&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/BERT.jpg&#34;
	width=&#34;814&#34;
	height=&#34;678&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/BERT_hu2b56a0421f2b3e40ccfe960e48fcc658_56061_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/BERT_hu2b56a0421f2b3e40ccfe960e48fcc658_56061_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;BERT&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;288px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Architecture: Deep Transformer (small = 12, large = 24)&lt;/li&gt;
&lt;li&gt;Layer 0 = wordpiece embeddings&lt;/li&gt;
&lt;li&gt;Trained on masked language modeling + next-sentence prediction&lt;/li&gt;
&lt;li&gt;Typically finetuned by updating all parameters (though there are other
strategies)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gpt&#34;&gt;GPT&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/GPT.jpg&#34;
	width=&#34;992&#34;
	height=&#34;422&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/GPT_hu91026a3bf37eb04acd08f5a3cbf6ac39_71104_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/GPT_hu91026a3bf37eb04acd08f5a3cbf6ac39_71104_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;GPT&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;564px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-pretraining/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Architecture: Deep Transformer&lt;/li&gt;
&lt;li&gt;Layer 0 = BPE&lt;/li&gt;
&lt;li&gt;Trained on vanilla language modeling&lt;/li&gt;
&lt;li&gt;Notable because the recent versions (GPT-3) are HUGE, and very
impressive&lt;/li&gt;
&lt;li&gt;Typically finetuned by updating all parameters (for the small models)
or (for the large models) &lt;strong&gt;prompting&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;different-transfer-methods&#34;&gt;Different Transfer Methods&lt;/h2&gt;
&lt;h3 id=&#34;frozen&#34;&gt;Frozen&lt;/h3&gt;
&lt;p&gt;Just pool representations and train a new classifier on top。&lt;/p&gt;
&lt;h3 id=&#34;finetuning&#34;&gt;Finetuning&lt;/h3&gt;
&lt;p&gt;Treat pretraining as a good initialization and continue to update all parameters on new tasks. Models perform better and require less data to learn the target task.&lt;/p&gt;
&lt;h3 id=&#34;prompt-tuning&#34;&gt;&amp;ldquo;Prompt&amp;rdquo; Tuning&lt;/h3&gt;
&lt;p&gt;Update a small number of parameters at the bottom of the network.&lt;/p&gt;
&lt;h3 id=&#34;adapters&#34;&gt;Adapters&lt;/h3&gt;
&lt;p&gt;Update a small number of parameters throughout the network.&lt;/p&gt;
&lt;h3 id=&#34;zero-shot-or-in-context-learning&#34;&gt;Zero-Shot or &amp;ldquo;In-Context&amp;rdquo; Learning&lt;/h3&gt;
&lt;p&gt;Cast all tasks as an instance of the task that the model was trained on (e.g., language modeling).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Results are high variance, depending on exact wording of the prompts.&lt;/li&gt;
&lt;li&gt;Unclear what was seen during training, so hard to know how much they are truly learning and generalizing vs. parroting.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ch3nye.top/ELMO-BERT-GPT/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ELMO, BERT, GPT Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1uF411Y7F7/?spm_id_from=333.337.search-card.all.click&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;图解BERT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Language Modeling - Neural Architecture</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/</link>
        <pubDate>Tue, 18 Oct 2022 14:32:35 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 18th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 20th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;multi-layer-perceptron&#34;&gt;Multi-layer Perceptron&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MLP doesn’t readily support long, sequential inputs&lt;/li&gt;
&lt;li&gt;MLP doesn’t consider encoding word order, essentially a BOW model.&lt;/li&gt;
&lt;li&gt;Inputs either become muddy (adding everything together, i.e., &amp;ldquo;bag-of-vectors&amp;rdquo;) or too large (concatenating everything)&lt;/li&gt;
&lt;li&gt;&amp;ldquo;bag-of-vectors&amp;rdquo; classifiers are common and often work well for
&lt;strong&gt;basic&lt;/strong&gt; applications&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recurrent-neural-network-rnn&#34;&gt;Recurrent Neural Network (RNN)&lt;/h2&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;The basic idea is generation of word $i+1$ depends on word $i$ plus &amp;ldquo;memory&amp;rdquo; of words generated up to $i$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view1.jpg&#34;
	width=&#34;714&#34;
	height=&#34;628&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view1_hu2f49a735a03e56027346c1b27d8044b1_34408_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view1_hu2f49a735a03e56027346c1b27d8044b1_34408_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;&amp;ldquo;Unrolled&amp;rdquo;&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;113&#34;
		data-flex-basis=&#34;272px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view2.jpg&#34;
	width=&#34;358&#34;
	height=&#34;534&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view2_hu74f30de8a34c66e2143b7e317a6fab4e_11034_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view2_hu74f30de8a34c66e2143b7e317a6fab4e_11034_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;&amp;ldquo;Recurrent / Recursive&amp;rdquo;&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;67&#34;
		data-flex-basis=&#34;160px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view3.jpg&#34;
	width=&#34;568&#34;
	height=&#34;506&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view3_hu428cbdb151055c717b37789e2c1a1e84_8961_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/view3_hu428cbdb151055c717b37789e2c1a1e84_8961_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;&amp;ldquo;A single step of recurrent / recursive&amp;rdquo;&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;269px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;$$h_{t}=g\left(U h_{t-1}+W x_{t}\right)$$
$$y_{t}=f\left(V h_{t}\right)$$&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;function&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ForwardRNN&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rnn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sequence&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;h_0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;do&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;h_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;U&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;y_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;V&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;training-considerations&#34;&gt;Training Considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In practice, using unrolled and padded to a fixed length is better for batching.&lt;/li&gt;
&lt;li&gt;When producing word $i$, predict based on the &lt;strong&gt;real&lt;/strong&gt; $i-1$, not the
&lt;strong&gt;predicted&lt;/strong&gt; $i-1$ (which is likely wrong).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;long-short-term-memory-lstm&#34;&gt;Long-Short Term Memory (LSTM)&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RNNs struggle with &lt;strong&gt;long range dependencies&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vanishing gradients&lt;/strong&gt; makes it hard to update early hidden states
for long sequences.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;architecture-1&#34;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introducing a &lt;strong&gt;&amp;ldquo;gating&amp;rdquo; mechanisms&lt;/strong&gt; to manage the hidden state/
memory&lt;/li&gt;
&lt;li&gt;Passing through &amp;ldquo;gates&amp;rdquo;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Forget&amp;rdquo; gate removes information no longer needed&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Add&amp;rdquo; gate adds new information likely to be useful in the
future&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Output&amp;rdquo; gate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adding explicit previous &amp;ldquo;context&amp;rdquo; in addition to prior hidden state&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/lstm.jpg&#34;
	width=&#34;1400&#34;
	height=&#34;580&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/lstm_hu041f8b1d561fb25df604ac2817f56ef3_57530_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/lstm_hu041f8b1d561fb25df604ac2817f56ef3_57530_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LSTM Computation Graph&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;579px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gate&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn some mask (i.e., vector) via backpropagation&lt;/li&gt;
&lt;li&gt;Apply the mask(i.e., elementwise multiplication) to some hidden state&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Computation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute &amp;ldquo;current&amp;rdquo; state, &amp;ldquo;add&amp;rdquo; gate, &amp;ldquo;forget&amp;rdquo; gate, and &amp;ldquo;output&amp;rdquo; gate from previous hidden state and current input.
&lt;ul&gt;
&lt;li&gt;g = current state&lt;/li&gt;
&lt;li&gt;f = forget gate&lt;/li&gt;
&lt;li&gt;i = add gate&lt;/li&gt;
&lt;li&gt;o = output gate (controls what to output and retains the relavant info right now.)&lt;/li&gt;
&lt;li&gt;k = intermediate output (context after &amp;ldquo;forgetting&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;j = intermediate output (info to be added to the context)&lt;/li&gt;
&lt;li&gt;c = updated context&lt;/li&gt;
&lt;li&gt;h = updated hidden state&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Combine those things using Hadamard product.&lt;/li&gt;
&lt;li&gt;Update context and hidden state for next iteration.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$g=\tanh \left(U_{g} h_{t-1}+W_{g} x_{t}\right) $$
$$f=\operatorname{sigmoid}\left(U_{f} h_{t-1}+W_{f} x_{t}\right) $$
$$i_{t}=\operatorname{sigmoid}\left(U_{i} h_{t-1}+W_{i} x_{t}\right) $$
$$o_{t}=\operatorname{sigmoid}\left(U_{o} h_{t-1}+W_{o} x_{t}\right) $$
$$k_{t}=f_{t} \odot c_{t-1} $$
$$j_{t}=i_{t} \odot g_{t} $$
$$c_{t}=j_{t}+k_{t} $$
$$h_{t}=o_{t} \odot \tanh \left(c_{t}\right) $$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;transformer&#34;&gt;Transformer&lt;/h2&gt;
&lt;h3 id=&#34;architecture-2&#34;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;Representation of the word depends on (slightly less contextualized) representation of other words.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/transformer.jpg&#34;
	width=&#34;1270&#34;
	height=&#34;540&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/transformer_hub8fb5f921e84dda4eb3c5a79434a7612_34993_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/transformer_hub8fb5f921e84dda4eb3c5a79434a7612_34993_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Transformer&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;564px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;self-attention&#34;&gt;Self Attention&lt;/h3&gt;
&lt;p&gt;The idea is to learn a &lt;strong&gt;distribution/weighted combination&lt;/strong&gt; of hidden states that inform this hidden state.&lt;/p&gt;
&lt;p&gt;Each word has three roles at each timestep and we learn three weight matrices $(Q,K,V)$ to cast each word into each role. The &lt;strong&gt;dot product&lt;/strong&gt; of key and Query produces a weight and the next layer receives the weighted combination of values.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Query&lt;/strong&gt;: The word as the current focus&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key&lt;/strong&gt;: The word as a context word&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Value&lt;/strong&gt;: The word as part of the output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/self-attention.jpg&#34;
	width=&#34;1352&#34;
	height=&#34;1222&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/self-attention_hudff2f91419a60808ee0972ddc4b5d5f0_95851_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/self-attention_hudff2f91419a60808ee0972ddc4b5d5f0_95851_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Self-attention&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;110&#34;
		data-flex-basis=&#34;265px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Multi-threaded self-attention is repeating the attention process multiple times. Each KQV set can focus on a slightly different aspect of the representation.&lt;/p&gt;
&lt;h3 id=&#34;blocks&#34;&gt;Blocks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/TB.jpg&#34;
	width=&#34;1690&#34;
	height=&#34;914&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/TB_huc391e28b06d8b613b934126bcf6e40b9_84008_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/TB_huc391e28b06d8b613b934126bcf6e40b9_84008_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;A Transformer Block&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;184&#34;
		data-flex-basis=&#34;443px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Residual Connection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It adds input to output to help with training/vanishing gradients.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Layer Normalize&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It has the same idea of Z-score normalization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedforward Layer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is a simple perceptron-style layer to combine everything togethrt.&lt;/p&gt;
&lt;h3 id=&#34;positional-encoding&#34;&gt;Positional Encoding&lt;/h3&gt;
&lt;p&gt;Transformers aren’t actually aware of the order in which words occur, becasue they are a bag of words essentially.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/PE.jpg&#34;
	width=&#34;1224&#34;
	height=&#34;628&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/PE_hud8e377890faa05347da7d0f37a3f43ec_76411_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/PE_hud8e377890faa05347da7d0f37a3f43ec_76411_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;A Simple Positional Encoding&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;467px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-language-modeling-neural-architecture/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Positional encodings add an embedding representation of the &lt;strong&gt;absolute position&lt;/strong&gt; to the input word embedding. However,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not the same as relative/order information in language&lt;/li&gt;
&lt;li&gt;Less supervision for later positions&lt;/li&gt;
&lt;li&gt;Hard to deal with recursive human language&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-is-it-a-big-deal&#34;&gt;why is it a big deal?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Attention
&lt;ul&gt;
&lt;li&gt;has minimal inductive bias&lt;/li&gt;
&lt;li&gt;can learn arbitrary graph structure&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multiheadedness
&lt;ul&gt;
&lt;li&gt;Each &amp;ldquo;head&amp;rdquo; focuses on a different subspace of the input&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scalability
&lt;ul&gt;
&lt;li&gt;At layer N, no dependency between timesteps, so it can be trained in parallel&lt;/li&gt;
&lt;li&gt;Faster training = bigger models + more data&lt;/li&gt;
&lt;li&gt;Allows for massive pretraining&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Language Modeling - Ngram Models</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-language-modeling-ngram-models/</link>
        <pubDate>Thu, 13 Oct 2022 14:36:39 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-language-modeling-ngram-models/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 13th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 16th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;langauge-modeling&#34;&gt;Langauge Modeling&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It assigns a probability to a sequence of words&lt;/li&gt;
&lt;li&gt;Given a sequence of words, predict the most likely next word&lt;/li&gt;
&lt;li&gt;Generate likely sequences of words&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications&#34;&gt;Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unconstrained text generation&lt;/li&gt;
&lt;li&gt;Conditional text generation,
&lt;ul&gt;
&lt;li&gt;Machine Translation&lt;/li&gt;
&lt;li&gt;Speech Recognition&lt;/li&gt;
&lt;li&gt;Summarization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Representation Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;n-gram-langauge-models&#34;&gt;N-gram Langauge Models&lt;/h2&gt;
&lt;h3 id=&#34;directly-computing-corpus-stats&#34;&gt;Directly Computing Corpus Stats&lt;/h3&gt;
&lt;p&gt;It just computes the probability directly, but langauge is dynamic and we cannot ensure same sentence will exist in the corpus for multiple times.&lt;/p&gt;
&lt;p&gt;$$P(w_0, &amp;hellip;, w_1) = \frac{n}{N(l)} $$&lt;/p&gt;
&lt;p&gt;, where $n$ is num of occurances of $w_0, &amp;hellip;, w_1$ and $N$ is number of sequences with length $l$.&lt;/p&gt;
&lt;h3 id=&#34;unigram-language-model&#34;&gt;Unigram Language Model&lt;/h3&gt;
&lt;p&gt;$$ P\left(w_{0} \ldots w_{n}\right)=P\left(w_{0}\right) P\left(w_{1} \mid w_{0}\right) P\left(w_{2} \mid w_{0}, w_{1}\right) \ldots P\left(w_{n} \mid w_{0} \ldots w_{n-1}\right)$$&lt;/p&gt;
&lt;p&gt;According to &lt;strong&gt;Naive Asumption&lt;/strong&gt;, we have $ P\left(w_{i} \mid w_{0} \ldots w_{i-1}\right) \approx P\left(w_{i}\right) $.&lt;/p&gt;
&lt;p&gt;$$ P\left(w_{0} \ldots w_{n}\right) \approx P\left(w_{0}\right) P\left(w_{1}\right)  P\left(w_{2}\right)  \ldots P\left(w_{n}\right) $$&lt;/p&gt;
&lt;p&gt;However, $P($&amp;ldquo;give me an apple&amp;rdquo;$)$ equals $P($&amp;ldquo;give an apple me&amp;rdquo;$)$.&lt;/p&gt;
&lt;h3 id=&#34;bigram-language-model&#34;&gt;Bigram Language Model&lt;/h3&gt;
&lt;p&gt;According to &lt;strong&gt;Markov Assumption&lt;/strong&gt;, we have $P\left(w_{i} \mid w_{0} \ldots w_{i-1}\right) \approx P\left(w_{i} \mid w_{i-1}\right)$.&lt;/p&gt;
&lt;p&gt;$$P\left(w_{0} \ldots w_{n}\right) \approx P\left(w_{0}\right) P\left(w_{1} \mid w_{0}\right) P\left(w_{2} \mid w_{1}\right) \ldots P\left(w_{n} \mid w_{n-1}\right)$$&lt;/p&gt;
&lt;h3 id=&#34;n-gram-language-model&#34;&gt;N-gram Language Model&lt;/h3&gt;
&lt;p&gt;$$P\left(w_{0} \ldots w_{n}\right) \approx \prod_{i=0}^{n} P\left(w_{i} \mid w_{i-(n-1)} \ldots w_{i-1}\right)$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;smoothing&#34;&gt;Smoothing&lt;/h2&gt;
&lt;h3 id=&#34;laplace-smoothing&#34;&gt;Laplace Smoothing&lt;/h3&gt;
&lt;p&gt;The basic idea is to &lt;strong&gt;&amp;ldquo;add one&amp;rdquo;&lt;/strong&gt; to everything, so there won&amp;rsquo;t be zero counts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It needs to renormalize to keep it probability distribution&lt;br&gt;
$$P\left(w_{n} \mid w_{n-1}\right)=\frac{N\left(w_{n-1} w_{n}\right)+1}{\sum_{w}\left(N\left(w_{n-1} w\right)+1\right)}$$&lt;/li&gt;
&lt;li&gt;It is often intepreted as discounting, beacuse we borrow probability mass from high-frequency words in order to make room for unseen words.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backoff&#34;&gt;Backoff&lt;/h3&gt;
&lt;p&gt;We can estimate the probability of a longer sequence from the probabilities of its subsequences. If an ngram of length $n$ is not observed, use the corresponding length $n-1$ ngram instead.&lt;/p&gt;
&lt;p&gt;$$P(a, b, c) \cong P(a, b) \cong P(a)$$&lt;/p&gt;
&lt;h3 id=&#34;interpolation&#34;&gt;Interpolation&lt;/h3&gt;
&lt;p&gt;All counts are estimated using a weighted combination of smaller ngrams. It requires renormalization.&lt;/p&gt;
&lt;p&gt;$$P(a, b, c) = \lambda_1 P(a, b, c) \times \lambda_2 P(a, b) \times \lambda_3 P(a)$$&lt;/p&gt;
&lt;h3 id=&#34;kneser-ney-smoothing&#34;&gt;Kneser-Ney Smoothing&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s a state-of-the-art smoothing algorithm that combines several ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Absolute discounting (estimated from data)
$$ P_{\text {AbsoluteDiscounting }}\left(w_{i} \mid w_{i-1}\right)=\frac{C\left(w_{i-1} w_{i}\right)-d}{\sum_{\nu} C\left(w_{i-1} v\right)}+\lambda\left(w_{i-1}\right) P\left(w_{i}\right) $$&lt;/li&gt;
&lt;li&gt;Replacing ngram probabilities with continuation probabilities.
$$P\left(w_{i} \mid w_{i-1}\right)=\frac{\max \left(C\left(w_{i-1} w_{i}\right)-d, 0\right)}{C\left(w_{i-1}\right)}+\lambda\left(w_{i-1}\right) P_{\mathrm{CONTINUATION}}\left(w_{i}\right)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;perplexity&#34;&gt;Perplexity&lt;/h2&gt;
&lt;p&gt;A good language model should assign high probability to sentences that actually appear. Instead of using probability directly, we use a &lt;strong&gt;metric&lt;/strong&gt; called &amp;ldquo;perplexity&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;$$PPL(W)=\sqrt[n]{\prod_{i=1}^{n} \frac{1}{P\left(w_{1} \ldots w_{n}\right)}}$$&lt;/p&gt;
&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Weighted average branching factor&amp;rdquo; -&amp;gt; how many next words can
follow any given word?&lt;/li&gt;
&lt;li&gt;A model with lower PPL is less &amp;ldquo;surprised&amp;rdquo; by new data and has &lt;strong&gt;more certainty about true sequences&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;It considers branching factors to be lower, because it has a good sense of what should come next.&lt;/li&gt;
&lt;li&gt;In natural language, distributions are &lt;strong&gt;highly non-uniform&lt;/strong&gt;, so branching factors are (relatively) low.&lt;/li&gt;
&lt;li&gt;PPL will &lt;strong&gt;never be zero&lt;/strong&gt;! Natural language has inherent uncertainty.&lt;/li&gt;
&lt;li&gt;PPL is not comparable across different datasets.&lt;/li&gt;
&lt;li&gt;Higher-order n-grams lead to lower ppl in general, but
&lt;ul&gt;
&lt;li&gt;it is more likely to &lt;strong&gt;overfit&lt;/strong&gt; to training data,&lt;/li&gt;
&lt;li&gt;requires &lt;strong&gt;more memory&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;results in &lt;strong&gt;more zero counts&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Word Embeddings From NNs</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/</link>
        <pubDate>Tue, 11 Oct 2022 14:33:11 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 11th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 11th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;word-embeddings&#34;&gt;Word Embeddings&lt;/h2&gt;
&lt;p&gt;Word Embeddings can capture &lt;strong&gt;abstractions&lt;/strong&gt; over input features, which can be trained with backpropagation. They are regarded as &lt;strong&gt;word representations&lt;/strong&gt;, although they are derived from hidden states.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/WE.jpg&#34;
	width=&#34;1238&#34;
	height=&#34;706&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/WE_hu6c639b65d1ce78a286ef795b18669621_110978_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/WE_hu6c639b65d1ce78a286ef795b18669621_110978_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Word Embeddings&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-word-embeddings-from-nns/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;nns-vs-svd&#34;&gt;NNs vs. SVD&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Both of them use dimension reduction to obtain abstractions.&lt;/li&gt;
&lt;li&gt;Embeddings from NNs can become more powerful, but harder to interpret.
&lt;ul&gt;
&lt;li&gt;More layers&lt;/li&gt;
&lt;li&gt;More non-linearity&lt;/li&gt;
&lt;li&gt;More complex training objectives&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neural Word Embedding as Implicit Matrix Factorization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Topic Modeling</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/</link>
        <pubDate>Thu, 06 Oct 2022 14:31:09 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 6th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 6th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;topic-model&#34;&gt;Topic Model&lt;/h2&gt;
&lt;h3 id=&#34;what-is-topic-model&#34;&gt;What is Topic Model?&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s a method for &lt;strong&gt;automatically&lt;/strong&gt; organizing a collection of a text and is used for &lt;strong&gt;unlabled&lt;/strong&gt; document collections.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Topics are defined as combinations of words.&lt;/li&gt;
&lt;li&gt;Documents are defined as combinations of topics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Since you read an article about basketball, you might also like other articles about basketball.&amp;rdquo;&lt;br&gt;
&amp;ldquo;In the past year, people have been talking more about the economy than about schools.&amp;rdquo;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;latent-semantic-analysis-lsa&#34;&gt;Latent Semantic Analysis (LSA)&lt;/h2&gt;
&lt;p&gt;It uses dimensionality reduction/linear algebra to generate a topic model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white.jpg&#34;
	width=&#34;461&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LSA1.jpg&#34;
	width=&#34;1214&#34;
	height=&#34;572&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LSA1_hudd945732388ab9a80e4531bfe814f0c5_58892_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LSA1_hudd945732388ab9a80e4531bfe814f0c5_58892_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LSA via SVD&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;212&#34;
		data-flex-basis=&#34;509px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white.jpg&#34;
	width=&#34;461&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;latent-dirichelet-allocation-lda&#34;&gt;Latent Dirichelet Allocation (LDA)&lt;/h2&gt;
&lt;p&gt;It uses &lt;strong&gt;probability&lt;/strong&gt; models/&lt;strong&gt;graphical&lt;/strong&gt; models in contrast to LSA which has no notion of probability.&lt;/p&gt;
&lt;h3 id=&#34;generative-stories&#34;&gt;Generative Stories&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s the first step to build models that makes &lt;strong&gt;assumptions&lt;/strong&gt; about the structure of the data or problem, &lt;strong&gt;which tells a story about how the observed data came to be&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Repeat:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling a topic&lt;/li&gt;
&lt;li&gt;Sampling a word from that topic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, we need to further associate them with syntax, word order, semantics, discourse, etc.&lt;/p&gt;
&lt;p&gt;$$ P\left(w_{i}\right)=\sum_{j=1}^{T} P\left(w_{i} \mid z_{i}=j\right) P\left(z_{i}=j\right) $$&lt;/p&gt;
&lt;p&gt;, where $ P\left(w_{i}\right)$ is the probability of the data (a given word), $P\left(w_{i} \mid z_{i}=j\right)$ is the probability of that word for a given topic, $P\left(z_{i}=j\right)$ is overall probability of that topic.&lt;/p&gt;
&lt;p&gt;Given the observed word, find the $P(z)$ and $P(w|z)$ that make the observed word most likely.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white.jpg&#34;
	width=&#34;461&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA1.jpg&#34;
	width=&#34;450&#34;
	height=&#34;434&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA1_hu73037c3c3ed415d4cc33db37d347d6d2_12247_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA1_hu73037c3c3ed415d4cc33db37d347d6d2_12247_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;103&#34;
		data-flex-basis=&#34;248px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white.jpg&#34;
	width=&#34;461&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;graphical-model-notation&#34;&gt;Graphical Model Notation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white.jpg&#34;
	width=&#34;461&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA2.jpg&#34;
	width=&#34;834&#34;
	height=&#34;396&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA2_hu32140dd3780aa1e1ae6b68e026366931_22000_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA2_hu32140dd3780aa1e1ae6b68e026366931_22000_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;210&#34;
		data-flex-basis=&#34;505px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white.jpg&#34;
	width=&#34;461&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/white_hu4d0223eae35e310af94e82dbc185b4bd_2797_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;$z$ and $w$ depend on multinomial distribution of $P(z)$ and $P(w|z)$ respectively. Then, we assume those distributions come from a Dirichlet distributions $ \alpha $ and $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA3.jpg&#34;
	width=&#34;532&#34;
	height=&#34;468&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA3_hu8a110922baa2438ab93c6d006c03b40a_28035_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA3_hu8a110922baa2438ab93c6d006c03b40a_28035_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;113&#34;
		data-flex-basis=&#34;272px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA4.jpg&#34;
	width=&#34;562&#34;
	height=&#34;472&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA4_hua31bab57eba8c059a0b1800f0458295c_29613_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA4_hua31bab57eba8c059a0b1800f0458295c_29613_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;285px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA5.jpg&#34;
	width=&#34;500&#34;
	height=&#34;464&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA5_hu92f0328f358a786dba36e38c3a619f9e_25416_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA5_hu92f0328f358a786dba36e38c3a619f9e_25416_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;107&#34;
		data-flex-basis=&#34;258px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA6.jpg&#34;
	width=&#34;512&#34;
	height=&#34;460&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA6_hue7524b16b9304a7b336f35b5369d13b7_30121_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-topic-modeling/LDA6_hue7524b16b9304a7b336f35b5369d13b7_30121_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;111&#34;
		data-flex-basis=&#34;267px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;training--evaluation&#34;&gt;Training &amp;amp; Evaluation&lt;/h3&gt;
&lt;p&gt;We resort to approximate methods for posterior estimation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling Methods (MCMC, Gibbs Sampling)&lt;/li&gt;
&lt;li&gt;Variational Inference&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=fCmIceNqVog&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Topic Models: Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Distributional Hypothesis</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/</link>
        <pubDate>Tue, 04 Oct 2022 14:38:34 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 4th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 4th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;theories-of-word-meaning&#34;&gt;Theories of Word Meaning&lt;/h2&gt;
&lt;h3 id=&#34;the-meaning-of-a-word&#34;&gt;The Meaning of a word&lt;/h3&gt;
&lt;p&gt;Naive BOW model has no notion of word meaning (e.g. “cat” and “kitten” shoud have similar meanings, but it treats them as different words). Here comes to a problem: How to set boundaries to define words?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Words refer to sets&lt;/li&gt;
&lt;li&gt;Words refer to things&lt;/li&gt;
&lt;li&gt;Words refer to concepts&lt;/li&gt;
&lt;li&gt;Words are defined by the context&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-distributional-hypothesis&#34;&gt;The Distributional Hypothesis&lt;/h3&gt;
&lt;p&gt;The meaning of a word is defined by its context, and this leads to another question: What is context?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Perceptional context / Linguistic context&lt;/li&gt;
&lt;li&gt;Symbolic features / Real-valued &amp;ldquo;impressions&amp;rdquo;&lt;/li&gt;
&lt;li&gt;First order associations / Higher-order abstraction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The distributional hypothesis is that words found in the same contexts usually have similar meanings. It is clear how words are &amp;ldquo;learned&amp;rdquo; according to the theory and the model correlates well with lots of data on humans. However, it is &amp;ldquo;holostic&amp;rdquo;, because the meaning is always changing based on different contextss.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;vector-space-models&#34;&gt;Vector Space Models&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Words are represented as vectors, and those have similar meanings are nearby in the space.&lt;/p&gt;
&lt;h3 id=&#34;term-document-matrix&#34;&gt;Term-document Matrix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;word meaning = set of documents in which it occurs&lt;/li&gt;
&lt;li&gt;It can be binary indicators, real value counts, tf-idf values, etc.&lt;/li&gt;
&lt;li&gt;It captures &lt;strong&gt;broad topical-similarity&lt;/strong&gt; and &lt;strong&gt;co-occurrence&lt;/strong&gt;, rather than the &amp;ldquo;same meaning&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;It is good for document classification tasks, retrieval.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;word-context-matrix&#34;&gt;Word-Context Matrix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It finds all sentences containing that word.&lt;/li&gt;
&lt;li&gt;It can be binary indicators, real value counts, tf-idf values, etc.&lt;/li&gt;
&lt;li&gt;The similar words don&amp;rsquo;t necessarily co-occur, but they &lt;strong&gt;occur in similar contexts&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;It captures more grammatical similarity and lexical similarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;computing-similarity&#34;&gt;Computing Similarity&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Extract Equivalence&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;w1 == w2 iff their vector representations are identical&lt;/li&gt;
&lt;li&gt;However, the language is too varaible and this would never work&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Jaccard Similarity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S= \frac{intersection(v_1, v_2)}{union(v_1, v_2)} $&lt;/li&gt;
&lt;li&gt;It works well for binary vectos, but needs adjustments for real-valued dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Euclidean Distance&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E = \sqrt{\sum_{i=0}^{n}\left(v_{1}^{i}-v_{2}^{i}\right)^{2}} $&lt;/li&gt;
&lt;li&gt;It assumes similar words will be of similar magnitude (i.e., occur with similar frequency).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cosine Similarity&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$C = \frac{\overrightarrow{v 1} \cdot \overrightarrow{v 2}}{|\overrightarrow{v 1}||\overrightarrow{v 2}|} = \frac{\sum_{i=0}^{n} v_{1}^{i} v_{2}^{i}}{\sqrt{\sum_{i=0}^{n} (v_{1}^{i})^{2}} \sqrt{\sum_{i=0}^{n} (v_{2}^{i})^{2}}}$&lt;/li&gt;
&lt;li&gt;Dot product (scalar product / inner product) of two words vectors&lt;/li&gt;
&lt;li&gt;It can be explained as projection of v1 onto v2&lt;/li&gt;
&lt;li&gt;v1·v2 == 0 if vectos are orthogonal&lt;/li&gt;
&lt;li&gt;v1·v2 == -1 if they have opposite directions&lt;/li&gt;
&lt;li&gt;v1·v2 == 1 if they are parallel&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;word-embeddings&#34;&gt;Word Embeddings&lt;/h2&gt;
&lt;h3 id=&#34;word-vectors-vs-embeddings&#34;&gt;Word Vectors vs. Embeddings&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Word Vectors&lt;/strong&gt;: sparse, very high-dimensional&lt;br&gt;
&lt;strong&gt;Word Embeddings&lt;/strong&gt;: dense, low dimensional, dimensions are not
directly interpretable&lt;/p&gt;
&lt;h3 id=&#34;why-word-embeddings&#34;&gt;Why Word Embeddings?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lower dimensional = less computationally intensive&lt;/li&gt;
&lt;li&gt;Lower dimensional forces abstraction&lt;/li&gt;
&lt;li&gt;Lower dimensional removes noise:&lt;/li&gt;
&lt;li&gt;Dimensionality reduction can capture &amp;ldquo;second order&amp;rdquo; effects (E.g., w1 occurs with c1, w2 occurs with c2, c1 and c2 are similar. Thus, w1 and w2 are
similar.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can use Dimensionality Reduction to get them: 1) Matrix Factorization, 2) Neural Networks.&lt;/p&gt;
&lt;h3 id=&#34;dimensionality-reduction&#34;&gt;Dimensionality Reduction&lt;/h3&gt;
&lt;p&gt;It represents the data points in a new feature space by transforming the feature matrix. The new feature space is &lt;strong&gt;more informative for ML&lt;/strong&gt; but is &lt;strong&gt;less interpretable to humans&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Principle Component Analysis (PCA)&lt;/strong&gt;
Low Rank Assumptioin: we typically assume that our features contain a large amount of redundant information.&lt;br&gt;
M = U * D * V, where U contains word embeddings.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white.jpg&#34;
	width=&#34;347&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white_hu064a556f857e0a8082adeb4881b44613_2369_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white_hu064a556f857e0a8082adeb4881b44613_2369_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;102&#34;
		data-flex-basis=&#34;244px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/SVD1.jpg&#34;
	width=&#34;1216&#34;
	height=&#34;680&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/SVD1_hu145470b727ffd71a718b1cff761b8a2a_67007_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/SVD1_hu145470b727ffd71a718b1cff761b8a2a_67007_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Singular Value Decomposition&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;178&#34;
		data-flex-basis=&#34;429px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white.jpg&#34;
	width=&#34;347&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white_hu064a556f857e0a8082adeb4881b44613_2369_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white_hu064a556f857e0a8082adeb4881b44613_2369_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;102&#34;
		data-flex-basis=&#34;244px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white.jpg&#34;
	width=&#34;347&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white_hu064a556f857e0a8082adeb4881b44613_2369_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white_hu064a556f857e0a8082adeb4881b44613_2369_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;102&#34;
		data-flex-basis=&#34;244px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/SVD2.jpg&#34;
	width=&#34;1210&#34;
	height=&#34;660&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/SVD2_hu16d14cfb5d054156bb43770858c1a3d9_56031_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/SVD2_hu16d14cfb5d054156bb43770858c1a3d9_56031_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Truncated Singular Value Decomposition&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;440px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white.jpg&#34;
	width=&#34;347&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white_hu064a556f857e0a8082adeb4881b44613_2369_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-distributional-hypothesis/white_hu064a556f857e0a8082adeb4881b44613_2369_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;102&#34;
		data-flex-basis=&#34;244px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;For more info, please read this article: &lt;a class=&#34;link&#34; href=&#34;https://www.enjoyalgorithms.com/blog/principal-component-analysis-in-ml&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Principal Component Analysis in Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Tokenization</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-tokenization/</link>
        <pubDate>Thu, 22 Sep 2022 14:43:26 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-tokenization/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 22nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 22nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;morphology&#34;&gt;Morphology&lt;/h2&gt;
&lt;p&gt;Why can&amp;rsquo;t we just split on white space?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compounding&lt;/li&gt;
&lt;li&gt;Many language systems don&amp;rsquo;t use white space&lt;/li&gt;
&lt;li&gt;Clitics&lt;/li&gt;
&lt;li&gt;Words formation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;finite-state-machines&#34;&gt;Finite State Machines&lt;/h2&gt;
&lt;p&gt;This is the main &lt;strong&gt;rule-based&lt;/strong&gt; computational tool for morphological parsing in NLP.&lt;/p&gt;
&lt;h3 id=&#34;finite-state-automata-fsa&#34;&gt;Finite State Automata (FSA)&lt;/h3&gt;
&lt;p&gt;It is used to check whether to &amp;lsquo;accept&amp;rsquo; a string.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Formal Definition&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A finite set of states: $ \mathrm{Q}={q_{0}, q_{1}, \ldots q_{N-1} } $&lt;/li&gt;
&lt;li&gt;A finite set of symbols (alphabet): $ \Sigma={i_{0}, i_{1}, \ldots i_{{M}-1}} $&lt;/li&gt;
&lt;li&gt;A start state &amp;amp; Final States&lt;/li&gt;
&lt;li&gt;A transition function that returns the new state, given a state and a symbol: $ \delta(q, i) $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Psuedocode for Implementation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;recognize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;machine&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Boolean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;loop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reach&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;finalState&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;curState&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Non-Determinism&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple links out of the same node&lt;/li&gt;
&lt;li&gt;Epsilon transitions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-tokenization/fsa.jpg&#34;
	width=&#34;1200&#34;
	height=&#34;676&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-tokenization/fsa_hu5392257e4780065f174edec241aef7f5_83429_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-tokenization/fsa_hu5392257e4780065f174edec241aef7f5_83429_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;An example of FSA&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;finite-state-transducer-fst&#34;&gt;Finite State Transducer (FST)&lt;/h3&gt;
&lt;p&gt;It is used to &amp;rsquo;translate&amp;rsquo; one string into another, which produces a parse as output.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-tokenization/fst.jpg&#34;
	width=&#34;1216&#34;
	height=&#34;664&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-tokenization/fst_hud3b660378d7f13fa5005b150d28c6d7d_68404_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-tokenization/fst_hud3b660378d7f13fa5005b150d28c6d7d_68404_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;An example of FST&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;439px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;subword-tokenization&#34;&gt;Subword Tokenization&lt;/h2&gt;
&lt;p&gt;Subword Tokenization is a way to algorithmically identify common sub-sequences of characters and merge them into atomic units.&lt;/p&gt;
&lt;h3 id=&#34;problems&#34;&gt;Problems&lt;/h3&gt;
&lt;p&gt;Tokenizers result in many unknown words (OOVs), and morphological analysis can help, but:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Running morphological analyzers is expensive and language
specific&lt;/li&gt;
&lt;li&gt;Rare words might be “guessable” from cues&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;byte-pair-encoding&#34;&gt;Byte Pair Encoding&lt;/h3&gt;
&lt;p&gt;Steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Break words into characters&lt;/li&gt;
&lt;li&gt;Define a number of iterations to run&lt;/li&gt;
&lt;li&gt;Count the frequency of all pairs of symbols&lt;/li&gt;
&lt;li&gt;Merge the most frequent pair of characters, treating them as a new token&lt;/li&gt;
&lt;li&gt;Repeat&lt;/li&gt;
&lt;li&gt;Tp parse new inputs, just apply merge in order&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/course/chapter6/5?fw=pt#bytepair-encoding-tokenization&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Byte-Pair Encoding tokenization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Byte-Pair Encoding: Subword-based tokenization algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Text Classifier: Features</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/</link>
        <pubDate>Tue, 20 Sep 2022 14:35:51 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 20th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 20th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;data-pre-processing&#34;&gt;Data Pre-processing&lt;/h2&gt;
&lt;h3 id=&#34;preprocessing-vs-features&#34;&gt;Preprocessing VS. Features&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;: Defining the vocabulary&lt;br&gt;
&lt;strong&gt;Features&lt;/strong&gt;: Capturing the aspects of the language that is independent&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Differences in steps&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Load the data&lt;/li&gt;
&lt;li&gt;Preprocess the data&lt;/li&gt;
&lt;li&gt;Extract the features&lt;/li&gt;
&lt;li&gt;Split the data&lt;/li&gt;
&lt;li&gt;Train and test the model&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;preprocessing-steps&#34;&gt;Preprocessing Steps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Strip boilerplate/html/etc&lt;/li&gt;
&lt;li&gt;Tokenization&lt;/li&gt;
&lt;li&gt;Stemming/Lemmatization&lt;/li&gt;
&lt;li&gt;Lowercasing&lt;/li&gt;
&lt;li&gt;Removing punctuation, stop words, numbers, etc.&lt;/li&gt;
&lt;li&gt;Setting a Vocab Size/Defining out-of-vocabulary (OOV)&lt;/li&gt;
&lt;li&gt;Word Sense Disambiguation (one word with multiple meanings)&lt;/li&gt;
&lt;li&gt;Combining Synonyms/Paraphrases&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h2&gt;
&lt;h3 id=&#34;weighting-strategies&#34;&gt;Weighting Strategies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Basic BOW&lt;/strong&gt;: 0 and 1 for appearance&lt;br&gt;
&lt;strong&gt;Count&lt;/strong&gt;: Frequency in a document&lt;br&gt;
&lt;strong&gt;TF-IDF&lt;/strong&gt;: Weighting schemes for important words&lt;/p&gt;
&lt;p&gt;TF-IDF is about assigning higher weights to words that differentiate this document from other documents. More info could be find &lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;TF(word, doc) = # of occurrences of the word in the doc / # of words in the doc&lt;br&gt;
IDF(word) = log(# of all docs / # of docs that contain the word)&lt;br&gt;
TF-IDF(word, doc) = TF(word, doc) * IDF(word)&lt;/p&gt;
&lt;p&gt;You could try TF-IDF Demo &lt;a class=&#34;link&#34; href=&#34;https://remykarem.github.io/tfidf-demo/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;n-grams&#34;&gt;N-Grams&lt;/h3&gt;
&lt;p&gt;N-gram is a sequence of words of lenght N, such as unigram, bigram, trigram, 4-gram, etc. The algorithm uses sliding window with N length to extract features.&lt;/p&gt;
&lt;p&gt;However, it may lead to &lt;strong&gt;large feature space&lt;/strong&gt;. It&amp;rsquo;s a tradeoff between &lt;strong&gt;expressivity&lt;/strong&gt; and &lt;strong&gt;generalization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/2-gram.jpg&#34;
	width=&#34;1186&#34;
	height=&#34;490&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/2-gram_hue813c4cf87e5fd453b898c0a36f70a0d_48757_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-text-classifier-features/2-gram_hue813c4cf87e5fd453b898c0a36f70a0d_48757_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;An example of bigram&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;242&#34;
		data-flex-basis=&#34;580px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;more-advanced-features&#34;&gt;More Advanced Features&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Syntactic Features&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;People often use dependency parsers to find meaningful links
between &lt;strong&gt;non-adjacent&lt;/strong&gt; words&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Text Classifier: ML Models</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/</link>
        <pubDate>Fri, 16 Sep 2022 14:27:25 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 16th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 16th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-classification&#34;&gt;Supervised Classification&lt;/h2&gt;
&lt;h3 id=&#34;supervised-vs-unsupervised&#34;&gt;Supervised vs. Unsupervised&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Supervised&lt;/strong&gt;: I have lots of sentences. Given words 1…k in a sentence, can I predict word k+1?
&lt;strong&gt;Unsupervised&lt;/strong&gt;: I have a ton of news articles. Can I cluster them into meaningful groups?&lt;/p&gt;
&lt;h3 id=&#34;classification-vs-regression&#34;&gt;Classification vs. Regression&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;: Is the sentiment positive or negative?&lt;br&gt;
&lt;strong&gt;Regression&lt;/strong&gt;: How many likes will this tweet get?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;feature-matrices-and-bow-models&#34;&gt;Feature Matrices and BOW Models&lt;/h2&gt;
&lt;h3 id=&#34;building-feature-matrices&#34;&gt;Building Feature Matrices&lt;/h3&gt;
&lt;p&gt;ML models require input to be represented as &lt;strong&gt;numeric features&lt;/strong&gt;, which could be encoded in a &lt;strong&gt;feature matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bag-of-words-mode&#34;&gt;Bag of Words Mode&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s a model that uses the words as features.  &lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/BoW.jpg&#34;
	width=&#34;1148&#34;
	height=&#34;434&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/BoW_hu3480f5b17b16e182c53fb0fad8e46d92_47253_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-text-classifier-ml-models/BoW_hu3480f5b17b16e182c53fb0fad8e46d92_47253_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;BoW&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;264&#34;
		data-flex-basis=&#34;634px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No information about order or syntax&lt;/li&gt;
&lt;li&gt;Binary representation&lt;/li&gt;
&lt;li&gt;High Dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;naive-bayes-text-classifier&#34;&gt;Naive Bayes Text Classifier&lt;/h2&gt;
&lt;p&gt;It is based on &lt;strong&gt;Bayes Rule&lt;/strong&gt; to estimate/maximize $P(Y|X)$.
$$ P(Y \mid X)=\frac{P(X,Y)}{P(X)} = \frac{P(X \mid Y) P(Y)}{P(X)} $$&lt;/p&gt;
&lt;p&gt;We can apply &lt;strong&gt;chain rule&lt;/strong&gt; to compute $ P(Y \mid X) = P(Y \mid x_{1}, x_{2}, \ldots, x_{k}) $ if there are multiple features maching the class:
$$
P(Y \mid X) = P(x_{1} \mid x_{2}, \ldots, x_{k}, Y) P(x_{2} \mid x_{3}, \ldots, x_{k}, Y) \ldots P(x_{k} \mid Y) P(Y)
$$&lt;/p&gt;
&lt;p&gt;According to &lt;strong&gt;Naive Assumption&lt;/strong&gt;, we assume features are independent.
$$
P(Y \mid X) = P(x_{1} \mid Y) P(x_{2} \mid Y) \ldots P(x_{k} \mid Y) P(Y)
$$&lt;/p&gt;
&lt;p&gt;Now, we can easily compute the result. Note that $ P(Y) $ could be obtained through &lt;strong&gt;domain knwoledge&lt;/strong&gt; or could be estimated from &lt;strong&gt;data&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;logistic-regression-text-classifier&#34;&gt;Logistic Regression Text Classifier&lt;/h2&gt;
&lt;h3 id=&#34;lr-vs-nb&#34;&gt;LR vs. NB&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Logistic Regression&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Naive Bayes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conditional Distribution $P(Y \mid X)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Joint Distribution $P(X,Y)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Discriminative Model&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Generative Model&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Maybe better in general&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Better with smaller data&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;logistic-regression-overview&#34;&gt;Logistic Regression Overview&lt;/h3&gt;
&lt;p&gt;Logistic Regression is based on &lt;strong&gt;Linear Regression&lt;/strong&gt;, and is for &lt;strong&gt;classification&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
y=\frac{1}{1+e^{-(\vec{w} \cdot \vec{x})}}
$$&lt;/p&gt;
&lt;p&gt;$$
L = -Y \log \hat{Y}+(1-Y) \log (1-\hat{Y})
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;experimental-design-in-ml&#34;&gt;Experimental Design in ML&lt;/h2&gt;
&lt;h3 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h3&gt;
&lt;p&gt;Models overfit when then model idiosyncrasies of the training data
that don’t necessarily generalize.&lt;/p&gt;
&lt;h3 id=&#34;train-test-splits&#34;&gt;Train-Test Splits&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Train&lt;/strong&gt;: Used to train the model, i.e., set the parameters&lt;br&gt;
&lt;strong&gt;Dev&lt;/strong&gt;: Used during development, to inspect and choose &amp;ldquo;hyperparameters&amp;rdquo;&lt;br&gt;
&lt;strong&gt;Test&lt;/strong&gt;: In good experimental design, only allowed
to evaluate on test one time to avoid &amp;ldquo;cheating&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;i.i.d.&lt;/strong&gt;: Train and Test data are drawn from the same distribution. In reality, test isn’t always i.i.d.&lt;/p&gt;
&lt;h3 id=&#34;common-baselines&#34;&gt;Common Baselines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Guess at random&lt;/li&gt;
&lt;li&gt;State-of-the-art (SOTA)&lt;/li&gt;
&lt;li&gt;Task-specific Heuristics&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Most frequent class&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Skylines&amp;rdquo;, e.g. human performance, result under ideal conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Tasks &amp; Benchmarks</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-tasks-benchmarks/</link>
        <pubDate>Wed, 14 Sep 2022 10:33:24 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-tasks-benchmarks/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 14th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 14th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;task-driven-culture&#34;&gt;Task-Driven Culture&lt;/h2&gt;
&lt;p&gt;NLP research has tended to be organized around &amp;ldquo;tasks&amp;rdquo; with shared ideas. Instead of building a whole system themselves, they will justify the problem in relation to some commonly-agreed upon important &amp;ldquo;tasks&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;extrinsic--intrinsic-tasks&#34;&gt;Extrinsic &amp;amp; Intrinsic Tasks&lt;/h3&gt;
&lt;p&gt;Here, we take ACL Submission Topics as examples to make a classification.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extrinsic Tasks&lt;/strong&gt;: An &amp;ldquo;end user&amp;rdquo; might want to use&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dialog &amp;amp; Interactive Sys&lt;/li&gt;
&lt;li&gt;Info Retrieval &amp;amp; Text Mining&lt;/li&gt;
&lt;li&gt;Language Grounding to Vision, Robotics &amp;amp; Beyond&lt;/li&gt;
&lt;li&gt;Machine Translation &amp;amp; Multilinguality&lt;/li&gt;
&lt;li&gt;NLP Applications&lt;/li&gt;
&lt;li&gt;Question Answering&lt;/li&gt;
&lt;li&gt;Sementiment Analysis, Stylistic Analysis &amp;amp; Argument Mining&lt;/li&gt;
&lt;li&gt;Speech &amp;amp; Multimodality&lt;/li&gt;
&lt;li&gt;Summarization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Intrinsic Tasks&lt;/strong&gt;: Part of a bigger system&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generation&lt;/li&gt;
&lt;li&gt;ML for NLP&lt;/li&gt;
&lt;li&gt;Info Extraction&lt;/li&gt;
&lt;li&gt;Resources &amp;amp; Evaluation&lt;/li&gt;
&lt;li&gt;Discourses &amp;amp; Pragmatics&lt;/li&gt;
&lt;li&gt;Phonology, Morphology &amp;amp; Word Segmentation&lt;/li&gt;
&lt;li&gt;Syntax: Tagging, Chunking &amp;amp; Parsing&lt;/li&gt;
&lt;li&gt;Semantics: Lexical&lt;/li&gt;
&lt;li&gt;Sementics: Sentence-level Sementics, Textual Inference &amp;amp; Other Areas&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bakeoffs--leaderboards&#34;&gt;Bakeoffs &amp;amp; Leaderboards&lt;/h3&gt;
&lt;p&gt;Statistical Revolution uses the same inputs, and compares against the same &lt;strong&gt;ground truth&lt;/strong&gt; (standardized test sets) outputs, using the same metric. The filed places a lot of stock in &lt;strong&gt;empirical comparison of ideas&lt;/strong&gt; and &lt;strong&gt;applicability of tech&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DARPA&lt;/li&gt;
&lt;li&gt;SemEval&lt;/li&gt;
&lt;li&gt;SQuAD&lt;/li&gt;
&lt;li&gt;GLUE&lt;/li&gt;
&lt;li&gt;SuperGlue&lt;/li&gt;
&lt;li&gt;GEM&lt;/li&gt;
&lt;li&gt;DynaBench&lt;/li&gt;
&lt;li&gt;BigBench&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;major-downstream-tasks-and-evals&#34;&gt;Major &amp;ldquo;Downstream&amp;rdquo; Tasks and Evals&lt;/h2&gt;
&lt;h3 id=&#34;classification&#34;&gt;Classification&lt;/h3&gt;
&lt;h4 id=&#34;input--output&#34;&gt;Input &amp;amp; Output&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: a piece of text&lt;br&gt;
&lt;strong&gt;Output&lt;/strong&gt;: a label&lt;/p&gt;
&lt;h4 id=&#34;examples&#34;&gt;Examples&lt;/h4&gt;
&lt;p&gt;Sentiment prediction (e.g., for stock trading)&lt;br&gt;
Language detection (e.g., before machine translation)&lt;br&gt;
Relevance prediction (e.g., for retrieval or ad targeting)&lt;br&gt;
Intent classification (e.g., for goal-oriented dialog)&lt;/p&gt;
&lt;h4 id=&#34;standard-metrics&#34;&gt;Standard Metrics&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: How often is the label correct?&lt;br&gt;
&lt;strong&gt;Precision&lt;/strong&gt;: Probability that it is spam, given that the model says it is.&lt;br&gt;
&lt;strong&gt;Recall&lt;/strong&gt;: Probability that the model says its spam, given that it actually is spam.&lt;br&gt;
&lt;strong&gt;F-1 Score&lt;/strong&gt;: Harmonic Mean of Precision and Recall&lt;br&gt;
$$
F_{1}=2 \times \frac{\text { precision } \times \text { recall }}{\text { precision }+\text { recall }}
$$
&lt;strong&gt;AUC&lt;/strong&gt;: Area Under the True-Positive vs. False Positive Curve&lt;/p&gt;
&lt;h3 id=&#34;info-retrieval&#34;&gt;Info Retrieval&lt;/h3&gt;
&lt;h4 id=&#34;input--output-1&#34;&gt;Input &amp;amp; Output&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Query or Doc Collection&lt;br&gt;
&lt;strong&gt;Output&lt;/strong&gt;: Ranked List of Doc&lt;/p&gt;
&lt;h4 id=&#34;examples-1&#34;&gt;Examples&lt;/h4&gt;
&lt;p&gt;They have increasingly complex goals, which are interwoven with other NLP tasks (summarization, question answering).&lt;/p&gt;
&lt;h4 id=&#34;standard-metrics-1&#34;&gt;Standard Metrics&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Precision@K&lt;/strong&gt;: How many of the top K documents are relevant?&lt;br&gt;
&lt;strong&gt;Recall@K&lt;/strong&gt;: How many of the relevant documents occur in the top K?&lt;br&gt;
&lt;strong&gt;Mean Reciprocal Rank&lt;/strong&gt;: At what rank does the relevant document appear?&lt;br&gt;
&lt;strong&gt;Discounted Cumulative Gain (DCG)&lt;/strong&gt;: How much benefit with one more rank down the list?
$$
DCG_{p}=\sum_{i=1}^{p} \frac{rel_{i}}{\log _{2}(i+1)}
$$&lt;br&gt;
&lt;strong&gt;Area Under the Curve (AUC)&lt;/strong&gt;: Summary metrics for precision-recall curves
&lt;strong&gt;Averaged Precision (AP)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
AP=\sum_{n}\left(R_{n}-R_{n-1}\right) P_{n}
$$&lt;/p&gt;
&lt;h4 id=&#34;ground-truth&#34;&gt;Ground Truth&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Query Relevances (&amp;ldquo;qrels&amp;rdquo;)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Humans manually annotate documents for relevance, based on description of query&lt;/li&gt;
&lt;li&gt;Too expensive to estimate recall (requires labeling every document for every query)&lt;/li&gt;
&lt;li&gt;Not “ecologically valid”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;User Behavior/AB Testing&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the user click on links?&lt;/li&gt;
&lt;li&gt;Do they stay on the page after clicking?&lt;/li&gt;
&lt;li&gt;Do they go back and reword the query?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;questions-answering&#34;&gt;Questions Answering&lt;/h3&gt;
&lt;h4 id=&#34;variants-of-qa&#34;&gt;Variants of QA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Open Book&lt;/li&gt;
&lt;li&gt;Cloased Book&lt;/li&gt;
&lt;li&gt;QA over Databases&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;open-book-qa&#34;&gt;Open Book QA&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Question + Doc&lt;br&gt;
&lt;strong&gt;Output&lt;/strong&gt;: Answer / Highlighted Span&lt;/p&gt;
&lt;h4 id=&#34;closed-book-qa&#34;&gt;Closed Book QA&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Question
&lt;strong&gt;Output&lt;/strong&gt;: Answer&lt;br&gt;
&lt;strong&gt;Note&lt;/strong&gt;: Hard to verify answer source&lt;/p&gt;
&lt;h4 id=&#34;database-book-qa&#34;&gt;Database Book QA&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Question + Database&lt;br&gt;
&lt;strong&gt;Output&lt;/strong&gt;: Answer&lt;br&gt;
&lt;strong&gt;Note&lt;/strong&gt;: 1) Good for short, factoid questions 2) Requires formal query languages&lt;/p&gt;
&lt;h4 id=&#34;standard-metrics-2&#34;&gt;Standard Metrics&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: What % of questions were correct?&lt;br&gt;
&lt;strong&gt;Exact Match Accuracy&lt;/strong&gt;: % of times model’s answer was string identical to ground truth
&lt;strong&gt;Precision&lt;/strong&gt;: How many of the predicted tokens were correct?&lt;br&gt;
&lt;strong&gt;Recall&lt;/strong&gt;: How many of the correct tokens were predicted?&lt;br&gt;
&lt;strong&gt;F-1&lt;/strong&gt;: Harmonic mean of precision and recall&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;important-intrinsic-tasks-and-evals&#34;&gt;Important Intrinsic Tasks and Evals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tokenization&lt;/li&gt;
&lt;li&gt;Sentence Splitting&lt;/li&gt;
&lt;li&gt;Part-of-speech Tagging&lt;/li&gt;
&lt;li&gt;Morphological Analysis&lt;/li&gt;
&lt;li&gt;Named Entity Recognition&lt;/li&gt;
&lt;li&gt;Syntactic Parsing&lt;/li&gt;
&lt;li&gt;Coreference Resolution&lt;/li&gt;
&lt;li&gt;Other Annotations&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;current-debates&#34;&gt;Current Debates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Extrinsic vs. Intrinsic&lt;/li&gt;
&lt;li&gt;Scientific Validity&lt;/li&gt;
&lt;li&gt;Leaderboardism&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Comp Linguistics] Intro to NLP</title>
        <link>https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/</link>
        <pubDate>Mon, 12 Sep 2022 22:06:54 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Sept 12th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Sept 12th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1460 Computational Linguistics&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. The class aims to explore techniques regarding recent advances in NLP with deep learning. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-nlp&#34;&gt;What is NLP?&lt;/h2&gt;
&lt;p&gt;The goal of this field is to get computers to perform useful tasks involving human language.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Search&lt;/li&gt;
&lt;li&gt;Auto Completion&lt;/li&gt;
&lt;li&gt;Info Retrieval&lt;/li&gt;
&lt;li&gt;Summarization&lt;/li&gt;
&lt;li&gt;Ad Targeting&lt;/li&gt;
&lt;li&gt;Recommendations&lt;/li&gt;
&lt;li&gt;Language Identification&lt;/li&gt;
&lt;li&gt;Language Translation&lt;/li&gt;
&lt;li&gt;Text Generation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-brief-history&#34;&gt;A Brief History&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/timeline.jpg&#34;
	width=&#34;995&#34;
	height=&#34;464&#34;
	srcset=&#34;https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/timeline_hu58ee0901517331e3436221a7560a45c9_287527_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/comp-linguistics-intro-to-nlp/timeline_hu58ee0901517331e3436221a7560a45c9_287527_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;NLP History&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;214&#34;
		data-flex-basis=&#34;514px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;language-processing-pipeline&#34;&gt;Language Processing Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Phonology
&lt;ul&gt;
&lt;li&gt;Sounds&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Morphology
&lt;ul&gt;
&lt;li&gt;Analysis of the individual components of words&lt;/li&gt;
&lt;li&gt;e.g. suffixes and prefixes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lexicon
&lt;ul&gt;
&lt;li&gt;Words / Forms&lt;/li&gt;
&lt;li&gt;It captures the meanings of words.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Syntax
&lt;ul&gt;
&lt;li&gt;Sentences / Clauses&lt;/li&gt;
&lt;li&gt;Structural relationship or grammar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantics
&lt;ul&gt;
&lt;li&gt;Meanings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pragmatics
&lt;ul&gt;
&lt;li&gt;Language use&lt;/li&gt;
&lt;li&gt;Goals &amp;amp; purposes in a sequential level&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discourse
&lt;ul&gt;
&lt;li&gt;Context&lt;/li&gt;
&lt;li&gt;Expressions beyond sentences &amp;amp; paragraphs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://csci-1460-computational-linguistics.github.io/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CSCI1460 Computational Linguistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
