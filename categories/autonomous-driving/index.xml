<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Autonomous Driving on Tony Feng</title>
        <link>https://tonyfpy.github.io/categories/autonomous-driving/</link>
        <description>Recent content in Autonomous Driving on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language><atom:link href="https://tonyfpy.github.io/categories/autonomous-driving/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[AD Fundamentals] Control</title>
        <link>https://tonyfpy.github.io/p/ad-fundamentals-control/</link>
        <pubDate>Tue, 17 Jan 2023 11:35:11 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/ad-fundamentals-control/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Jan. 17th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Jan. 17th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains notes from the course &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.udacity.com/course/self-driving-car-fundamentals-featuring-apollo--ud0419&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Driving Fundamentals: Featuring Apollo&lt;/a&gt;&lt;/strong&gt; published by &lt;strong&gt;Udacity&lt;/strong&gt; &amp;amp; &lt;strong&gt;Baidu Apollo&lt;/strong&gt;. This course aims to deliver key parts of self-driving cars, including HD Map, localization, perception, prediction, planning and control. I posted these &amp;ldquo;notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;intro-to-control&#34;&gt;Intro to Control&lt;/h2&gt;
&lt;p&gt;Control is a strategy of actuating the vehicle to move it towards the road. For a car, the basic control inputs are &lt;strong&gt;steering&lt;/strong&gt;, &lt;strong&gt;acceleration&lt;/strong&gt; and &lt;strong&gt;break&lt;/strong&gt;. The car uses controller inputs to &lt;strong&gt;minimize the deviation from the planning&lt;/strong&gt; and &lt;strong&gt;maximize the passenger comfort&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/trajectory&amp;amp;control.jpg&#34;
	width=&#34;583&#34;
	height=&#34;272&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/trajectory&amp;amp;control_hu8adb95acf296c928cdecea023268d465_14944_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-control/trajectory&amp;amp;control_hu8adb95acf296c928cdecea023268d465_14944_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;214&#34;
		data-flex-basis=&#34;514px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The controller receives trajectory as a sequence of way points. We use control inputs to move the vehicle towards those way points. &lt;strong&gt;The result of control should be as close as possible to the target trajectory.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Controller needs to be accurate.&lt;/li&gt;
&lt;li&gt;The control strategy should be favaroble to vehicles.&lt;/li&gt;
&lt;li&gt;Actuation should be continuous to make the driving smooth.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/planning&amp;amp;real.jpg&#34;
	width=&#34;588&#34;
	height=&#34;247&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/planning&amp;amp;real_hu1c0ac25105ec9d798abd4f66d48aefb7_19953_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-control/planning&amp;amp;real_hu1c0ac25105ec9d798abd4f66d48aefb7_19953_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;238&#34;
		data-flex-basis=&#34;571px&#34;
	
&gt; &lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/executable.jpg&#34;
	width=&#34;559&#34;
	height=&#34;237&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/executable_hu86a815fca35398023d18a3c4f9f01987_15930_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-control/executable_hu86a815fca35398023d18a3c4f9f01987_15930_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;566px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;control-pipeline&#34;&gt;Control Pipeline&lt;/h2&gt;
&lt;p&gt;There are two inputs of controller aspects: &lt;strong&gt;target trajectory&lt;/strong&gt; and &lt;strong&gt;vehicle states&lt;/strong&gt;. We use these 2 inputs to calculate the deviation between real trajectory and target trajectory.&lt;/p&gt;
&lt;p&gt;The outputs of the controller are the values for the control inputs: &lt;strong&gt;steering&lt;/strong&gt;, &lt;strong&gt;acceleration&lt;/strong&gt;, and &lt;strong&gt;break&lt;/strong&gt;. They are used to correct the deviation from the target trajectory.&lt;/p&gt;
&lt;h3 id=&#34;target-trajectory&#34;&gt;Target Trajectory&lt;/h3&gt;
&lt;p&gt;It comes from the &lt;strong&gt;planning&lt;/strong&gt; module. Each way point is designated a position and a reference velocity. The trajectory is updated at every timestamp.&lt;/p&gt;
&lt;h3 id=&#34;vehicle-state&#34;&gt;Vehicle State&lt;/h3&gt;
&lt;p&gt;It is provided by the &lt;strong&gt;localization&lt;/strong&gt; module. It includes the position of the vehicle. Also, it also includes data from &lt;strong&gt;internal sensors&lt;/strong&gt;, such as speed, steering, and acceleration.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pid&#34;&gt;PID&lt;/h2&gt;
&lt;h3 id=&#34;term-p&#34;&gt;Term P&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;P controller&lt;/strong&gt; will pull the vehicle back to the target trajectory as soon as it starts to deviate. &lt;strong&gt;Proportional&lt;/strong&gt; means that the further the vehicle deviates, the harder the controller will steer back toward the target trajectory.&lt;/p&gt;
&lt;p&gt;$$a=-K_{P} e$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/PID1.jpg&#34;
	width=&#34;588&#34;
	height=&#34;167&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/PID1_hu4df66db1696a281d0a3d15e0c79afcfa_10491_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-control/PID1_hu4df66db1696a281d0a3d15e0c79afcfa_10491_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;352&#34;
		data-flex-basis=&#34;845px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;term-d&#34;&gt;Term D&lt;/h3&gt;
&lt;p&gt;One problem of P controller is that &lt;strong&gt;it&amp;rsquo;s easy to overshoot the reference trajectory&lt;/strong&gt;. We need the controller to be steadier when the vehicle is closer to the reference trajectory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The D term of the PID steadies its motion.&lt;/strong&gt; The PD controller has a damping term that &lt;strong&gt;minimizes how quickly the controller output changes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$a=-K_{P} e-K_{D} \frac{d e}{d t}$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/PID2.jpg&#34;
	width=&#34;590&#34;
	height=&#34;190&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/PID2_hu94e2d61be329cfa2b7c73bdcb945d5b7_11282_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-control/PID2_hu94e2d61be329cfa2b7c73bdcb945d5b7_11282_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;310&#34;
		data-flex-basis=&#34;745px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;term-i&#34;&gt;Term I&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;The I term is responsible for correcting any systemic bias of the vehicle.&lt;/strong&gt; For example, the steering might be out of alignment, which would result in a constant steering offset. In that case, we need to steer a little bit to the side just to keep going straight. To handle this problem, the I controller &lt;strong&gt;penalilizes the accumulated error of the system&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$a=-K_{P} e-K_{I} \int e d t-K_{D} \frac{d e}{d t}$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/PID3.jpg&#34;
	width=&#34;589&#34;
	height=&#34;154&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/PID3_huf4c2dce8735687a8e7e61607f910b9ee_8797_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-control/PID3_huf4c2dce8735687a8e7e61607f910b9ee_8797_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;382&#34;
		data-flex-basis=&#34;917px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;pros--cons&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Pros&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proportional-Integral-Derivative Control&lt;/strong&gt; (PID) only needs to know how far we have deviated from the target trajectory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PID is a &lt;strong&gt;linear algorithm&lt;/strong&gt;, so it is not able to deal with complex problems. In real life, a self-driving car requires multiple PIDs, which means it&amp;rsquo;s hard to combine a latitudinal and longitudinal control.&lt;/li&gt;
&lt;li&gt;Another problem is that it depends on &lt;strong&gt;real-time error measurement&lt;/strong&gt;. Measurement delays can compromise the performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lqr&#34;&gt;LQR&lt;/h2&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Linear Qudratic Regulator&lt;/strong&gt; is a model-based controller that uses the state of the vehicle to minimize error.&lt;/p&gt;
&lt;p&gt;Apollo uses LQR for lateral control, which contains 4 conponents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The lateral error&lt;/li&gt;
&lt;li&gt;The rate of the change of lateral error (derivative)&lt;/li&gt;
&lt;li&gt;The heading error&lt;/li&gt;
&lt;li&gt;The rate of the change of the heading error (derivative))&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
x=\left[\begin{array}{c}
c t e \
c \dot{t} e \
\theta \
\dot{\theta}
\end{array}\right]
$$&lt;/p&gt;
&lt;p&gt;$$
u=\left[\begin{array}{c}
\text { steering } \
\text { acceleration } \
\text { brake }
\end{array}\right]
$$&lt;/p&gt;
&lt;p&gt;This model can be represented by a &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/State-space_representation&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;State-space Equation&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;$$ \dot{x} = Ax + Bu $$&lt;/p&gt;
&lt;p&gt;, where $\dot{x} $ vector is the rate of change of the $x$ vector, so each component of $\dot{x} $ is just the &lt;strong&gt;derivative&lt;/strong&gt; of the corresponding component of $x$. &lt;strong&gt;The equation captures how the change in state.&lt;/strong&gt; &lt;a class=&#34;link&#34; href=&#34;http://python-control.readthedocs.io/en/latest/generated/control.lqr.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;Here&lt;/strong&gt;&lt;/a&gt; you can find the implementation of LQR in Python&lt;/p&gt;
&lt;h3 id=&#34;term-l&#34;&gt;Term L&lt;/h3&gt;
&lt;p&gt;The equation is &lt;strong&gt;linear&lt;/strong&gt;. When we there is a change to $x$ and $u$,&lt;/p&gt;
&lt;p&gt;$$ \dot{x}+\Delta \dot{x}=A(x+\Delta x)+B(u+\Delta u) $$&lt;/p&gt;
&lt;p&gt;the change to $\dot{x} $ will also satisfy this equation.&lt;/p&gt;
&lt;p&gt;$$ \Delta \dot{x}=A \Delta x+B \Delta u $$&lt;/p&gt;
&lt;h3 id=&#34;term-q&#34;&gt;Term Q&lt;/h3&gt;
&lt;p&gt;We want to apply as few control inputs as possible to &lt;strong&gt;decrease overhead&lt;/strong&gt;. In order to minimize these factors, we can keep running summation of errors and summation of control inputs.&lt;/p&gt;
&lt;p&gt;E.g. When the car turns too far to the right, we add that error to the sum. When the control input steers the car back to the left, we subtract a bit from the control input sum. However, this approach causes problems. &lt;strong&gt;The positive errors to the right simply cancel out the negative errors to the left. The same would be true for control inputs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Instead, &lt;strong&gt;we can multiply $x$ and $u$ by themselves&lt;/strong&gt;, so the negative values will produce positive squares. They are called &lt;strong&gt;quadratic terms&lt;/strong&gt;. We assign &lt;strong&gt;weights&lt;/strong&gt; to these terms and the optimal $u$ should minimize summation overtime.&lt;/p&gt;
&lt;p&gt;$$ w_{1} c t e^{2}+w_{2} c \dot{t e} e^{2}+w_{3} \theta^{2}+w_{4} \dot{\theta}^{2}+\ldots $$&lt;/p&gt;
&lt;p&gt;Mathematically, they are defined using a cost function:&lt;/p&gt;
&lt;p&gt;$$ \text { cost }=\int_{0}^{\infty}\left(x^{T} Q x+u^{T} R u\right) d t $$&lt;/p&gt;
&lt;p&gt;, where $Q$ and $R$ represents a collection of weights for $x$ and $u$.&lt;/p&gt;
&lt;p&gt;To minimize this cost function, one solution provided by Apollo is using a complicated scheme $K$, which can obtain $u$ through $x$.&lt;/p&gt;
&lt;p&gt;$$ u = -Kx$$&lt;/p&gt;
&lt;p&gt;So, finding an optimal $u$ is finding an optimal $K$. Many tools have access to solve $K$, once you provide $A$, $B$, $Q$, $R$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mpc&#34;&gt;MPC&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Model Predictive Control&lt;/strong&gt; heavily relies on mathematical optimzation. It&amp;rsquo;s a &lt;strong&gt;repeating&lt;/strong&gt; process. It looks into the future to calculate and optimize a sequence of control inputs.&lt;/p&gt;
&lt;h3 id=&#34;general-steps&#34;&gt;General Steps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Building a model of the vehicle&lt;/li&gt;
&lt;li&gt;Using an optimization engine to calculate control inputs over a finite time horizon&lt;/li&gt;
&lt;li&gt;Implementing &lt;strong&gt;the first&lt;/strong&gt; set of control inputs in the sequence&lt;/li&gt;
&lt;li&gt;Repeating the cycle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Why only to carry out the first set of control inputs?&lt;/strong&gt;&lt;br&gt;
Our measurements and calculation are only &lt;strong&gt;approximate&lt;/strong&gt;. If we were to implement the entire sequence, the actual resulting vehicle state would diverge sharply from our model. We are better off continuously &lt;strong&gt;re-evaluating the optimal sequence of control inputs at every timestamp&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;defining-vehicle-model&#34;&gt;Defining Vehicle Model&lt;/h3&gt;
&lt;p&gt;This model approximates the physics of our car. It will model what will happen after we apply a set of control inputs to the vehicle.&lt;/p&gt;
&lt;h3 id=&#34;time-horizon&#34;&gt;Time Horizon&lt;/h3&gt;
&lt;p&gt;We decide how far into the future we want MPC to look.  However, there is a &lt;strong&gt;trade-off between the accruracy and how quickly we need to get a result&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The further we look, more accurate our controller will be.&lt;/li&gt;
&lt;li&gt;The faster we get the result, the faster we can update the control inputs to the actual vehicle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/MPC1.jpg&#34;
	width=&#34;605&#34;
	height=&#34;243&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/MPC1_hu0c2439c99addb37463dde86c60426579_20077_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-control/MPC1_hu0c2439c99addb37463dde86c60426579_20077_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;248&#34;
		data-flex-basis=&#34;597px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;optimization&#34;&gt;Optimization&lt;/h3&gt;
&lt;p&gt;We send the model to &lt;strong&gt;an optimization engine&lt;/strong&gt; to search for the best control inputs among a dense mathemetical space.&lt;/p&gt;
&lt;p&gt;Since our goal is to find a suitable control sequence, considering &lt;strong&gt;constraints&lt;/strong&gt; can narrow the scope of our consideration and speed up the execution of the algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Steering range that vehicle can achieve&lt;/li&gt;
&lt;li&gt;Acceleration range that can be used for acceleration/deceleration.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The optimization engine evaluates control inputs indirectly &lt;strong&gt;by modelling a trajectory for the vehicle&lt;/strong&gt;. It uses a cost function which considers deviation from the target trajectory, acceleration, passenger comfort, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/MPC2.jpg&#34;
	width=&#34;605&#34;
	height=&#34;258&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-control/MPC2_hu34c74c88aa512aa8230236faed201477_23279_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-control/MPC2_hu34c74c88aa512aa8230236faed201477_23279_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;234&#34;
		data-flex-basis=&#34;562px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;pros--cons-1&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Pros&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More accurate than PID contorl&lt;/li&gt;
&lt;li&gt;Works for different situations by using different cost functions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More complex, slowere and harder to implement than PID control&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[AD Fundamentals] Planning</title>
        <link>https://tonyfpy.github.io/p/ad-fundamentals-planning/</link>
        <pubDate>Sat, 14 Jan 2023 22:19:57 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/ad-fundamentals-planning/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Jan. 14th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Jan. 14th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains notes from the course &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.udacity.com/course/self-driving-car-fundamentals-featuring-apollo--ud0419&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Driving Fundamentals: Featuring Apollo&lt;/a&gt;&lt;/strong&gt; published by &lt;strong&gt;Udacity&lt;/strong&gt; &amp;amp; &lt;strong&gt;Baidu Apollo&lt;/strong&gt;. This course aims to deliver key parts of self-driving cars, including HD Map, localization, perception, prediction, planning and control. I posted these &amp;ldquo;notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;intro-to-planning&#34;&gt;Intro to Planning&lt;/h2&gt;
&lt;p&gt;In planning, we incorporate HD maps, localization, and prediction to build a trajectory for the vehicle.&lt;/p&gt;
&lt;p&gt;The first step of planning is &lt;strong&gt;route navigation&lt;/strong&gt;, which focuses on how to go from A to B on the map. Routing takes &lt;strong&gt;the map data as input&lt;/strong&gt; and &lt;strong&gt;output a navigation path&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Once we have constructed a high-level route, we zoom into trajectory planning. It is about how to &lt;strong&gt;make subtle decisons&lt;/strong&gt; to avoid obstacles and create a smooth ride for passengers.&lt;/p&gt;
&lt;p&gt;The goal of trajectory planning is to generate a collision-free and comfortable trajectory to execute. This trajectory is defined by &lt;strong&gt;a sequence of points&lt;/strong&gt;, each of which has &lt;strong&gt;an associated velocity&lt;/strong&gt; and &lt;strong&gt;a timestamp&lt;/strong&gt; to indicates when we should arrive at that point.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;routing&#34;&gt;Routing&lt;/h2&gt;
&lt;p&gt;The goal of routing is to find the best path to travel from A to B on the map. It takes 3 input:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The map
&lt;ul&gt;
&lt;li&gt;Road network&lt;/li&gt;
&lt;li&gt;Real-time traffic info&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Our current position on the map&lt;/li&gt;
&lt;li&gt;The destination&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;world-to-graph&#34;&gt;World to Graph&lt;/h3&gt;
&lt;p&gt;Apollo uses &lt;strong&gt;searching&lt;/strong&gt; algorithms to find a route. Before that, the system reformats the map into a &lt;strong&gt;graph&lt;/strong&gt;, where the &lt;strong&gt;nodes&lt;/strong&gt; represent the sections of road and &lt;strong&gt;edges&lt;/strong&gt; represent connections between those sections.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/w2g.jpg&#34;
	width=&#34;585&#34;
	height=&#34;264&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/w2g_huf08bac82b04eb79034708466bac48a05_21777_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/w2g_huf08bac82b04eb79034708466bac48a05_21777_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;221&#34;
		data-flex-basis=&#34;531px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;a-and-grid-world&#34;&gt;A* and Grid World&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/A*1.jpg&#34;
	width=&#34;577&#34;
	height=&#34;197&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/A*1_hu217f816ba0d0f3c7a1170aa8a931fc94_13809_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/A*1_hu217f816ba0d0f3c7a1170aa8a931fc94_13809_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;292&#34;
		data-flex-basis=&#34;702px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;First, we calculate the cost it takes to travel from the start node to that candidate node. ($g$ -&amp;gt; candidate cost)&lt;/p&gt;
&lt;p&gt;Second, we estimate how much it costs to travel from that candidate node to the goal. ($h$ -&amp;gt; estimated cost / heuristic cost)&lt;/p&gt;
&lt;p&gt;The best candidate node is the node that has the minimum cost $f = g + h$.&lt;/p&gt;
&lt;p&gt;The cost may vary based on the real-time traffic info.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/A*2.jpg&#34;
	width=&#34;587&#34;
	height=&#34;257&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/A*2_huccad48e4054980aa293bb7464efa3b0f_19394_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/A*2_huccad48e4054980aa293bb7464efa3b0f_19394_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;228&#34;
		data-flex-basis=&#34;548px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;For example, it takes a lot of effort to turn through the intersection, so we assign a higher $g$ value to the node indicating turning left.&lt;/p&gt;
&lt;p&gt;For the highway option, we realize that we may have to travel a long way to exit the highway and return back to reach the goal. So we assign higher $h$ value to this node.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;trajectory-generation&#34;&gt;Trajectory Generation&lt;/h2&gt;
&lt;p&gt;The high-level map route is only part of the planning process. We still need to build low-level trajectories to deal with objects, like bicycles, pedestrians, that aren&amp;rsquo;t part to the map. This scenario requires &lt;strong&gt;lower-level&lt;/strong&gt;, &lt;strong&gt;high-precision&lt;/strong&gt; planning.&lt;/p&gt;
&lt;h3 id=&#34;frenet-coordinates&#34;&gt;Frenet Coordinates&lt;/h3&gt;
&lt;p&gt;Tradionally, we describe the position of an object using &lt;strong&gt;Cartesian&lt;/strong&gt; coordinates, but it is not optimal for vehicles. If we don&amp;rsquo;t know the road, we don&amp;rsquo;t know how far the vehicle has traveled or whether it is deviated from the center of the lane.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frenet&lt;/strong&gt; coordinate describes the position of the car with respect to the road. &lt;strong&gt;Longitudinal axis&lt;/strong&gt; represents the distance along the road, while &lt;strong&gt;lateral axis&lt;/strong&gt; represents the displacement from the longitudinal line. These two axis are &lt;strong&gt;perpendicular&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/FC.jpg&#34;
	width=&#34;563&#34;
	height=&#34;234&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/FC_hu67eb9f22f3c181bf792ed7b828f0b2ba_15719_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/FC_hu67eb9f22f3c181bf792ed7b828f0b2ba_15719_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;240&#34;
		data-flex-basis=&#34;577px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;3d-trajectory&#34;&gt;3D Trajectory&lt;/h3&gt;
&lt;p&gt;The goal is to generate a trajectory defined by a series of points. We can fit a curve to these points to create a &lt;strong&gt;geometric representation&lt;/strong&gt; of the trajectory.&lt;/p&gt;
&lt;p&gt;Since moving obstacles may block parts of the road temporarily, each points has a &lt;strong&gt;timestamp&lt;/strong&gt;. We can combine the timestamp with the output of the prediction module to verify if the points are vacant at the time we plan to move through them.&lt;/p&gt;
&lt;p&gt;These timestamps create a 3D trajectory with each point defined by a 2D space and a third dimension in time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/3DT.jpg&#34;
	width=&#34;568&#34;
	height=&#34;206&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/3DT_hu568cac5d3e9217b87cef41e48641c8e5_17994_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/3DT_hu568cac5d3e9217b87cef41e48641c8e5_17994_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;275&#34;
		data-flex-basis=&#34;661px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We also designate a &lt;strong&gt;velocity&lt;/strong&gt; for each point, which ensures that the vehicle can reach the point on schedule.&lt;/p&gt;
&lt;h3 id=&#34;evaluating-a-trajectory&#34;&gt;Evaluating a Trajectory&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Constraints&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The final position aligns with the center line of the lane.&lt;/li&gt;
&lt;li&gt;The distance from obstacles at every point of the trajectory&lt;/li&gt;
&lt;li&gt;Traffic laws, such as speed limit, etc.&lt;/li&gt;
&lt;li&gt;The trajectory should be physically viable for the vehicle.&lt;/li&gt;
&lt;li&gt;Passenger&amp;rsquo;s comfort&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cost Function&lt;/strong&gt;
We can use cost function to choose a right trajectory among different possible candidates. Different scenarios require different cost functions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/CF.jpg&#34;
	width=&#34;583&#34;
	height=&#34;287&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/CF_hu98a06625e9ce5ae16a642d01a81c7a95_25540_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/CF_hu98a06625e9ce5ae16a642d01a81c7a95_25540_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;203&#34;
		data-flex-basis=&#34;487px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;em-planning&#34;&gt;EM Planning&lt;/h2&gt;
&lt;p&gt;EM Planning is also called &lt;strong&gt;Path-Velocity Decoupled Planning&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;path-planning&#34;&gt;Path Planning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generating &lt;strong&gt;candidate&lt;/strong&gt; paths
&lt;ul&gt;
&lt;li&gt;Segmenting the road into &lt;strong&gt;cells&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Randomly sampling&lt;/strong&gt; the points from each cell&lt;/li&gt;
&lt;li&gt;Selecting one point from each cell and connecting points together&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using a &lt;strong&gt;cost&lt;/strong&gt; function which considers smoothness, safety, and other factors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ranking&lt;/strong&gt; the paths based on their costs and choose a best path&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/CG.jpg&#34;
	width=&#34;584&#34;
	height=&#34;195&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/CG_hu8958c18cb0dfded1bc2debac519123d0_15439_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/CG_hu8958c18cb0dfded1bc2debac519123d0_15439_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;299&#34;
		data-flex-basis=&#34;718px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;speed-planning&#34;&gt;Speed Planning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Determining a sequence of speeds (&lt;strong&gt;a speed profile&lt;/strong&gt;) associated with points&lt;/li&gt;
&lt;li&gt;Using ST Graph can help design and select the speed profile
&lt;ul&gt;
&lt;li&gt;S represents the longitudinal displacement of the vehicle&lt;/li&gt;
&lt;li&gt;T represents the time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discretizing&lt;/strong&gt; the ST graph into multiple cells
&lt;ul&gt;
&lt;li&gt;Speed changes between cells&lt;/li&gt;
&lt;li&gt;Speed remains constant with each cell&lt;/li&gt;
&lt;li&gt;Obstacles can be drawn as rectangles that block off certain parts of the road during certain time periods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimizing&lt;/strong&gt; the speed profile subject to various constraints&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/ST1.jpg&#34;
	width=&#34;437&#34;
	height=&#34;241&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/ST1_hu78d8f8d2d79498cbe72e68e14f2e7e81_10739_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/ST1_hu78d8f8d2d79498cbe72e68e14f2e7e81_10739_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;435px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/ST2.jpg&#34;
	width=&#34;358&#34;
	height=&#34;249&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/ST2_hu691a174c9695507476ab7af3188cd198_15138_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/ST2_hu691a174c9695507476ab7af3188cd198_15138_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;345px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;trajectory-optimization&#34;&gt;Trajectory Optimization&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Discretization&lt;/strong&gt; makes problems easier to solve, but the solution are not smooth. We can &lt;strong&gt;quadratic programming&lt;/strong&gt; to convert discrete solutions into a smooth trajectory, which fits a smooth non-linear curve to these piecewise linear segments.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lattice-planning&#34;&gt;Lattice Planning&lt;/h2&gt;
&lt;p&gt;3D Trajectory $\rightarrow$ Longitudinal Dimension + Lateral Dimension + Time Dimension&lt;/p&gt;
&lt;p&gt;We can convert it into 2D problems&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ST Trajectory: Longitudinal Dimension + Time Dimension&lt;/li&gt;
&lt;li&gt;SL Trajectory: Longitudinal Dimension + Lateral Dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the problems share the same Longitudinal Dimension, we can use it to transform them back to the Cartesian coordinate frame and combine them to construct a 3D trajectory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/lattice1.jpg&#34;
	width=&#34;560&#34;
	height=&#34;230&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/lattice1_hu9edf9aa63836aa1f955e8df4ddd967a4_10275_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/lattice1_hu9edf9aa63836aa1f955e8df4ddd967a4_10275_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;584px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;steps&#34;&gt;Steps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We start by projecting the vehicle&amp;rsquo;s initial state onto ST and SL frames.&lt;/li&gt;
&lt;li&gt;We select ending state by sampling multiple candidate ending states from pre-selected patterns.&lt;/li&gt;
&lt;li&gt;For each candidate ending state, we build a set of trajectories to transition our vehicle from its inital state to the ending state.&lt;/li&gt;
&lt;li&gt;We use cost function to select an optimal trajectory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/lattice2.jpg&#34;
	width=&#34;552&#34;
	height=&#34;220&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/lattice2_hudc62f95b52677e7f0baefeef9b2edf25_13855_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/lattice2_hudc62f95b52677e7f0baefeef9b2edf25_13855_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;250&#34;
		data-flex-basis=&#34;602px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ending-states-for-st&#34;&gt;Ending States for ST&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cruising&lt;/li&gt;
&lt;li&gt;Following&lt;/li&gt;
&lt;li&gt;Stopping&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ending-states-for-sl&#34;&gt;Ending States for SL&lt;/h3&gt;
&lt;p&gt;We assume that the vehicle will steadily align with the central line of the lane no matter what ending state it would enter. The trajectory should end with the vehicle aligned with the lane and driving straight.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/ESSL.jpg&#34;
	width=&#34;569&#34;
	height=&#34;240&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-planning/ESSL_hu1334ef048ed9d95d2ad3beba8125062d_13248_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-planning/ESSL_hu1334ef048ed9d95d2ad3beba8125062d_13248_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;237&#34;
		data-flex-basis=&#34;569px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;polynomial-fitting&#34;&gt;Polynomial Fitting&lt;/h3&gt;
&lt;p&gt;We will fit a polynomial to the initial state and the ending state.&lt;/p&gt;
&lt;p&gt;Here, the initial conditions and end conditions are both tuples with &lt;strong&gt;position, speed, and acceleration&lt;/strong&gt; on $s$ coordinate. Note that speed is the first-order derivative of position and acceleration is the second-order derivative of position.&lt;/p&gt;
&lt;p&gt;Usually, we denote the derivative with respect to time as a dot above a variable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initial condition: $(s_{0}, \dot{s_0}, \ddot{s_0})$&lt;/li&gt;
&lt;li&gt;Ending condition: $(s_{1}, \dot{s_1}, \ddot{s_1})$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The polynomial we are going to fit looks like:&lt;/p&gt;
&lt;p&gt;$$s(t)=a t^{5}+b t^{4}+c t^{3}+d t^{2}+e t+f$$&lt;/p&gt;
&lt;p&gt;By taking the derivative of this form we get:&lt;/p&gt;
&lt;p&gt;$$\dot{s(t)}=5 a t^{4}+4 b t^{3}+3 c t^{2}+2 d t+e$$
$$\ddot{s(t)}=20 a t^{3}+12 b t^{2}+6 c t+2 d$$&lt;/p&gt;
&lt;p&gt;Now we plug in the initial condition and ending condition, we have 6 equations. Here we suppose the ending condition happen at time $t = T$.&lt;/p&gt;
&lt;p&gt;$$s_{0}=f$$
$$\dot{s_0}=e$$
$$\ddot{s_0}=2d$$
$$s_{1}=a T^{5}+b T^{4}+c T^{3}+d T^{2}+eT+f$$
$$\dot{s_{1}}=5 a T^{4}+4 b T^{3}+3 c T^{2}+2 d T+e$$
$$\ddot{s_{1}}=20 a T^{3}+12 b T^{2}+6 c T+2 d$$&lt;/p&gt;
&lt;p&gt;With these equations, we can solve for $a$, $b$, $c$, $d$, $e$, and $f$. This represents a curve that smoothly connects two conditions.&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[AD Fundamentals] Prediction</title>
        <link>https://tonyfpy.github.io/p/ad-fundamentals-prediction/</link>
        <pubDate>Fri, 13 Jan 2023 16:17:36 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/ad-fundamentals-prediction/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Jan. 13th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Jan. 13th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains notes from the course &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.udacity.com/course/self-driving-car-fundamentals-featuring-apollo--ud0419&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Driving Fundamentals: Featuring Apollo&lt;/a&gt;&lt;/strong&gt; published by &lt;strong&gt;Udacity&lt;/strong&gt; &amp;amp; &lt;strong&gt;Baidu Apollo&lt;/strong&gt;. This course aims to deliver key parts of self-driving cars, including HD Map, localization, perception, prediction, planning and control. I posted these &amp;ldquo;notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;prediction-approaches&#34;&gt;Prediction Approaches&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/approaches.jpg&#34;
	width=&#34;576&#34;
	height=&#34;273&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/approaches_hufe14f6e94e41aa6fc286b962ad80ca7f_30509_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-prediction/approaches_hufe14f6e94e41aa6fc286b962ad80ca7f_30509_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;210&#34;
		data-flex-basis=&#34;506px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Neither approach (model-based or data-driven) is strictly better than the other but there are certain situations in which one is more useful than the other.&lt;/p&gt;
&lt;h3 id=&#34;model-based-prediction&#34;&gt;Model-based Prediction&lt;/h3&gt;
&lt;p&gt;The approach is &lt;strong&gt;intuitive&lt;/strong&gt;, which incorporates with our existing knowledge of physics, traffic laws and human behavior.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constructing different models for scenarios to indicate different vehicle behaviors.&lt;/li&gt;
&lt;li&gt;Observing the vehicle to see if the movement matches one of the models.&lt;/li&gt;
&lt;li&gt;E.g. Determining maximum safe turning speed on a wet road.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-driven-prediction&#34;&gt;Data-driven Prediction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Feeding the data to train a &lt;strong&gt;machine learning&lt;/strong&gt; model.&lt;/li&gt;
&lt;li&gt;E.g. Predicting the behavior of an unidentified object sitting on the road.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lane-sequence-prediction&#34;&gt;Lane Sequence Prediction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A model-based prediction adopted by Apollo&lt;/li&gt;
&lt;li&gt;Diviing the road into multiple &lt;strong&gt;lane segments&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Each segment covers a region in which car motion can be easy to describe.&lt;/li&gt;
&lt;li&gt;Grouping vehicle behaviors into different patterns and describning those patterns as sequences of lane segments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/LS.jpg&#34;
	width=&#34;561&#34;
	height=&#34;261&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/LS_hu636e2c8d4adf2e407e4be4c901f5a5c2_25588_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-prediction/LS_hu636e2c8d4adf2e407e4be4c901f5a5c2_25588_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;214&#34;
		data-flex-basis=&#34;515px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;obstacle-status&#34;&gt;Obstacle Status&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Position&lt;/li&gt;
&lt;li&gt;Speed&lt;/li&gt;
&lt;li&gt;Direction&lt;/li&gt;
&lt;li&gt;Acceleration&lt;/li&gt;
&lt;li&gt;Position inside the lane segment
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;longitudinal&lt;/strong&gt; distances from the object to the lane segement boundaries&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;lateral&lt;/strong&gt; distances from the object to the lane segement boundaries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;State info from previous timesteps&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;target-lane-prediction&#34;&gt;Target Lane Prediction&lt;/h2&gt;
&lt;h3 id=&#34;lane-segment-transitions&#34;&gt;Lane Segment Transitions&lt;/h3&gt;
&lt;p&gt;The goal of lane prediction is to generate trajectories for objects on the road. This is quite complicated.&lt;/p&gt;
&lt;p&gt;We can look at predicting the transitions between lane segments. We can make this choice by calculating the probabilities for each lane sequence.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/TL.jpg&#34;
	width=&#34;567&#34;
	height=&#34;266&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/TL_hue1f2c6f5f252b79d1994e145cad532e4_18367_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-prediction/TL_hue1f2c6f5f252b79d1994e145cad532e4_18367_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;213&#34;
		data-flex-basis=&#34;511px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;rnn-for-target-lane-prediction&#34;&gt;RNN for Target Lane Prediction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/RNN.jpg&#34;
	width=&#34;575&#34;
	height=&#34;261&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/RNN_hua88231a01393e4874e58106c9c3ebcb0_25345_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-prediction/RNN_hua88231a01393e4874e58106c9c3ebcb0_25345_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;220&#34;
		data-flex-basis=&#34;528px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Apollo provides one RNN model for the lane sequences and another RNN model for the associated objects states.&lt;/p&gt;
&lt;p&gt;Apollo then concatenates the outputs of these two RNNs and feeds them into another NN which estimates &lt;strong&gt;the probabilty for each lane sequence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/RNNTL.jpg&#34;
	width=&#34;565&#34;
	height=&#34;262&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/RNNTL_hue432bad21cbccdf7caa25dafead4b35f_13564_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-prediction/RNNTL_hue432bad21cbccdf7caa25dafead4b35f_13564_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;215&#34;
		data-flex-basis=&#34;517px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;trajectory-generation&#34;&gt;Trajectory Generation&lt;/h3&gt;
&lt;p&gt;Trajectory generation is the final step of lane prediction. How to generate the most likely trajectories after predicting the lance sequence?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/candidates.jpg&#34;
	width=&#34;570&#34;
	height=&#34;264&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-prediction/candidates_hu55af051caa4696df99d812b9629dcde1_20157_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-prediction/candidates_hu55af051caa4696df99d812b9629dcde1_20157_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;215&#34;
		data-flex-basis=&#34;518px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Setting constraints between A and B
&lt;ul&gt;
&lt;li&gt;Assuming the car will align with the &lt;strong&gt;center&lt;/strong&gt; of the target lane.&lt;/li&gt;
&lt;li&gt;Ignoring trajectories that are impossible to be physically executed by the vehicle.&lt;/li&gt;
&lt;li&gt;Considering the vehicle&amp;rsquo;s current state&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Constructing a &lt;strong&gt;motion model&lt;/strong&gt; given the initial state and final state
&lt;ul&gt;
&lt;li&gt;E.g. Polynomial model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;prediction-demo&#34;&gt;Prediction Demo&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/zZ107sB6-5A&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Video Link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[AD Fundamentals] Perception</title>
        <link>https://tonyfpy.github.io/p/ad-fundamentals-perception/</link>
        <pubDate>Tue, 22 Nov 2022 16:20:15 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/ad-fundamentals-perception/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Nov. 22th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Nov. 22th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains notes from the course &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.udacity.com/course/self-driving-car-fundamentals-featuring-apollo--ud0419&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Driving Fundamentals: Featuring Apollo&lt;/a&gt;&lt;/strong&gt; published by &lt;strong&gt;Udacity&lt;/strong&gt; &amp;amp; &lt;strong&gt;Baidu Apollo&lt;/strong&gt;. This course aims to deliver key parts of self-driving cars, including HD Map, localization, perception, prediction, planning and control. I posted these &amp;ldquo;notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;computer-vision-for-ad&#34;&gt;Computer Vision for AD&lt;/h2&gt;
&lt;h3 id=&#34;detection&#34;&gt;Detection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find all pedestrians in the driving record image.&lt;/li&gt;
&lt;li&gt;Localize the position of the traffic light in an image.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Alogorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf?spm=5176.100239.blogcont55892.8.pm8zm1&amp;amp;file=Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Fast R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Faster R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1506.02640.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;YOLO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1512.02325.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;SSD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;classification&#34;&gt;Classification&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Determine if the obstacle in a picture is a pedestrian or a biker.&lt;/li&gt;
&lt;li&gt;Recognize the status of a traffic light.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ![](white.jpg)![](classification.jpg)![](white.jpg) --&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/classification.jpg&#34;
	width=&#34;1122&#34;
	height=&#34;568&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/classification_hufa211ebd315be5d2e35ac8c7fcb1592c_54921_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-perception/classification_hufa211ebd315be5d2e35ac8c7fcb1592c_54921_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;474px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;tracking&#34;&gt;Tracking&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mark a dangerously driven vehicle on the road.&lt;/li&gt;
&lt;li&gt;Differentiating multiple cars on the road in a sequence of continuous driving record frames.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Why tracking?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tracking handles occlusion.&lt;/li&gt;
&lt;li&gt;Tracking preserves identity.&lt;/li&gt;
&lt;li&gt;Tracking could be combined with a predictive algorithm to predict the future behavior of a vehicle.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;segmentation&#34;&gt;Segmentation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Determine which pixels in the image captured by the camera correspond to the travelable area.&lt;/li&gt;
&lt;li&gt;Distinguish lanes and road signs in driving record images&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Alogorithm&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1411.4038.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Fully Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ![](white.jpg)![](FCN.jpg)![](white.jpg) --&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/FCN.jpg&#34;
	width=&#34;1158&#34;
	height=&#34;636&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/FCN_hu55251f95733d4dba1d40be495f619cd2_41317_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-perception/FCN_hu55251f95733d4dba1d40be495f619cd2_41317_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;182&#34;
		data-flex-basis=&#34;436px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;camera-images--lidar-images&#34;&gt;Camera Images &amp;amp; LiDAR Images&lt;/h2&gt;
&lt;h3 id=&#34;camera-images&#34;&gt;Camera Images&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Width * Height * Depth&lt;/li&gt;
&lt;li&gt;Pixel $\leftarrow$ (R, G, B)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lidar-images&#34;&gt;LiDAR Images&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LiDAR Pulses $\leftarrow$ Point Cloud Representation&lt;/li&gt;
&lt;li&gt;Shape and Surface Texture&lt;/li&gt;
&lt;li&gt;Spatial Info&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;appllo-perception&#34;&gt;Appllo Perception&lt;/h2&gt;
&lt;h3 id=&#34;obstacle-perceptioin&#34;&gt;Obstacle Perceptioin&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ROI Filtering&lt;/li&gt;
&lt;li&gt;3D Object Detection&lt;/li&gt;
&lt;li&gt;Detection to Track Association&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/3DOD.jpg&#34;
	width=&#34;1038&#34;
	height=&#34;456&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/3DOD_hu55ea689bb8c0766424289e4fc972f204_25579_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-perception/3DOD_hu55ea689bb8c0766424289e4fc972f204_25579_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;227&#34;
		data-flex-basis=&#34;546px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;traffic-lights-detection&#34;&gt;Traffic Lights Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HD Map $\leftarrow$ Location of the light&lt;/li&gt;
&lt;li&gt;Detection &amp;amp; Classification&lt;/li&gt;
&lt;li&gt;Matching multiple lights with lane lines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/LL.jpg&#34;
	width=&#34;1118&#34;
	height=&#34;528&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/LL_hue512f3189380d90b5d184b23bc2631ee_33206_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-perception/LL_hue512f3189380d90b5d184b23bc2631ee_33206_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;211&#34;
		data-flex-basis=&#34;508px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sensor-data-comparisons&#34;&gt;Sensor Data Comparisons&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/sensor_fusion.jpg&#34;
	width=&#34;1126&#34;
	height=&#34;566&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/sensor_fusion_hu2613074c8a73928e25d7d67d2873ef8b_46459_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-perception/sensor_fusion_hu2613074c8a73928e25d7d67d2873ef8b_46459_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;198&#34;
		data-flex-basis=&#34;477px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;radar&#34;&gt;Radar&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Wavelength in mm&lt;/li&gt;
&lt;li&gt;Can sense non-line of sight objects&lt;/li&gt;
&lt;li&gt;Can currently directly measure velocity&lt;/li&gt;
&lt;li&gt;Low Resolution&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lirar&#34;&gt;LiRAR&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Wavelength in infrared&lt;/li&gt;
&lt;li&gt;Higher Resolution&lt;/li&gt;
&lt;li&gt;Most affected by dirt and small debris&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;perception-fusion-strategy&#34;&gt;Perception Fusion Strategy&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/kalman_filter.jpg&#34;
	width=&#34;1020&#34;
	height=&#34;548&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-perception/kalman_filter_hu38f1ed307f8734fc67fa20ecbd189b0c_32753_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-perception/kalman_filter_hu38f1ed307f8734fc67fa20ecbd189b0c_32753_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;186&#34;
		data-flex-basis=&#34;446px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[AD Fundamentals] Localization</title>
        <link>https://tonyfpy.github.io/p/ad-fundamentals-localization/</link>
        <pubDate>Sun, 30 Oct 2022 16:02:49 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/ad-fundamentals-localization/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 30th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Nov 21st, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains notes from the course &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.udacity.com/course/self-driving-car-fundamentals-featuring-apollo--ud0419&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Driving Fundamentals: Featuring Apollo&lt;/a&gt;&lt;/strong&gt; published by &lt;strong&gt;Udacity&lt;/strong&gt; &amp;amp; &lt;strong&gt;Baidu Apollo&lt;/strong&gt;. This course aims to deliver key parts of self-driving cars, including HD Map, localization, perception, prediction, planning and control. I posted these &amp;ldquo;notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;intro-to-localization&#34;&gt;Intro to Localization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It makes the car know where it is.&lt;/li&gt;
&lt;li&gt;It compares the data observed from vehicle sensors and the data on the HD map to match its position.&lt;/li&gt;
&lt;li&gt;Vehicle coordinate frame $\Longleftrightarrow$ data transformation $\Longleftrightarrow$ map coordinate frame&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/coordinate.jpg&#34;
	width=&#34;1124&#34;
	height=&#34;620&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/coordinate_hud65c133f53f97bea3199aa4d5cf15836_53896_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/coordinate_hud65c133f53f97bea3199aa4d5cf15836_53896_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;435px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;gnss-rtk&#34;&gt;GNSS RTK&lt;/h2&gt;
&lt;p&gt;A GPS receiver capable of &lt;strong&gt;Real-time kinematic positioning&lt;/strong&gt; (RTK) takes in the normal signals from the &lt;strong&gt;Global Navigation Satellite Systems&lt;/strong&gt; (GNSS) along with a correction stream to achieve centimeter-level positional accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/RTK.jpg&#34;
	width=&#34;1124&#34;
	height=&#34;624&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/RTK_hu19ead76b1306ebe36460fe2f16adb6ed_46083_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/RTK_hu19ead76b1306ebe36460fe2f16adb6ed_46083_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;432px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;triangulation&#34;&gt;Triangulation&lt;/h3&gt;
&lt;p&gt;Triangulation is a method for calculating a position that relies on a known &lt;strong&gt;distance&lt;/strong&gt; between &lt;strong&gt;two&lt;/strong&gt; measuring apparatuses and the measured &lt;strong&gt;angles&lt;/strong&gt; from those two points to an object. This works using the angle-side-angle triangle congruency theorem to the find the location of an object.&lt;/p&gt;
&lt;h3 id=&#34;trilateration&#34;&gt;Trilateration&lt;/h3&gt;
&lt;p&gt;Trilateration is the more common method for position calculations. Trilateration uses the known &lt;strong&gt;distance&lt;/strong&gt; from at least &lt;strong&gt;three fixed points in 2D space&lt;/strong&gt; or &lt;strong&gt;four fixed points in 3D space&lt;/strong&gt; (as if on the surface of the Earth) to calculate the position of an object. Trilateration works by finding the intersection of a series of circles (imagine in a Venn diagram).&lt;/p&gt;
&lt;h3 id=&#34;global-positioning-system-gps&#34;&gt;Global Positioning System (GPS)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Satellite&lt;/li&gt;
&lt;li&gt;Control Stations
&lt;ul&gt;
&lt;li&gt;It keeps the system running and verify the accuracy of the signals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GPS receiver
&lt;ul&gt;
&lt;li&gt;It exists in mobile phones, computers, cars, etc.&lt;/li&gt;
&lt;li&gt;It shoud be able to detect &amp;gt;= 4 GPS satellites at one under appropriate environment.&lt;/li&gt;
&lt;li&gt;It uses &lt;strong&gt;time of fight&lt;/strong&gt; (ToF) to measure the distance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Pros &amp;amp; Cons&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accurate with RTK&lt;/li&gt;
&lt;li&gt;Poor performance in urban area and canyons&lt;/li&gt;
&lt;li&gt;Low frequency update&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;inertial-navigation&#34;&gt;Inertial Navigation&lt;/h2&gt;
&lt;h3 id=&#34;basic-idea&#34;&gt;Basic Idea&lt;/h3&gt;
&lt;p&gt;Acceleration, initial velocity, initial position could be used to calculate the velocity and location of the car.&lt;/p&gt;
&lt;h3 id=&#34;inertial-measurement-unit-imu&#34;&gt;Inertial Measurement Unit (IMU)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accelerometer&lt;/li&gt;
&lt;li&gt;Gyroscope&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Pros &amp;amp; Cons&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High frequncy update $\rightarrow$ Real-time Location Info&lt;/li&gt;
&lt;li&gt;Motion error increases with time&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lidar-localization&#34;&gt;LiDAR Localization&lt;/h2&gt;
&lt;h3 id=&#34;point-cloud-matching&#34;&gt;Point Cloud Matching&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Errors&lt;/strong&gt; produced when matching point cloud measurement with HD map
&lt;ul&gt;
&lt;li&gt;The # of points collected by LiDAR&lt;/li&gt;
&lt;li&gt;Moving objects, such as cars, pedestrians, etc.&lt;/li&gt;
&lt;li&gt;The transformation of point cloud measurement&lt;/li&gt;
&lt;li&gt;Error produced by LiDAR itself&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterative Closest Point&lt;/strong&gt; Algorithms&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Filter&lt;/strong&gt; Algorithms
&lt;ul&gt;
&lt;li&gt;Histogram filter&lt;/li&gt;
&lt;li&gt;Kalman  filter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pros--cons&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Robustness&lt;/li&gt;
&lt;li&gt;Maintaining up-to-date HD maps&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-localization&#34;&gt;Visual Localization&lt;/h2&gt;
&lt;h3 id=&#34;visual-data&#34;&gt;Visual Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Usually we combine &lt;strong&gt;camera data with a map and GPS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Partical Filter&lt;/strong&gt; Algorithms
&lt;ul&gt;
&lt;li&gt;Observations&lt;/li&gt;
&lt;li&gt;Probability&lt;/li&gt;
&lt;li&gt;Map&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pros--cons-1&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Camera data is easy to obtain&lt;/li&gt;
&lt;li&gt;Lack of 3D Info&lt;/li&gt;
&lt;li&gt;Reliance on 3D maps&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;apollo-localization&#34;&gt;Apollo Localization&lt;/h2&gt;
&lt;h3 id=&#34;multi-sensor-fusion&#34;&gt;Multi-sensor Fusion&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPS&lt;/li&gt;
&lt;li&gt;IMU&lt;/li&gt;
&lt;li&gt;LiDAR&lt;/li&gt;
&lt;li&gt;Radar&lt;/li&gt;
&lt;li&gt;HD Maps&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kalman-filter&#34;&gt;Kalman Filter&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/kalman-filter.jpg&#34;
	width=&#34;1124&#34;
	height=&#34;780&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/kalman-filter_hu10a07b120acce79578111e0bdef53a28_39933_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/kalman-filter_hu10a07b120acce79578111e0bdef53a28_39933_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;345px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-localization/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[AD Fundamentals] High-Definition Maps</title>
        <link>https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/</link>
        <pubDate>Sat, 29 Oct 2022 20:32:50 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 29th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 30th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains notes from the course &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.udacity.com/course/self-driving-car-fundamentals-featuring-apollo--ud0419&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Driving Fundamentals: Featuring Apollo&lt;/a&gt;&lt;/strong&gt; published by &lt;strong&gt;Udacity&lt;/strong&gt; &amp;amp; &lt;strong&gt;Baidu Apollo&lt;/strong&gt;. This course aims to deliver key parts of self-driving cars, including HD Map, localization, perception, prediction, planning and control. I posted these &amp;ldquo;notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hd-maps&#34;&gt;HD Maps&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Accurate 3D representation of the road network
&lt;ul&gt;
&lt;li&gt;e.g. layout of intersections, locations of signposts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Semantic info
&lt;ul&gt;
&lt;li&gt;e.g. traffic light, speed limit, lane rules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3D model of the city
&lt;ul&gt;
&lt;li&gt;e.g. roads, buildings, tunnels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;localization-percpeption-and-planning&#34;&gt;Localization, Percpeption and Planning&lt;/h2&gt;
&lt;h3 id=&#34;localization-with-hd-maps&#34;&gt;Localization with HD Maps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Comparisons between observed data and data in the HD map&lt;/li&gt;
&lt;li&gt;Preprocessing, coordinate transformation, data fusion&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;perception-with-hd-maps&#34;&gt;Perception with HD Maps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HD maps help the car&amp;rsquo;s next decions.
&lt;ul&gt;
&lt;li&gt;Sensors have limits in different situations&lt;/li&gt;
&lt;li&gt;Camera is affected by bad whether and dark environment.&lt;/li&gt;
&lt;li&gt;Other sensors may not detect what is behind the obstacle.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HD maps can narrow the detection scope.
&lt;ul&gt;
&lt;li&gt;Region of Interest (ROI) helps improve speed and accuracy.&lt;/li&gt;
&lt;li&gt;Region of Interest (ROI) helps to save the computation resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;planning-with-hd-maps&#34;&gt;Planning with HD Maps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HD Maps help to find suitable driving space.&lt;/li&gt;
&lt;li&gt;HD Maps help to identify different possible routing options and find best maneuver.&lt;/li&gt;
&lt;li&gt;HD Maps help to forecast future locations for other vehicles on the road.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;apollo-hd-maps&#34;&gt;Apollo HD Maps&lt;/h2&gt;
&lt;h3 id=&#34;road-definitions&#34;&gt;Road Definitions&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/AHDM.jpg&#34;
	width=&#34;1000&#34;
	height=&#34;482&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/AHDM_hu22a08059939e36906b453b9b84a406a1_68883_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/AHDM_hu22a08059939e36906b453b9b84a406a1_68883_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;497px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-high-definition-maps/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;standard-vs-apollo-opendrive&#34;&gt;Standard vs. Apollo OpenDRIVE&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Main Difference&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Standard OpenDRIVE&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Apollo OpenDRIVE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Application Scenario&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Simulation scenarios&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Real-world scenes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Elemental Form Expression&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Reference line&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Absolute coordinate sequences&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Elemental Richness&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Common elements&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;More expressions and attributes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Adaptive Driverless Algorithm&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;N/A&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Integrate Baidus driverless experience&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hd-maps-construction&#34;&gt;HD Maps Construction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data Sourcing
&lt;ul&gt;
&lt;li&gt;Survey vehicle with GPS, IMU, Antenna, LiDAR, Camera&lt;/li&gt;
&lt;li&gt;Crowdsourcing with mobile devices, in-car devices, connected cars, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data Processing
&lt;ul&gt;
&lt;li&gt;Form an inital map template by data sorting, classfiying, cleansing&lt;/li&gt;
&lt;li&gt;Without semantic info and annotations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Object Detection
&lt;ul&gt;
&lt;li&gt;Detect and classify static objects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Manual Verification
&lt;ul&gt;
&lt;li&gt;Ensure the automated map creation correctly&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Map Products
&lt;ul&gt;
&lt;li&gt;Top-down view localication maps&lt;/li&gt;
&lt;li&gt;3D point cloud maps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[AD Fundamentals] An Overview of Baidu Apollo</title>
        <link>https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/</link>
        <pubDate>Sat, 29 Oct 2022 16:07:45 -0400</pubDate>
        
        <guid>https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Oct 29th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Oct 30th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains notes from the course &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.udacity.com/course/self-driving-car-fundamentals-featuring-apollo--ud0419&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Driving Fundamentals: Featuring Apollo&lt;/a&gt;&lt;/strong&gt; published by &lt;strong&gt;Udacity&lt;/strong&gt; &amp;amp; &lt;strong&gt;Baidu Apollo&lt;/strong&gt;. This course aims to deliver key parts of self-driving cars, including HD Map, localization, perception, prediction, planning and control. I posted these &amp;ldquo;notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-self-driving&#34;&gt;What is Self-Driving?&lt;/h2&gt;
&lt;h3 id=&#34;why-self-driving-cars&#34;&gt;Why Self-Driving Cars?&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Human&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Self-Driving Car&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;High Traffic Accident Rate&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;More Reliable Driving&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Learn to Drive From Scratch&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Learnable Driving Sys&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Parking Trouble&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No Parking Trouble&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;levels-of-ad&#34;&gt;Levels of AD&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Level 1 (Driver Assistance)
&lt;ul&gt;
&lt;li&gt;Driver fully engaged&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Level 2 (Partial Automation)
&lt;ul&gt;
&lt;li&gt;Auto cruise control&lt;/li&gt;
&lt;li&gt;Auto lane keeping&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Level 3 (Conditional Automation)
&lt;ul&gt;
&lt;li&gt;Human take over whenever necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Level 4 (No Human Interference)
&lt;ul&gt;
&lt;li&gt;Without sterring wheel, throttle or brake&lt;/li&gt;
&lt;li&gt;Restricted in geofence (certain areas)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Level 5 (Full Automation)
&lt;ul&gt;
&lt;li&gt;Better than human in all scenarios&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;how-self-driving-cars-work&#34;&gt;How Self-Driving Cars Work?&lt;/h2&gt;
&lt;h3 id=&#34;key-components&#34;&gt;Key Components&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Computer Vision
&lt;ul&gt;
&lt;li&gt;How the car sees the world&lt;/li&gt;
&lt;li&gt;e.g. bounding boxes, contour, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sensor Fusion
&lt;ul&gt;
&lt;li&gt;How the car understands the around situtaions&lt;/li&gt;
&lt;li&gt;e.g. distance, relative velocity, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Localization
&lt;ul&gt;
&lt;li&gt;How the car learns where it is&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Path Planning
&lt;ul&gt;
&lt;li&gt;How the car figures out where to go&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Control
&lt;ul&gt;
&lt;li&gt;How the car follows the path or reacts to various situations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;apollo-structure&#34;&gt;Apollo Structure&lt;/h3&gt;
&lt;p&gt;Apollos system centers around HD Maps and Localization. The other components of the system revolve around Perception, Prediction, Planning and Control.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/apollo-structure.jpg&#34;
	width=&#34;1153&#34;
	height=&#34;759&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/apollo-structure_huf49d23a28b0baffc3fbc142237b37f34_48159_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/apollo-structure_huf49d23a28b0baffc3fbc142237b37f34_48159_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;364px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/white.jpg&#34;
	width=&#34;285&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/ad-fundamentals-an-overview-of-baidu-apollo/white_hu0053ed58c52b0a79a00a3298e4f2752e_2127_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;83&#34;
		data-flex-basis=&#34;201px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hardware&#34;&gt;Hardware&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Controller Area Network (CAN)
&lt;ul&gt;
&lt;li&gt;Vehicle&amp;rsquo;s internal communication network&lt;/li&gt;
&lt;li&gt;Signal transmission for acceleration, braking and steering, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Global Positioning System (GPS)
&lt;ul&gt;
&lt;li&gt;Location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Inertial Measurement Unit (IMU)
&lt;ul&gt;
&lt;li&gt;Speed, acceleration and other factors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LiDAR is an array of pulse lasers,
&lt;ul&gt;
&lt;li&gt;The reflection of this laser beams builds the point cloud for enviroment understanding.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Camera
&lt;ul&gt;
&lt;li&gt;Visual data capturing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Radar
&lt;ul&gt;
&lt;li&gt;Obstacle detection&lt;/li&gt;
&lt;li&gt;Low resolution but economical and robust to different wheather/lighting conditions&lt;/li&gt;
&lt;li&gt;Speed measuring of other vehicles&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;open-software-stack&#34;&gt;Open Software Stack&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Real-time Operating Sys (ROTS)
&lt;ul&gt;
&lt;li&gt;It can produce timely calculations, analysis and execute decisions in a short time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Runtime Framework
&lt;ul&gt;
&lt;li&gt;A customized version of Robot Operating System (ROS)&lt;/li&gt;
&lt;li&gt;Modules are independent.&lt;/li&gt;
&lt;li&gt;Shared memory for decentralization and data comparability (Protobuf)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Application modules
&lt;ul&gt;
&lt;li&gt;MAP engine&lt;/li&gt;
&lt;li&gt;Localization, Perception, Planning, Control,&lt;/li&gt;
&lt;li&gt;End-to-end driving&lt;/li&gt;
&lt;li&gt;Human-machine interface (HMI)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cloud-service&#34;&gt;Cloud Service&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data Storage&lt;/li&gt;
&lt;li&gt;Multiple Tools
&lt;ul&gt;
&lt;li&gt;HD map&lt;/li&gt;
&lt;li&gt;Simulation&lt;/li&gt;
&lt;li&gt;Data platform&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;Over-the-air software updates (OTA)&lt;/li&gt;
&lt;li&gt;Intelligent voice system (DuerOS)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
