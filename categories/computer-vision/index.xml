<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Computer Vision on Tony Feng</title>
        <link>https://tonyfpy.github.io/categories/computer-vision/</link>
        <description>Recent content in Computer Vision on Tony Feng</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language><atom:link href="https://tonyfpy.github.io/categories/computer-vision/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[Computer Vision] Stereo Vision</title>
        <link>https://tonyfpy.github.io/p/computer-vision-stereo-vision/</link>
        <pubDate>Thu, 23 Feb 2023 17:21:09 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-stereo-vision/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 23th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 23th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;basic-ideas&#34;&gt;Basic Ideas&lt;/h2&gt;
&lt;p&gt;One case of 3D reconstruction is when we don’t know how far a 3D object is nor the shape of it. But we do know the camera geometry and the projection of that shape onto our images. Now, we’re trying to find what $X$ is.&lt;/p&gt;
&lt;p&gt;However, multiple points in 3D can result to the same projection!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-stereo-vision/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/goal.jpg&#34;
	width=&#34;594&#34;
	height=&#34;340&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/goal_hue07455692750e7c7f0206bbfa357fd8c_20810_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-stereo-vision/goal_hue07455692750e7c7f0206bbfa357fd8c_20810_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-stereo-vision/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We need one point that is along the lines that is the true $(x, y)$ position. Triangulation uses two images and you try to find the same $x$ point in both images. Since both images represent the same object in space, they would both &lt;strong&gt;intersect at a point&lt;/strong&gt;. The second camera/images fixes the image of $X$ along the line we see.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-stereo-vision/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/stereo.jpg&#34;
	width=&#34;662&#34;
	height=&#34;365&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/stereo_huc262580e27ea1f7c1b011d7d15d71961_44322_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-stereo-vision/stereo_huc262580e27ea1f7c1b011d7d15d71961_44322_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;435px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-stereo-vision/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-stereo-vision/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;In the depth image, brighter means it’s closer to the camera, and darker means it’s farther to the camera.&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Camera Calibration</title>
        <link>https://tonyfpy.github.io/p/computer-vision-camera-calibration/</link>
        <pubDate>Thu, 23 Feb 2023 11:30:15 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-camera-calibration/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 23th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 23th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;camera-matrix-model&#34;&gt;Camera Matrix Model&lt;/h2&gt;
&lt;p&gt;Now, we have the camera matrix model&lt;/p&gt;
&lt;p&gt;$$x_{\text{image}} = K\left [ R\mid t \right ] X_{\text{world}}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Camera Simulation&lt;/strong&gt;&lt;br&gt;
We know camera parameters and the world coordinates of an object, the goal is to get its position in an image.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Camera Calibration&lt;/strong&gt;&lt;br&gt;
We know the world coordinates and image coordinates of an object, the goal is to calculate the camera matrices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3D Reconstruction&lt;/strong&gt;&lt;br&gt;
We know the camera parameters and image coordinates of an object, the goal is to get the world coordinates of an object. Sometimes, we don&amp;rsquo;t even know the camera parameters, and just use image coordinates of an object to estimate its world coordinates.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;solving-camera-parameters&#34;&gt;Solving Camera Parameters&lt;/h2&gt;
&lt;p&gt;When we calibrating the camera, we can combine $R$ and $t$ into a matrix $M$, whose dimension is $3 * 4$. How to get $M$, given $x_{\text{image}}$ and $X_{\text{world}}$?&lt;/p&gt;
&lt;h3 id=&#34;least-squares-regression&#34;&gt;Least Squares Regression&lt;/h3&gt;
&lt;p&gt;Given data $(x_1, y_1), (x_2, y_2), &amp;hellip;, (x_n, y_n)$, how to find $a$ and $b$ to fit a linear equation $y = ax+b$?&lt;/p&gt;
&lt;p&gt;$$ E=\sum_{i=1}^{n}\left(y_{i}-m x_{i}-b\right)^{2} = ||Y - XP || ^2$$&lt;/p&gt;
&lt;p&gt;$$\Downarrow $$&lt;/p&gt;
&lt;p&gt;$$ E= {Y}^{T} {Y}-2({XP})^{T} {Y}+({X} {P})^{T}({XP})$$&lt;/p&gt;
&lt;p&gt;$$\Downarrow $$&lt;/p&gt;
&lt;p&gt;$$ \frac{d E}{d P}=2 {X}^{T} {X} {P}-2 {X}^{T} {Y}=0$$&lt;/p&gt;
&lt;p&gt;$$\Downarrow $$&lt;/p&gt;
&lt;p&gt;$$ {P}=\left({X}^{T} {X}\right)^{-1} {X}^{T} {Y}$$&lt;/p&gt;
&lt;h3 id=&#34;matrix-computation&#34;&gt;Matrix Computation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-calibration/LSR_1.jpg&#34;
	width=&#34;469&#34;
	height=&#34;151&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-calibration/LSR_1_hu8d08348d6b35b661cffc0c1410fdf6d7_17176_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-calibration/LSR_1_hu8d08348d6b35b661cffc0c1410fdf6d7_17176_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;310&#34;
		data-flex-basis=&#34;745px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;$$s u =m_{11} X+m_{12} Y+m_{13} Z+m_{14}$$&lt;/p&gt;
&lt;p&gt;$$s v =m_{21} X+m_{22} Y+m_{23} Z+m_{24} $$&lt;/p&gt;
&lt;p&gt;$$s =m_{31} X+m_{32} Y+m_{33} Z+m_{34} $$&lt;/p&gt;
&lt;p&gt;$$\Downarrow $$&lt;/p&gt;
&lt;p&gt;$$u =\frac{m_{11} X+m_{12} Y+m_{13} Z+m_{14}}{m_{31} X+m_{32} Y+m_{33} Z+m_{34}} $$&lt;/p&gt;
&lt;p&gt;$$v =\frac{m_{21} X+m_{22} Y+m_{23} Z+m_{24}}{m_{31} X+m_{32} Y+m_{33} Z+m_{34}} $$&lt;/p&gt;
&lt;p&gt;$$\Downarrow $$&lt;/p&gt;
&lt;p&gt;$$ 0=m_{11} X+m_{12} Y+m_{13} Z+m_{14}-m_{31} u X-m_{32} u Y-m_{33} u Z-m_{34} u $$&lt;/p&gt;
&lt;p&gt;$$ 0=m_{21} X+m_{22} Y+m_{23} Z+m_{24}-m_{31} v X-m_{32} v Y-m_{33} v Z-m_{34} v $$&lt;/p&gt;
&lt;p&gt;$$\Downarrow $$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-calibration/LSR_2.jpg&#34;
	width=&#34;412&#34;
	height=&#34;233&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-calibration/LSR_2_hu20fc0669513e19db4b3c688b4e1953ba_17074_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-calibration/LSR_2_hu20fc0669513e19db4b3c688b4e1953ba_17074_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;176&#34;
		data-flex-basis=&#34;424px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;rq-decomposition&#34;&gt;RQ Decomposition&lt;/h3&gt;
&lt;p&gt;We can decompose $M$ back to $K\left [ R\mid t \right ]$. Here is the &lt;a class=&#34;link&#34; href=&#34;https://ksimek.github.io/2012/08/14/decompose/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;link&lt;/strong&gt;&lt;/a&gt; for more information.&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Camera Geometry</title>
        <link>https://tonyfpy.github.io/p/computer-vision-camera-geometry/</link>
        <pubDate>Thu, 16 Feb 2023 11:11:12 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-camera-geometry/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 16th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 23th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;parametric-transformation&#34;&gt;Parametric Transformation&lt;/h2&gt;
&lt;p&gt;It refers to a transformation $T$ is a coordinate-changing operation so that $p&amp;rsquo; = T(p)$ for any point $p(x,y)$.&lt;/p&gt;
&lt;h3 id=&#34;common-transformations&#34;&gt;Common Transformations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Translation&lt;/li&gt;
&lt;li&gt;Rotation&lt;/li&gt;
&lt;li&gt;Scaling
&lt;ul&gt;
&lt;li&gt;Uniform scaling&lt;/li&gt;
&lt;li&gt;Non-uniform scaling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Affine
&lt;ul&gt;
&lt;li&gt;A combination of linear transformations, and translations&lt;/li&gt;
&lt;li&gt;Lines map to lines&lt;/li&gt;
&lt;li&gt;Parallel lines remain parallel&lt;/li&gt;
&lt;li&gt;Ratios are preserved&lt;/li&gt;
&lt;li&gt;Closed under composition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Perspective&lt;/li&gt;
&lt;li&gt;Projective
&lt;ul&gt;
&lt;li&gt;A combination of affine transformations, and projective warps&lt;/li&gt;
&lt;li&gt;Lines map to lines&lt;/li&gt;
&lt;li&gt;Parallel lines do not necessarily remain parallel&lt;/li&gt;
&lt;li&gt;Ratios are not preserved&lt;/li&gt;
&lt;li&gt;Closed under composition&lt;/li&gt;
&lt;li&gt;Models change of basis&lt;/li&gt;
&lt;li&gt;Projective matrix is defined up to a scale (&lt;strong&gt;8 degrees of freedom&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/2D_transformations.jpg&#34;
	width=&#34;670&#34;
	height=&#34;384&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/2D_transformations_hu9d13bb69f3779370c610495059c43802_36996_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/2D_transformations_hu9d13bb69f3779370c610495059c43802_36996_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;418px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/2D_transformations_1.jpg&#34;
	width=&#34;510&#34;
	height=&#34;168&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/2D_transformations_1_hu3271957f501ed9fea781a9792822fb6f_17030_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/2D_transformations_1_hu3271957f501ed9fea781a9792822fb6f_17030_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;303&#34;
		data-flex-basis=&#34;728px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Name&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Matrix&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Degree of Freedom&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Translation&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\left [I \mid t \right ]_{2\times 3} $&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Rigid (Euclidean)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\left [R \mid t \right ]_{2\times 3} $&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Similarity&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\left [sR \mid t \right ]_{2\times 3}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Affine&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\left [A \right ]_{2\times 3}        $&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Projective&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$\left [\bar{H} \right ]_{2\times 3}  $&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;camera-projection&#34;&gt;Camera Projection&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/CP.jpg&#34;
	width=&#34;831&#34;
	height=&#34;342&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/CP_huce8cb7b219f13d4e45f206a633fb1fdf_30218_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/CP_huce8cb7b219f13d4e45f206a633fb1fdf_30218_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;242&#34;
		data-flex-basis=&#34;583px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;homogeneous-coordinates&#34;&gt;Homogeneous Coordinates&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s an an alternative coordinate system for projective geometry.&lt;/p&gt;
&lt;p&gt;Converting to homogeneous coordinates&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D image coordinates $(x,y) \Rightarrow (x,y,1)$&lt;/li&gt;
&lt;li&gt;3D scene coordinates $(x,y,z) \Rightarrow (x,y,z,1)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Converting from homogeneous coordinates&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D image coordinates $(x,y,w) \Rightarrow (\frac{x}{w}, \frac{y}{w})$&lt;/li&gt;
&lt;li&gt;3D scene coordinates $(x,y,z,w) \Rightarrow (\frac{x}{w}, \frac{y}{w}, \frac{z}{w})$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scale-ambiguity&#34;&gt;Scale Ambiguity&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/SA.jpg&#34;
	width=&#34;396&#34;
	height=&#34;183&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/SA_hu88eb905ea0f9dbc8eff9d19355784a0d_18748_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/SA_hu88eb905ea0f9dbc8eff9d19355784a0d_18748_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;216&#34;
		data-flex-basis=&#34;519px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/SA2.jpg&#34;
	width=&#34;344&#34;
	height=&#34;300&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/SA2_hucc37423277e96079fd6b890aed3224b1_14732_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/SA2_hucc37423277e96079fd6b890aed3224b1_14732_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;114&#34;
		data-flex-basis=&#34;275px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;This means that no matter how we scale the projection space (i.e. the homogeneous coordinate representation), we do not change the underlying image represented by the coordinates.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;projection-matrix&#34;&gt;Projection Matrix&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/CPM.jpg&#34;
	width=&#34;850&#34;
	height=&#34;222&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/CPM_huc2fd62786157e4678915b21978a46458_24425_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/CPM_huc2fd62786157e4678915b21978a46458_24425_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;382&#34;
		data-flex-basis=&#34;918px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We want to find out where a 3D point $P$ in the scene will be located in the 2D image, and this diagram visualizes that process.&lt;/p&gt;
&lt;h3 id=&#34;intrinsic-matrix&#34;&gt;Intrinsic Matrix&lt;/h3&gt;
&lt;p&gt;Previously, we have 2 equations $x = \frac{fX}{Z}$ and $y = \frac{fY}{Z}$. They are not linear as we divide the coordinates by $Z$. But we can use &lt;strong&gt;homogeneous coordinates&lt;/strong&gt; to manupulate the matrix to derive $x$, $y$ from $X$, $Y$, $Z$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/CT.jpg&#34;
	width=&#34;2190&#34;
	height=&#34;430&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/CT_hu49b520efb2e95a6ee537b02ec5b1b836_66407_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/CT_hu49b520efb2e95a6ee537b02ec5b1b836_66407_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;509&#34;
		data-flex-basis=&#34;1222px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Here, $K$ is camera projection matrix, also called &lt;strong&gt;intrinsic matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Optical Center:
The above expression assumed that the origin in the image plane is at the &lt;strong&gt;principle point&lt;/strong&gt; where the $Z$ axis hits the plane. It is not for most case. So, $x = \frac{fX}{Z}+u$ and $y = \frac{fY}{Z}+v$ should be considered.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unit Aspect Ratio:
Also, different camera sensors have different pixel physical pixel size. Hence, we need to find some way to &lt;strong&gt;convert focal length between physical size and pixels&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Skewness:
We need to consider &lt;strong&gt;diagnal distortion&lt;/strong&gt; of the image plane by adding the parameter $s$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, we can obtain&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/intrinsic.jpg&#34;
	width=&#34;1836&#34;
	height=&#34;366&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/intrinsic_hu49b520efb2e95a6ee537b02ec5b1b836_33243_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/intrinsic_hu49b520efb2e95a6ee537b02ec5b1b836_33243_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;501&#34;
		data-flex-basis=&#34;1203px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;extrinsic-matrix&#34;&gt;Extrinsic Matrix&lt;/h3&gt;
&lt;p&gt;So far we assumed that the origin of the 3D coordinate system is located at the pinhole. Because camera may be far from the object and using the position of camera as orgin is not convenient, we like to use world coordinate system in reality to mark the location of an object. We can represent a world point in the camera’s coordinate system by considering the relation between the coordinates of $p$ in camera and world coordinate system.&lt;/p&gt;
&lt;p&gt;$$x_{\text{camera}} = R(x_{\text{world}} - c) = Rx_{\text{world}} - RC = Rx_{\text{world}} - T $$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/w2c.jpg&#34;
	width=&#34;3854&#34;
	height=&#34;815&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/w2c_hu26e740fb0f0718cc919bfe81d98535c6_205520_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/w2c_hu26e740fb0f0718cc919bfe81d98535c6_205520_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;472&#34;
		data-flex-basis=&#34;1134px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;!-- \begin{bmatrix}
X_{\text{camera}} \\
Y_{\text{camera}} \\
Z_{\text{camera}} \\
1
\end{bmatrix} = \begin{bmatrix}
R_{1,1}  &amp;R_{1,2}  &amp;R_{1,3}  &amp;t_{x} \\
R_{2,1}  &amp;R_{2,2}  &amp;R_{2,3}  &amp;t_{y} \\
R_{3,1}  &amp;R_{3,2}  &amp;R_{3,3}  &amp;t_{z} \\
0  &amp;0  &amp;0  &amp;1
\end{bmatrix}\begin{bmatrix}
X_{\text{world}} \\
Y_{\text{world}} \\
Z_{\text{world}} \\
1
\end{bmatrix} = \begin{bmatrix}
R  &amp;t \\
0  &amp;1
\end{bmatrix} \begin{bmatrix}
X_{\text{world}} \\
Y_{\text{world}} \\
Z_{\text{world}} \\
1
\end{bmatrix} --&gt;
&lt;h3 id=&#34;from-world-to-image-plane&#34;&gt;From World to Image Plane&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Transformation from world coordinates to camera coordinates&lt;/li&gt;
&lt;li&gt;Projection onto ideal image plane&lt;/li&gt;
&lt;li&gt;Applying radial lens distortion $x&amp;rsquo; = \text{warp}(x)$&lt;/li&gt;
&lt;li&gt;Mapping to image plane to pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/w2i.jpg&#34;
	width=&#34;3891&#34;
	height=&#34;787&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/w2i_hu26e740fb0f0718cc919bfe81d98535c6_186029_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/w2i_hu26e740fb0f0718cc919bfe81d98535c6_186029_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;494&#34;
		data-flex-basis=&#34;1186px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-camera-geometry/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Here, intrinsic matrix has 5 DoF and extrinsic matrix has 6 DoF.&lt;/p&gt;
&lt;!-- Z_{\text{camera}} \begin{bmatrix}
X_{\text{image}} \\
Y_{\text{image}} \\
1
\end{bmatrix}=\begin{bmatrix}
f_x  &amp;s  &amp;u  &amp; 0\\
 0 &amp; f_y &amp;v &amp;0 \\
  0&amp; 0 &amp;1  &amp;0
\end{bmatrix}\begin{bmatrix}
R_{1,1}  &amp;R_{1,2}  &amp;R_{1,3}  &amp;t_{x} \\
R_{2,1}  &amp;R_{2,2}  &amp;R_{2,3}  &amp;t_{y} \\
R_{3,1}  &amp;R_{3,2}  &amp;R_{3,3}  &amp;t_{z} \\
0  &amp;0  &amp;0  &amp;1
\end{bmatrix} \begin{bmatrix}
X_{\text{world}} \\
Y_{\text{world}} \\
Z_{\text{world}} \\
1
\end{bmatrix} --&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Feature Description &amp; Matching</title>
        <link>https://tonyfpy.github.io/p/computer-vision-feature-description-matching/</link>
        <pubDate>Tue, 14 Feb 2023 14:54:35 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-feature-description-matching/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 14th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 14th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;local-feature-description&#34;&gt;Local Feature Description&lt;/h2&gt;
&lt;p&gt;Before we’re able to start matching points in images, we must first find a way to meaningfully &lt;strong&gt;describe each key point&lt;/strong&gt;. Harris Corners only tell us where the key points are and not what information exists at those points&lt;/p&gt;
&lt;h3 id=&#34;possible-feature-descriptors&#34;&gt;Possible Feature Descriptors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Templates
&lt;ul&gt;
&lt;li&gt;Intensity, gradients, or other self-defined features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Histograms
&lt;ul&gt;
&lt;li&gt;Color, texture, SIFT descriptors, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;histograms&#34;&gt;Histograms&lt;/h3&gt;
&lt;p&gt;A histogram is a plot where the x axis corresponds to pixel values (0-255 for 8-bit encoding), and the y axis measures to the frequency of pixels with values corresponding to the x-axis.&lt;/p&gt;
&lt;p&gt;e.g. at an intensity of 50, we see that there are around 5500 pixels in the image with that intensity).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;scale-invariant-feature-transform&#34;&gt;Scale Invariant Feature Transform&lt;/h2&gt;
&lt;p&gt;SIFT: At a high level, we want to compute a histogram around each key point using local information.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/SIFT.jpg&#34;
	width=&#34;1642&#34;
	height=&#34;390&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/SIFT_hu49b520efb2e95a6ee537b02ec5b1b836_133144_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/SIFT_hu49b520efb2e95a6ee537b02ec5b1b836_133144_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;421&#34;
		data-flex-basis=&#34;1010px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Find Harris corners scale-space extrema as feature point locations&lt;/li&gt;
&lt;li&gt;Corner post-processing
&lt;ul&gt;
&lt;li&gt;Subpixel position interpolation&lt;/li&gt;
&lt;li&gt;Discard low-contrast points&lt;/li&gt;
&lt;li&gt;Eliminate points along edges&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Orientation estimation per feature point&lt;/li&gt;
&lt;li&gt;Descriptor extraction&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scale-space-extrema&#34;&gt;Scale-space Extrema&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Find points whose surrounding patches (at some scale) are distinctive&lt;/li&gt;
&lt;li&gt;Apply convolution with a Gaussian mask gives some idea of what is going on around a pixel&lt;/li&gt;
&lt;li&gt;Gaussian masks have a natural scale - their standard deviation
&lt;ul&gt;
&lt;li&gt;Difference of Gaussians&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/DoG.jpg&#34;
	width=&#34;1070&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/DoG_hu49b520efb2e95a6ee537b02ec5b1b836_103995_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/DoG_hu49b520efb2e95a6ee537b02ec5b1b836_103995_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;336&#34;
		data-flex-basis=&#34;807px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatic Scale Selection&lt;/li&gt;
&lt;li&gt;Normalize the patch to fix size&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;orientation-estimation&#34;&gt;Orientation Estimation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Gradient Orientation Histogram&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To build this histogram, we bin the possible range of gradient orientations/directions (0 to 2pi radians) into a discrete number of bins (8 here) and, for each bin, we count the number of gradients that have those corresponding orientations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dominant Orientation $\theta$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We also compute an average gradient direction (black vector) of all the gradients in the local window; this is the &lt;strong&gt;dominant orientation&lt;/strong&gt; of this neighborhood.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Orientation Normalization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We then rotate each region’s gradients so that all dominant gradient orientations point in the same direction (e.g. up). This will affect the orientation histogram by &lt;strong&gt;shifting the plot&lt;/strong&gt;. We do this because we want the key point features &lt;strong&gt;to be invariant to rotation&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;keypoint-descriptor&#34;&gt;Keypoint Descriptor&lt;/h3&gt;
&lt;p&gt;A 16x16 neighbourhood around the keypoint is taken. It is divided into 16 sub-blocks of 4x4 size.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/descriptor.jpg&#34;
	width=&#34;1116&#34;
	height=&#34;242&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/descriptor_hu49b520efb2e95a6ee537b02ec5b1b836_56257_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/descriptor_hu49b520efb2e95a6ee537b02ec5b1b836_56257_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Actually 16x16, only showing 8x8&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;461&#34;
		data-flex-basis=&#34;1106px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;For each sub-block, 8 bin orientation histogram is created. So a total of 128 bin values are available.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/SDE.jpg&#34;
	width=&#34;968&#34;
	height=&#34;476&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/SDE_hu58ee0901517331e3436221a7560a45c9_70977_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/SDE_hu58ee0901517331e3436221a7560a45c9_70977_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;203&#34;
		data-flex-basis=&#34;488px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;It is represented as a vector to form keypoint descriptor. (16 cells * 8 orientations = 128 dimensional descriptor)&lt;/p&gt;
&lt;p&gt;In addition to this, several measures are taken to achieve robustness against illumination changes, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One trick we can employ to &lt;strong&gt;reduce the contribution of pixels farther from the keypoint&lt;/strong&gt; is to apply a Gaussian filter over the entire 16x16 window.&lt;/li&gt;
&lt;li&gt;Clamp all vector values &amp;gt; 0.2 to 0.2&lt;/li&gt;
&lt;li&gt;Renormalization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;feature-matching&#34;&gt;Feature Matching&lt;/h2&gt;
&lt;h3 id=&#34;basic-idea&#34;&gt;Basic Idea&lt;/h3&gt;
&lt;p&gt;It generally refers to computing distance between feature vectors to find correspondence.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Euclidean Distance&lt;/li&gt;
&lt;li&gt;Cosine Similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;problems&#34;&gt;Problems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Threshold is hard to pick.&lt;/li&gt;
&lt;li&gt;Non-distinctive features could have lots of close matches, only one of which is correct.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nn-distance-ratio&#34;&gt;NN Distance Ratio&lt;/h3&gt;
&lt;p&gt;Here, we introduce the approach by comparing distances of 2 nearest feature vector neighbor.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $d_1 \approx  d_2$, ratio $\frac{d_1}{d_2}$. (too close)&lt;/li&gt;
&lt;li&gt;AS $d_1 \ll d_2$, ratio $\frac{d_1}{d_2}$ tends to 0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Too choose the ratio, we can compute a probability distribution functions of ratios based on keypoints with hand-labeled ground truth.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/NNDR.jpg&#34;
	width=&#34;525&#34;
	height=&#34;266&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/NNDR_hu12f06fe6ca17368e0b72f80b710cd939_40089_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/NNDR_hu12f06fe6ca17368e0b72f80b710cd939_40089_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;473px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-description-matching/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;other-local-descriptors&#34;&gt;Other Local Descriptors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;SURF&lt;/li&gt;
&lt;li&gt;Shape Context&lt;/li&gt;
&lt;li&gt;Self-similarity Descriptor&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Feature Points &amp; Corners</title>
        <link>https://tonyfpy.github.io/p/computer-vision-feature-points-corners/</link>
        <pubDate>Fri, 10 Feb 2023 14:42:23 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-feature-points-corners/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 10nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 14nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Edges and corners are typically called &lt;strong&gt;features&lt;/strong&gt; that are unique to some part of the image. &lt;strong&gt;Local features&lt;/strong&gt;  describe one component of the image, not the entirely image.&lt;/p&gt;
&lt;p&gt;Corners are also called &lt;strong&gt;interest points&lt;/strong&gt; or &lt;strong&gt;key points&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Corners are important, because we can get &lt;strong&gt;spare correspondences&lt;/strong&gt;, which allows for fast matching between images.&lt;/p&gt;
&lt;p&gt;Finding distinctive and repeatable feature points can be difficult when we want our features to be &lt;strong&gt;invariant to large transformations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;geometric variation (translation, rotation, scale, perspective)&lt;/li&gt;
&lt;li&gt;appearance variation (reflectance, illumination)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-panorama-stitching&#34;&gt;Example: Panorama Stitching&lt;/h3&gt;
&lt;p&gt;Using panorama requires &lt;strong&gt;correspondences&lt;/strong&gt;. We have two images of a scene, and we can see that there is some overlay, but the images are not quite the same.&lt;/p&gt;
&lt;p&gt;We need our features/corners to have some sort of properties in this case: 1) &lt;strong&gt;distinctiveness&lt;/strong&gt;, 2) &lt;strong&gt;repeatability&lt;/strong&gt;, 3) &lt;strong&gt;compactness&lt;/strong&gt;, 4) &lt;strong&gt;efficiency&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;local-feature-matching&#34;&gt;Local Feature Matching&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Detection
&lt;ul&gt;
&lt;li&gt;Find a set of distinctive features (e.g. key points, edges)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Description:
&lt;ul&gt;
&lt;li&gt;Extract feature descriptor around each interest point as vector.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Matching:
&lt;ul&gt;
&lt;li&gt;Compute distance between feature vectors to find correspondence.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;corner-detection&#34;&gt;Corner Detection&lt;/h2&gt;
&lt;h3 id=&#34;basic-idea&#34;&gt;Basic idea&lt;/h3&gt;
&lt;p&gt;We do not know which other image locations the feature will end up being matched against. But we can compute how stable a location is in appearance with respect to small variations in its position. In other words, &lt;strong&gt;we can compare image patch against local neighbors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We want a window &lt;strong&gt;shift in any direction&lt;/strong&gt; to give a &lt;strong&gt;large change&lt;/strong&gt; in intensity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/CD.jpg&#34;
	width=&#34;539&#34;
	height=&#34;201&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/CD_hub5186c6b3aa5c483690a5be405bb9d7b_22375_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/CD_hub5186c6b3aa5c483690a5be405bb9d7b_22375_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;643px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;auto-correlation&#34;&gt;Auto-correlation&lt;/h3&gt;
&lt;p&gt;$$E(u, v)=\sum_{x, y} w(x, y)[I(x+u, y+v)-I(x, y)]^{2}$$&lt;/p&gt;
&lt;p&gt;, where $w(x, y)$ is window function, $I(x+u, y+v)$ is shifted intensity, $I(x, y)$ is intensity. If our &amp;ldquo;red box&amp;rdquo; goes outside the image, then we would need to pad the image.&lt;/p&gt;
&lt;p&gt;However, this approach is &lt;strong&gt;computationally expensive&lt;/strong&gt;. It needs $O(W^2+S^2+I^2)$, where $W$, $S$, $I$ stand for window size, shift range and image size, respectively.&lt;/p&gt;
&lt;h3 id=&#34;quadratic-surface&#34;&gt;Quadratic Surface&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/QS.jpg&#34;
	width=&#34;512&#34;
	height=&#34;214&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/QS_hu4409070e4a33ffe4681a89ec0705fd61_21561_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/QS_hu4409070e4a33ffe4681a89ec0705fd61_21561_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We can approximate $E(u,v)$ locally by a &lt;strong&gt;quadratic surface&lt;/strong&gt;, and look for that instead.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Taylor Series Expansion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A function $f$ can be represented by an infinite series of its derivatives at a single point $a$.&lt;/p&gt;
&lt;p&gt;$$ f = f(a)+\frac{f^{\prime}(a)}{1 !}(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^{2}+\frac{f^{\prime \prime \prime}(a)}{3 !}(x-a)^{3}+\cdots $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Local Quadratic Approximation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Local quadratic approximation of $E(u,v)$ in the neighborhood of $(0,0)$ is given by the &lt;strong&gt;second-order Taylor expansion&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA1.jpg&#34;
	width=&#34;599&#34;
	height=&#34;171&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA1_hub5f34c22bfb9b58a3200fadfe53457a9_25218_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA1_hub5f34c22bfb9b58a3200fadfe53457a9_25218_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;350&#34;
		data-flex-basis=&#34;840px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;This could be simplified as the following equation, where $M$ is a &lt;strong&gt;structure tensor&lt;/strong&gt; computed from image derivatives.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA2.jpg&#34;
	width=&#34;607&#34;
	height=&#34;200&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA2_hu58ee0901517331e3436221a7560a45c9_18732_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/LQA2_hu58ee0901517331e3436221a7560a45c9_18732_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;303&#34;
		data-flex-basis=&#34;728px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/M.jpg&#34;
	width=&#34;680&#34;
	height=&#34;200&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/M_hud4531995c10f99040ff50540ad6e262d_16734_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/M_hud4531995c10f99040ff50540ad6e262d_16734_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;340&#34;
		data-flex-basis=&#34;816px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/IxIy.jpg&#34;
	width=&#34;484&#34;
	height=&#34;156&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/IxIy_huddd6a4e4adacf5420d732890bc827c89_17364_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/IxIy_huddd6a4e4adacf5420d732890bc827c89_17364_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;310&#34;
		data-flex-basis=&#34;744px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpreting the Second Moment Matrix&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M$ is symmetric around the diagonal
&lt;ul&gt;
&lt;li&gt;Symmetric matrices have &lt;strong&gt;orthogonal eigenvectors&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$M$ is square
&lt;ul&gt;
&lt;li&gt;Square matrices are diagonalizable if some matrix $R$ exists such that $M = R^{-1}AR$ where $A$ has only diagonal entries and $R$ represents a change of basis (in 2D, a rotation).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/MRAR.jpg&#34;
	width=&#34;200&#34;
	height=&#34;68&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/MRAR_huad8e7fff2e9f1b263e070ffa4a705856_4224_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/MRAR_huad8e7fff2e9f1b263e070ffa4a705856_4224_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;294&#34;
		data-flex-basis=&#34;705px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We want to decompose $M$ to be able to &lt;strong&gt;analyze the scale of these quadratic surfaces&lt;/strong&gt; while &lt;strong&gt;ignoring the rotation&lt;/strong&gt;. This means we care that it is a corner or an edge, not which rotation of edge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/eigenvalues.jpg&#34;
	width=&#34;549&#34;
	height=&#34;273&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/eigenvalues_hufdf0a7aba31a58dbfa4f3b7a8911d286_29304_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/eigenvalues_hufdf0a7aba31a58dbfa4f3b7a8911d286_29304_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;482px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Or we can combine $\lambda _1$, $\lambda _2$ with some constant $\alpha$ to computer the &lt;strong&gt;cornerness score&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$ C={\lambda _1}{\lambda _2}-{\alpha}({\lambda _1}+{\lambda _2})^{2} $$&lt;/p&gt;
&lt;p&gt;In linear algebra, for diagonal matrices,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Determinant of a diagonal matrix is also the product of its diagonal entries&lt;/li&gt;
&lt;li&gt;Trace is just the sum of its diagonal entries&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we can avoid explicit eigenvalue computation by
$$C=\operatorname{det}(A)-\alpha \operatorname{trace}(A)^{2}$$&lt;/p&gt;
&lt;p&gt;Since $S^{-1}$ is the inverse matrix of $S$, we have $S^{-1}\cdot S = I$&lt;/p&gt;
&lt;p&gt;$$\operatorname{det}(I) = \operatorname{det}(S\cdot S^{-1}) = \operatorname{det}(S)\cdot \operatorname{det}(S) = 1$$&lt;/p&gt;
&lt;p&gt;We can now obtain that $\operatorname{det}(M) = \operatorname{det}(A)$&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;p&gt;$$C=\operatorname{det}(M)-\alpha \operatorname{trace}(M)^{2}$$&lt;/p&gt;
&lt;h3 id=&#34;harris-corner-detector&#34;&gt;Harris Corner Detector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Input image: computer $M$ at each pixel level&lt;/li&gt;
&lt;li&gt;Compute image derivatives $I_{x}$, $I_{y}$&lt;/li&gt;
&lt;li&gt;Compute $M$ components as squares of derivatives $I_{x}^{2}$, $I_{y}^{2}$&lt;/li&gt;
&lt;li&gt;Gaussian filter $g$ with width $s$: $g\left(I_{x}^{2}\right)$, $g\left(I_{y}^{2}\right)$, $g\left(I_{x} \circ I_{y}\right)$&lt;/li&gt;
&lt;li&gt;Compute cornerness $C=g\left(I_{x}^{2}\right) \circ g\left(I_{y}^{2}\right)-g\left(I_{x} \circ I_{y}\right)^{2} -\alpha\left[g\left(I_{x}^{2}\right)+g\left(I_{y}^{2}\right)\right]^{2}$&lt;/li&gt;
&lt;li&gt;Threshold $C$ to pick high cornerness&lt;/li&gt;
&lt;li&gt;Non-maximal suppression to pick peaks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: $a \circ b$ is &lt;strong&gt;Hadamard product&lt;/strong&gt; (element-wise multiplication).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;scale-invariant-detection&#34;&gt;Scale Invariant Detection&lt;/h2&gt;
&lt;h3 id=&#34;invariance--covariance&#34;&gt;Invariance &amp;amp; Covariance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Invariance&lt;/strong&gt;: image is transformed and corner locations do not change.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Covariance / Equivariance&lt;/strong&gt;: if we have two transformed versions of the same image, features should be detected in corresponding locations.&lt;/p&gt;
&lt;h3 id=&#34;properties-of-harris-detector&#34;&gt;Properties of Harris Detector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Corner response is invariant to image rotation
&lt;ul&gt;
&lt;li&gt;Ellipse rotates but its shape (i.e. eigenvalues) remains the same&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial invariance to &lt;strong&gt;affine intensity change&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Invariance to intensity shift $I \rightarrow I + b$&lt;/li&gt;
&lt;li&gt;Still affected by intensity scaling $I \rightarrow aI$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-invariant to image scale&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/scale.jpg&#34;
	width=&#34;752&#34;
	height=&#34;220&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/scale_hu49b520efb2e95a6ee537b02ec5b1b836_19368_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/scale_hu49b520efb2e95a6ee537b02ec5b1b836_19368_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;341&#34;
		data-flex-basis=&#34;820px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;automatic-scale-selection&#34;&gt;Automatic Scale Selection&lt;/h3&gt;
&lt;p&gt;For a point in one image, we can consider it as a function of region size&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/ASS.jpg&#34;
	width=&#34;997&#34;
	height=&#34;390&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/ASS_hu49b520efb2e95a6ee537b02ec5b1b836_79250_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/ASS_hu49b520efb2e95a6ee537b02ec5b1b836_79250_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;255&#34;
		data-flex-basis=&#34;613px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;For both image, we want to chose a scale that corresponds to the &lt;strong&gt;peak&lt;/strong&gt; in both images. It’s likely that the peaks is the best scale for finding the correpsondences.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/FR.jpg&#34;
	width=&#34;1352&#34;
	height=&#34;234&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-feature-points-corners/FR_hu49b520efb2e95a6ee537b02ec5b1b836_39560_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-feature-points-corners/FR_hu49b520efb2e95a6ee537b02ec5b1b836_39560_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;577&#34;
		data-flex-basis=&#34;1386px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Edge Detection</title>
        <link>https://tonyfpy.github.io/p/computer-vision-edge-detection/</link>
        <pubDate>Fri, 10 Feb 2023 10:54:42 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-edge-detection/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 10nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 10nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The process of identifying parts of a digital image with &lt;strong&gt;sharp changes&lt;/strong&gt; (discontinuities) in image intensity.&lt;/p&gt;
&lt;p&gt;It is helpful to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recognize objects&lt;/li&gt;
&lt;li&gt;Reconstruct scenes&lt;/li&gt;
&lt;li&gt;Edit images (artistically)&lt;/li&gt;
&lt;li&gt;Recover geometry and viewpoint&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;causes-of-edges&#34;&gt;Causes of Edges&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Surface orientation discontinuity
&lt;ul&gt;
&lt;li&gt;Physical edges in the 3D object&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Depth discontinuity
&lt;ul&gt;
&lt;li&gt;Viewing objects from a certain perspective&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Surface color discontinuity
&lt;ul&gt;
&lt;li&gt;Differences in colors between&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Illumination discontinuity
&lt;ul&gt;
&lt;li&gt;Lighting and shadows&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;characterizing-edges&#34;&gt;Characterizing Edges&lt;/h2&gt;
&lt;h3 id=&#34;gradient&#34;&gt;Gradient&lt;/h3&gt;
&lt;p&gt;Simple algorithm for edge detection is to find gradient. The first derivative is strongest (i.e. has the highest magnitude) where the intensity changes most rapidly. The sign of the derivative depends on whether the intensity falls from high to low or rises from low to high.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/ED.jpg&#34;
	width=&#34;1102&#34;
	height=&#34;476&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/ED_hu49b520efb2e95a6ee537b02ec5b1b836_54934_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/ED_hu49b520efb2e95a6ee537b02ec5b1b836_54934_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;231&#34;
		data-flex-basis=&#34;555px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;effects-of-noise&#34;&gt;Effects of Noise&lt;/h3&gt;
&lt;p&gt;Real images are &lt;strong&gt;noisy&lt;/strong&gt;, which results in lots of noise in the first derivative and starts to overwhelm the peaks. It would be almost impossible to detect where the edge occurred by inspecting the derivative graph with all that noise.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/noise.jpg&#34;
	width=&#34;782&#34;
	height=&#34;496&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/noise_hu49b520efb2e95a6ee537b02ec5b1b836_93450_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/noise_hu49b520efb2e95a6ee537b02ec5b1b836_93450_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;378px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/smooth.jpg&#34;
	width=&#34;824&#34;
	height=&#34;578&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/smooth_hu49b520efb2e95a6ee537b02ec5b1b836_99823_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/smooth_hu49b520efb2e95a6ee537b02ec5b1b836_99823_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;342px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We can fix this by filtering the signal (image) first to &lt;strong&gt;smooth out the noise&lt;/strong&gt; before computing the derivative. Here, a gaussian filter is common. To find edges, look for peaks in $\frac{\mathrm{d}}{\mathrm{d} x}(f*g)$&lt;/p&gt;
&lt;h3 id=&#34;derivative-theorem&#34;&gt;Derivative Theorem&lt;/h3&gt;
&lt;p&gt;$$\frac{d}{d x}(f * g)=f * \frac{d}{d x} g$$&lt;/p&gt;
&lt;p&gt;This means that we can compute the derivative just on the filter $g$ and convolve the image by the differentiated filter to get the same result. &lt;strong&gt;Instead of convolving the image twice (once to denoise and once to compute the derivative), we only need to convolve once.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;tradeoff&#34;&gt;Tradeoff&lt;/h3&gt;
&lt;p&gt;Smoothed derivative removes noise, but blurs edge, also finds edges at different frequencies. The bigger the kernel, the more smoothed out the edges of the image are since we start removing lower frequencies.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/SLT.jpg&#34;
	width=&#34;1478&#34;
	height=&#34;466&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/SLT_hu49b520efb2e95a6ee537b02ec5b1b836_77045_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/SLT_hu49b520efb2e95a6ee537b02ec5b1b836_77045_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;317&#34;
		data-flex-basis=&#34;761px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;designing-an-edge-detector&#34;&gt;Designing an Edge Detector&lt;/h2&gt;
&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Good detection
&lt;ul&gt;
&lt;li&gt;Finding all real edges&lt;/li&gt;
&lt;li&gt;Ignoring noise or other artifacts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Good localization
&lt;ul&gt;
&lt;li&gt;The edges detected must be as close as possible to the true edges&lt;/li&gt;
&lt;li&gt;Returning one point only for each true edge point&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cues-of-edge-detection&#34;&gt;Cues of Edge Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Differences in color, intensity, or texture across the boundary&lt;/li&gt;
&lt;li&gt;Continuity and closure&lt;/li&gt;
&lt;li&gt;High-level knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;canny-edge-detector&#34;&gt;Canny Edge Detector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Filter image with $x$, $y$ derivatives of Gaussian&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/DGF.jpg&#34;
	width=&#34;382&#34;
	height=&#34;157&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/DGF_hube11b2e98728249cb58beb349eb32f62_14110_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/DGF_hube11b2e98728249cb58beb349eb32f62_14110_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;583px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find magnitude and orientation of gradient&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ |G| =\sqrt{G_{x}^{2}+G_{y}^{2}} $$
$$\theta(x, y) =\arctan \left(\frac{G_{y}}{G_{x}}\right)$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-maximum Suppression e.g. Bilinear Interpolation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;BI.jpg&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;lsquo;Hysteresis&amp;rsquo; Thresholding
&lt;ul&gt;
&lt;li&gt;Define two thresholds: low and high
&lt;ul&gt;
&lt;li&gt;Grad. mag. &amp;gt; high threshold = strong edge&lt;/li&gt;
&lt;li&gt;Grad. mag. &amp;lt; low threshold = noise&lt;/li&gt;
&lt;li&gt;In between = weak edge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use the high threshold to start edge curves&lt;/li&gt;
&lt;li&gt;Use the low threshold to continue them into weak edges&lt;/li&gt;
&lt;li&gt;&amp;lsquo;Follow&amp;rsquo; edges starting from strong edge pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/canny.jpg&#34;
	width=&#34;448&#34;
	height=&#34;301&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/canny_hu0162cb701aa7ddb57f7a208d7daf3250_70554_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/canny_hu0162cb701aa7ddb57f7a208d7daf3250_70554_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Final Result&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;357px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-edge-detection/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://justin-liang.com/tutorials/canny/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CANNY EDGE DETECTION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://towardsdatascience.com/canny-edge-detection-step-by-step-in-python-computer-vision-b49c3a2d8123&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Canny Edge Detection Step by Step in Python — Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Fourier Series &amp; Fourier Transform</title>
        <link>https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/</link>
        <pubDate>Thu, 02 Feb 2023 11:16:17 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Feb 2nd, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 9nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fourier-series&#34;&gt;Fourier Series&lt;/h2&gt;
&lt;h3 id=&#34;a-general-idea&#34;&gt;A General Idea&lt;/h3&gt;
&lt;p&gt;Any univariate function can be rewritten as a weighted sum of sines and cosines of different frequencies.&lt;/p&gt;
&lt;p&gt;$$F_{Target} = F_{1}+F_{2}+F_{3} \ldots$$&lt;/p&gt;
&lt;p&gt;Here is the &lt;strong&gt;sine-cosine&lt;/strong&gt; form&lt;/p&gt;
&lt;p&gt;$$F = \sum_{n=1}^{\infty}\left(a_{n} \cos (n t)+b_{n} \sin (n t)\right)$$&lt;/p&gt;
&lt;h3 id=&#34;spatial-and-frequency-domain&#34;&gt;Spatial and Frequency Domain&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/SF.jpg&#34;
	width=&#34;372&#34;
	height=&#34;178&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/SF_hu2d9f041ef587ac7cce011e256b0c0385_9672_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/SF_hu2d9f041ef587ac7cce011e256b0c0385_9672_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;208&#34;
		data-flex-basis=&#34;501px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;amplitude--phase&#34;&gt;Amplitude &amp;amp; Phase&lt;/h3&gt;
&lt;p&gt;We can convert ine-cosine representation to &lt;strong&gt;amplitude-phase&lt;/strong&gt; form.&lt;/p&gt;
&lt;p&gt;$$F = \sum_{n=1}^{\infty}\left(a_{n} \sin \left(n x+\emptyset_{n}\right)\right)$$&lt;/p&gt;
&lt;p&gt;Amplitude encodes &lt;strong&gt;how much signal&lt;/strong&gt; there is at a particular frequency, while Phase encodes &lt;strong&gt;spatial information&lt;/strong&gt;. In other words, Amplitude tells you &amp;ldquo;how much&amp;rdquo; and Phase tells you &amp;ldquo;where&amp;rdquo;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fourier-transform&#34;&gt;Fourier transform&lt;/h2&gt;
&lt;h3 id=&#34;computing-the-fourier-transform&#34;&gt;Computing the Fourier Transform&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Continuous&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$H(\omega)=\int_{-\infty}^{\infty} h(x) e^{-j \omega x} d x$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discrete&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$H(k)=\frac{1}{N} \sum_{x=0}^{N-1} h(x) e^{-j \frac{2 \pi k x}{N}} \quad ,k=-\frac{k}{2} &amp;hellip; \frac{k}{2}$$&lt;/p&gt;
&lt;h3 id=&#34;properties-of-fourier-transforms&#34;&gt;Properties of Fourier Transforms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linearity $\mathrm{F}[a x(t)+b y(t)]=a \mathrm{~F}[x(t)]+b \mathrm{~F}[y(t)]$&lt;/li&gt;
&lt;li&gt;Fourier transform of a real signal is &lt;strong&gt;symmetric&lt;/strong&gt; about the origin.&lt;/li&gt;
&lt;li&gt;The energy of the signal is the same as the energy of its Fourier transform.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gaussian-filter-duality&#34;&gt;Gaussian Filter Duality&lt;/h3&gt;
&lt;p&gt;Fourier transform of one Gaussian is another Gaussian (with inverse variance).&lt;/p&gt;
&lt;p&gt;Why is this useful?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smooth degradation in frequency components&lt;/li&gt;
&lt;li&gt;No sharp cut-off&lt;/li&gt;
&lt;li&gt;No negative values&lt;/li&gt;
&lt;li&gt;Never zero (infinite extent)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2d-discrete-fourier-transform&#34;&gt;2D Discrete Fourier Transform&lt;/h3&gt;
&lt;p&gt;$$F[u, v]=\frac{1}{M N} \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} I[m, n] \cdot e^{-i 2 \pi\left(\frac{u m}{M}+\frac{v n}{N}\right)}$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/2DDFT.jpg&#34;
	width=&#34;627&#34;
	height=&#34;392&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/2DDFT_hu0f77aecba7dbdde634723272c2b72c55_50865_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/2DDFT_hu0f77aecba7dbdde634723272c2b72c55_50865_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;383px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Nyquist frequency is half the sampling rate of the signal. Sampling rate is size of image $M*N$, so Fourier transform images are ($\pm \frac{N}{2} $, $\pm \frac{N}{2} $).&lt;/p&gt;
&lt;p&gt;Image is rotationally symmetric about center because of negative frequencies.&lt;/p&gt;
&lt;p&gt;If we have infinite frequencies, why does the image end?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frequencies higher than Nyquist frequency end up falling on an existing sample.&lt;/li&gt;
&lt;li&gt;Nyquist frequency is half the sampling frequency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fourier-decomposition-image&#34;&gt;Fourier Decomposition Image&lt;/h3&gt;
&lt;p&gt;Intuitively, we can obtain the image by correlating the signal with a set of waves of increasing frequency.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In 2D, $O(N^2)$ operation&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;Fast Fourier Transform (FFT)&lt;/strong&gt;, $O(NlogN)$ (effective for larger image)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first row is set of the Fourier amplitude images and the second row is set of spatial domain imahe.
&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/amplitudematch.jpg&#34;
	width=&#34;697&#34;
	height=&#34;283&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/amplitudematch_huce222094ececc1ef2dc6fb35fb3a164b_59800_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/amplitudematch_huce222094ececc1ef2dc6fb35fb3a164b_59800_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;591px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The frequency amplitude of natural images are quite similar. &lt;strong&gt;Most information in the image is carried in the phase, not the amplitude.&lt;/strong&gt;
In Fourier space, Phase is more of the information that we see in the visual world.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/cheebra.jpg&#34;
	width=&#34;500&#34;
	height=&#34;250&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/cheebra_hudf23f477bf613526b59c8c9b9726af62_56824_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/cheebra_hudf23f477bf613526b59c8c9b9726af62_56824_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Zebra Phase &amp;#43; Cheetah Amplitude &amp; Cheetah Phase &amp;#43; Zebra Amplitude&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;200&#34;
		data-flex-basis=&#34;480px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-convolution-theorem&#34;&gt;The Convolution Theorem&lt;/h3&gt;
&lt;p&gt;The Fourier transform of the convolution of two functions is the product of their Fourier transforms.&lt;/p&gt;
&lt;p&gt;$$F[g \otimes h] = F[g]F[h]$$&lt;/p&gt;
&lt;p&gt;Convolution in spatial domain is equivalent to multiplication in frequency domain.&lt;/p&gt;
&lt;p&gt;$$g \otimes h=\mathrm{F}^{-1}[\mathrm{~F}[g] \mathrm{F}[h]]$$&lt;/p&gt;
&lt;p&gt;If convolution is just multiplication in the Fourier domain, isn’t deconvolution just using division?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sometimes, it clearly is invertible (e.g. a convolution with an identity filter)&lt;/li&gt;
&lt;li&gt;In one case, it clearly isn&amp;rsquo;t invertible (e.g. convolution with an all zero filter)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A Gaussian is only zero at infinity, so it is invertible&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/conv.jpg&#34;
	width=&#34;610&#34;
	height=&#34;350&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/conv_hu9395e367661edb0f9c21ab2dc3961cbf_56034_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/conv_hu9395e367661edb0f9c21ab2dc3961cbf_56034_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;418px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/deconv.jpg&#34;
	width=&#34;610&#34;
	height=&#34;349&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/deconv_huff484edc0f7480e440782c6f912da8c0_58189_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-fourier-series-fourier-transform/deconv_huff484edc0f7480e440782c6f912da8c0_58189_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;filtering-in-frequency-domain&#34;&gt;Filtering in Frequency Domain&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convert image and filter to Fourier domain&lt;/li&gt;
&lt;li&gt;Element-wise multiply their decompositions&lt;/li&gt;
&lt;li&gt;Convert result to spatial domain with inverse Fourier transform&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Image Filtering</title>
        <link>https://tonyfpy.github.io/p/computer-vision-image-filtering/</link>
        <pubDate>Tue, 31 Jan 2023 14:49:27 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-image-filtering/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Jan 31st, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Feb 2nd, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;filtering&#34;&gt;Filtering&lt;/h2&gt;
&lt;h3 id=&#34;def&#34;&gt;Def&lt;/h3&gt;
&lt;p&gt;An operation that modifies a (measured) signal, which includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;removing undesirable components&lt;/li&gt;
&lt;li&gt;transforming signal in a desirable way&lt;/li&gt;
&lt;li&gt;extract specific components of a signal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in measured or discrete signals because that is what we get from hardware such as sensors.&lt;/p&gt;
&lt;h3 id=&#34;1d-filtering-moving-average&#34;&gt;1D Filtering: Moving Average&lt;/h3&gt;
&lt;p&gt;We have a signal $S \in \mathcal{R}^{m \times 1}$, where $m$ is the number of samples we take of the signal, and each sample is a single scalar value. The moving average $h[n]$ at the point $n$ is the average value of the signal over the $k$ most recent samples, which can be represented as:&lt;/p&gt;
&lt;p&gt;$$h[n]=\frac{1}{k} \sum_{i=n-k+1}^{n} S[i]$$&lt;/p&gt;
&lt;p&gt;or
$$h[n]=\frac{1}{k} {I}^{T} S[n-k+1: n]$$&lt;/p&gt;
&lt;p&gt;, where $I$ is and identity vector.&lt;/p&gt;
&lt;h3 id=&#34;2d-filtering&#34;&gt;2D Filtering&lt;/h3&gt;
&lt;p&gt;For images, we’re interested in filtering along two spatial axes ($x$ and $y$) rather than a single one.&lt;/p&gt;
&lt;p&gt;$$h[m, n]=\sum_{k, l} f[k, l] S[m+k, n+l]$$&lt;/p&gt;
&lt;p&gt;The equation says that the filtered value at a location $(m,n)$ is the sum over products of the filtering function $f$ and the image $S$ in a local neighborhood. The size of the window is determined by $k$ and $l$.&lt;/p&gt;
&lt;h3 id=&#34;image-filtering&#34;&gt;Image Filtering&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/IF.jpg&#34;
	width=&#34;442&#34;
	height=&#34;226&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/IF_hud60089819ef7b0115b7722b64f02a782_17374_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/IF_hud60089819ef7b0115b7722b64f02a782_17374_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;195&#34;
		data-flex-basis=&#34;469px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Box Filter/Mean Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The box filter blurs the image because for each pixel, the values of its neighbors is &lt;strong&gt;averaged&lt;/strong&gt; with it, and &lt;strong&gt;preserve mean image intensity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gaussian Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The filter function is constructed by sampling the &lt;strong&gt;gaussian&lt;/strong&gt; function at uniform intervals. &lt;strong&gt;It&amp;rsquo;s a linear filter&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Gaussian pdfs have &lt;strong&gt;infinite extent&lt;/strong&gt;. &lt;strong&gt;Gaussian filters are discrete finite samplings of the Gaussian pdf&lt;/strong&gt;. What that means is that the filter can be arbitrarily large, and it will never be zero (to numerical precision).&lt;/p&gt;
&lt;p&gt;$$ G_{\sigma}=\frac{1}{2 \pi \sigma^{2}} e^{-\frac{\left(x^{2}+y^{2}\right)}{2 \sigma^{2}}} $$&lt;/p&gt;
&lt;p&gt;Unlike box filter, does not result in &amp;lsquo;grid&amp;rsquo;-like artifacts.&lt;/p&gt;
&lt;p&gt;Gaussian Filter Properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian convolved with Gaussian is another Gaussian&lt;/li&gt;
&lt;li&gt;We can smooth with small-width kernel, repeat, and get same result as larger-width kernel&lt;/li&gt;
&lt;li&gt;Convolving twice with Gaussian kernel of width $\sigma$ is same as convolving once with kernel of width $\sigma \sqrt{2} $&lt;/li&gt;
&lt;li&gt;How big should the Gaussian filter be?
&lt;ul&gt;
&lt;li&gt;Values at edges should be near zero&lt;/li&gt;
&lt;li&gt;Gaussians have infinite extent&lt;/li&gt;
&lt;li&gt;Set filter half-width to about $3\sigma$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sobel Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It has &amp;ldquo;1 2 1&amp;rdquo; pattern and is often used in &lt;strong&gt;edge detection&lt;/strong&gt;. It works by calculating the gradient of image intensity at each pixel within the image. It&amp;rsquo;s a &lt;strong&gt;high pass filter&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/sobel.jpg&#34;
	width=&#34;375&#34;
	height=&#34;66&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/sobel_hu2240eefee00e248333dc7c3fdcbcacc9_1728_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/sobel_hu2240eefee00e248333dc7c3fdcbcacc9_1728_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;568&#34;
		data-flex-basis=&#34;1363px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Median Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is a &lt;strong&gt;non-linear&lt;/strong&gt; filter. It operates over a window by selecting the &lt;strong&gt;median intensity&lt;/strong&gt; in the window. Median filtering is sorting.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s good at &lt;strong&gt;removing noises&lt;/strong&gt;. If we increase the size of the window, it can maintain strong edges, while other parts become blurred.&lt;/p&gt;
&lt;h3 id=&#34;practice-with-linear-filters&#34;&gt;Practice with Linear Filters&lt;/h3&gt;
&lt;p&gt;We can use filter to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enhance images
&lt;ul&gt;
&lt;li&gt;Denoise, resize, increase contrast, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Extract information from images
&lt;ul&gt;
&lt;li&gt;Texture, edges, distinctive points, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Detect patterns
&lt;ul&gt;
&lt;li&gt;Template matching&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F1.jpg&#34;
	width=&#34;521&#34;
	height=&#34;207&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F1_hu392a3c70df25a1f9896f7ab2d21ecd55_32259_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F1_hu392a3c70df25a1f9896f7ab2d21ecd55_32259_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;251&#34;
		data-flex-basis=&#34;604px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F2.jpg&#34;
	width=&#34;521&#34;
	height=&#34;198&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F2_huac7025d1a9ef42b2460b73ca69389e76_49504_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F2_huac7025d1a9ef42b2460b73ca69389e76_49504_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;263&#34;
		data-flex-basis=&#34;631px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F5.jpg&#34;
	width=&#34;564&#34;
	height=&#34;206&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F5_hu90f480af6279f8bfb5f99bd9066a024f_58167_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F5_hu90f480af6279f8bfb5f99bd9066a024f_58167_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;657px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F3.jpg&#34;
	width=&#34;628&#34;
	height=&#34;369&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F3_hu3f97afa3a3e8d31bfcaaf1a13dd2cccd_192991_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F3_hu3f97afa3a3e8d31bfcaaf1a13dd2cccd_192991_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;408px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F4.jpg&#34;
	width=&#34;630&#34;
	height=&#34;373&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/F4_hu3549cfd22b45a8393c24eb0751f31c80_191334_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/F4_hu3549cfd22b45a8393c24eb0751f31c80_191334_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;405px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dealing with Borders&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pixels are undefined outside the border of our image, so to apply the filter near the boundary, we nee to extend the image past its boundary using one of many possible methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Clip filter&lt;/strong&gt;: any value outside of the image is set to 0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Copy edge&lt;/strong&gt;: copy the pixel value of the nearest edge&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wrap around&lt;/strong&gt;: copy the values near the opposite edge&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reflect across edge&lt;/strong&gt;: treat out-of-bounds regions as ‘mirrors’ that reflect near-surface image pixels in reverse order&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b1.jpg&#34;
	width=&#34;318&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b1_hu58ab2db678a16a964fd350dc4a15b080_44902_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/b1_hu58ab2db678a16a964fd350dc4a15b080_44902_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b2.jpg&#34;
	width=&#34;318&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b2_hu60a746b593d6fe1a0dbc27ff358f7aac_51468_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/b2_hu60a746b593d6fe1a0dbc27ff358f7aac_51468_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b3.jpg&#34;
	width=&#34;318&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b3_hu661dca684b998aa4475277ab8d2445df_46269_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/b3_hu661dca684b998aa4475277ab8d2445df_46269_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b4.jpg&#34;
	width=&#34;318&#34;
	height=&#34;318&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/b4_hubec94fa1a5e4aff4b5caf37c182c502e_50111_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/b4_hubec94fa1a5e4aff4b5caf37c182c502e_50111_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;properties-of-image-filtering-methods&#34;&gt;Properties of Image Filtering Methods&lt;/h2&gt;
&lt;h3 id=&#34;correlation-and-convolution&#34;&gt;Correlation and Convolution&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;2D Correlation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$h[m, n]=\sum_{k, l} f[k, l] S[m+k, n+l]$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2D Convolution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$h[m, n]=\sum_{k, l} f[k, l] S[m-k, n-l]$$&lt;/p&gt;
&lt;p&gt;Convolution is the same as correlation with a 180° rotated filter kernel.&lt;/p&gt;
&lt;p&gt;Correlation and convolution are identical when the filter kernel is rotationally &lt;strong&gt;symmetric&lt;/strong&gt; (a square matrix that is equal to its transpose).&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;symmetric&lt;/strong&gt; filters: use either convolution or correlation&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;non-symmetric&lt;/strong&gt; filters: correlation is &lt;strong&gt;template matching&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;More can be found &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=tS-ib_mgGbU&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;linear-filter-properties&#34;&gt;Linear Filter Properties&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt; means that filtering an image with two (different) filters $f1$ and $f2$ results in the same output as filtering the images separately first before summing the intermediate outputs.
$$ imfilter(I, f1 + f2) = imfilter(I,f1) + imfilter(I,f2) $$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Translation invariance&lt;/strong&gt; means that shifting the original image before filtering results in the same output as filtering then shifting.
$$imfilter(I,shift(f)) = shift(imfilter(I,f))$$&lt;/li&gt;
&lt;li&gt;Any linear, shift-invariant operator can be represented as a convolution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;separability&#34;&gt;Separability&lt;/h3&gt;
&lt;p&gt;We can use an outer product to decompose the 2D filter into two 1D filters, where one is represented as a row column and the other a row vector.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/separability.jpg&#34;
	width=&#34;606&#34;
	height=&#34;366&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/separability_hu92cc13f735780ee724c3b009326abe22_53002_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/separability_hu92cc13f735780ee724c3b009326abe22_53002_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;397px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Note: Convolution vs filtering doesn’t matter here because the filter is &lt;strong&gt;symmetric&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Assume we have $M * N$ image, $P*Q$ filter&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;2D convolution: $MNPQ$ multiply-adds&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Separable 2D: $MN(P+Q)$ multiply-adds&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speed up = $\frac{PQ}{(P+Q)}$, e.g. 9*9 filter = 4.5 times faster&lt;/p&gt;
&lt;p&gt;In Gaussian Filters, a 2D Gaussian can be expressed as the product of two functions, a function of $x$ and a function of $y$.&lt;/p&gt;
&lt;p&gt;$$ G_{\sigma}=\frac{1}{2 \pi \sigma^{2}} e^{-\frac{\left(x^{2}+y^{2}\right)}{2 \sigma^{2}}} =  \left(\frac{1}{2 \pi \sigma^{2}} e^{-\frac{x^{2}}{2 \sigma^{2}}}\right)\left(\frac{1}{2 \pi \sigma^{2}} e^{-\frac{y^{2}}{2 \sigma^{2}}}\right)$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;applications-of-filters&#34;&gt;Applications of Filters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Template matching (SSD or Normxcorr2)
&lt;ul&gt;
&lt;li&gt;SSD can be done with linear filters, is sensitive to overall intensity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gaussian pyramid
&lt;ul&gt;
&lt;li&gt;Coarse-to-fine search, multi-scale detection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Laplacian pyramid
&lt;ul&gt;
&lt;li&gt;Teases apart different frequency bands while keeping spatial information&lt;/li&gt;
&lt;li&gt;Can be used for compositing in graphics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Downsampling
&lt;ul&gt;
&lt;li&gt;Need to sufficiently low-pass before downsampling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sampling&#34;&gt;Sampling&lt;/h2&gt;
&lt;h3 id=&#34;image-pyramid&#34;&gt;Image Pyramid&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/IP.jpg&#34;
	width=&#34;734&#34;
	height=&#34;365&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/IP_hu2a054bd143f1972ea72f966955bad124_213879_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/IP_hu2a054bd143f1972ea72f966955bad124_213879_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;482px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/white.jpg&#34;
	width=&#34;167&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/white_hu36e283984737934d1af0bcfe1c0c61db_1617_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/white_hu36e283984737934d1af0bcfe1c0c61db_1617_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;57&#34;
		data-flex-basis=&#34;137px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;aliasing&#34;&gt;Aliasing&lt;/h3&gt;
&lt;p&gt;Aliasing occurs when we sample a signal at a frequency that is too low such that we can’t properly reconstruct the original signal. In the image below, black points are samples, but we may get a different-looking reconstruction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/aliasing.jpg&#34;
	width=&#34;567&#34;
	height=&#34;143&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/aliasing_hu965fb104dfd6a7e46fe138c1e4e1232d_16542_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/aliasing_hu965fb104dfd6a7e46fe138c1e4e1232d_16542_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;396&#34;
		data-flex-basis=&#34;951px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;When two signals become indistinguishable from one another due to sampling, they are ‘aliases’ of one another.&lt;/p&gt;
&lt;p&gt;In real world, &lt;strong&gt;temporal and spatial aliasing&lt;/strong&gt; often happens when the sampling rate is not high enough.&lt;/p&gt;
&lt;h3 id=&#34;nyquist-shannon-sampling-theorem&#34;&gt;Nyquist-Shannon Sampling Theorem&lt;/h3&gt;
&lt;p&gt;When sampling a signal at discrete intervals, the sampling frequency must be $\ge  2 f_{max}$, max frequency of the input signal, to guarantee a perfect reconstruction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/NSST.jpg&#34;
	width=&#34;499&#34;
	height=&#34;205&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-image-filtering/NSST_hu7e0fa4cbda7f4a093a012710419ba7ef_35930_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-image-filtering/NSST_hu7e0fa4cbda7f4a093a012710419ba7ef_35930_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;584px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;anti-aliasing&#34;&gt;Anti-aliasing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sampling more often&lt;/li&gt;
&lt;li&gt;Removing all frequencies that are greater than half the new sampling frequency
&lt;ul&gt;
&lt;li&gt;Remove high frequencies with a &lt;strong&gt;low pass filter&lt;/strong&gt; (e.g. Gaussian Filter)&lt;/li&gt;
&lt;li&gt;It will lose info&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>[Computer Vision] Intro to CV and Images</title>
        <link>https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/</link>
        <pubDate>Fri, 27 Jan 2023 00:16:47 -0500</pubDate>
        
        <guid>https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;Authored by Tony Feng&lt;/p&gt;
&lt;p&gt;Created on Jan 27th, 2022&lt;/p&gt;
&lt;p&gt;Last Modified on Jan 27th, 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;This sereis of posts contains a summary of materials and readings from the course &lt;strong&gt;CSCI 1430 Computer Vision&lt;/strong&gt; that I&amp;rsquo;ve taken @ &lt;strong&gt;Brown University&lt;/strong&gt;. This course covers the topics of fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, motion estimation and tracking, image classification, scene understanding, and deep learning with neural networks. I posted these &amp;ldquo;Notes&amp;rdquo; (what I&amp;rsquo;ve learnt) for study and review only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-computer-vision&#34;&gt;What is Computer Vision&lt;/h2&gt;
&lt;h3 id=&#34;3r-concept&#34;&gt;3R Concept&lt;/h3&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://people.eecs.berkeley.edu/~malik/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jitendra Malik @ UC Berkeley&lt;/a&gt; has stated that the classic problems of computational vision are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;econstruction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;ecognition&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;e-organization&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cv--nearby-fields&#34;&gt;CV &amp;amp; Nearby Fields&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/CVCG.jpg&#34;
	width=&#34;679&#34;
	height=&#34;299&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/CVCG_hu9bac4c580a6d0d1aba38545f734c20b0_35515_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/CVCG_hu9bac4c580a6d0d1aba38545f734c20b0_35515_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;227&#34;
		data-flex-basis=&#34;545px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-an-image&#34;&gt;What is an Image&lt;/h2&gt;
&lt;h3 id=&#34;signal&#34;&gt;Signal&lt;/h3&gt;
&lt;p&gt;Signal is a (multi-dimensional) function that contains information about a &lt;strong&gt;phenomenon&lt;/strong&gt; – light, heat, gravity, population distribution, etc.&lt;/p&gt;
&lt;p&gt;Light reflected from an object creates a continuous signal that is measured by cameras. Natural signals are &lt;strong&gt;continuous&lt;/strong&gt;, but our measurements of them are &lt;strong&gt;discrete&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;sampling&#34;&gt;Sampling&lt;/h3&gt;
&lt;p&gt;It is a process of reduction of continuous signal to a discrete signal.&lt;/p&gt;
&lt;p&gt;Sampling in 1D takes a function and returns a &lt;strong&gt;vector&lt;/strong&gt; whose elements are values of that function at the sample points, while Sampling in 2D takes a function and returns a &lt;strong&gt;matrix&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/2D.jpg&#34;
	width=&#34;553&#34;
	height=&#34;485&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/2D_hu801390394efc7bef26f8a8b90f97aa6c_27441_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/2D_hu801390394efc7bef26f8a8b90f97aa6c_27441_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;114&#34;
		data-flex-basis=&#34;273px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A 2D image is a sampling of a 2D signal.&lt;/strong&gt; Note that the 2D signal can also be a projection (or slice) of a higher-dimensional signal like in MRI or CT scans. An image stores &lt;strong&gt;brightness/intensity&lt;/strong&gt; along $x$ and $y$ dimensions, while a video contains time-varying 2D signals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/intensity.jpg&#34;
	width=&#34;628&#34;
	height=&#34;382&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/intensity_hu57e23c9222c56aafa02b2eac14781631_55530_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/intensity_hu57e23c9222c56aafa02b2eac14781631_55530_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;164&#34;
		data-flex-basis=&#34;394px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;pixel&#34;&gt;Pixel&lt;/h3&gt;
&lt;p&gt;Pixel stands for &lt;strong&gt;picture element&lt;/strong&gt; and each associated with a value. We can approximate a pixel as a &lt;strong&gt;square frustum&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/frustum.jpg&#34;
	width=&#34;740&#34;
	height=&#34;393&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/frustum_hub96a13c44519ffe5d551985eb6a14ed3_43490_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/frustum_hub96a13c44519ffe5d551985eb6a14ed3_43490_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;451px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The diagram above arguably has an error (for convenience). The scene element should be flipped as projection onto the camera plane typically turns the image &lt;strong&gt;upside down&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;quantization&#34;&gt;Quantization&lt;/h3&gt;
&lt;p&gt;The function itself is continuous over the amount of light. When we convert it into a digital image, we need to take a continuous signal and turn it into a discrete set of intensity values to store in our image.&lt;/p&gt;
&lt;p&gt;After quantization, the original signal cannot be reconstructed anymore. This is in contrast to sampling, as a sampled but not quantized signal can be reconstructed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/quantization.jpg&#34;
	width=&#34;700&#34;
	height=&#34;279&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/quantization_hua4ea65834e375aacb0dc4b99bb2b29fd_25716_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/quantization_hua4ea65834e375aacb0dc4b99bb2b29fd_25716_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;250&#34;
		data-flex-basis=&#34;602px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quantization Effects – Radiometric Resolution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We often call this &lt;strong&gt;bit depth&lt;/strong&gt;. For photography, this is also related to &lt;strong&gt;dynamic range&lt;/strong&gt;, the minimum and maximum range of light intensity that can be measured/perceived/represented/displayed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/rr.jpg&#34;
	width=&#34;677&#34;
	height=&#34;222&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/rr_huc5828ede93c608458b752f0abf27a952_43514_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/rr_huc5828ede93c608458b752f0abf27a952_43514_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;304&#34;
		data-flex-basis=&#34;731px&#34;
	
&gt;&lt;img src=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white.jpg&#34;
	width=&#34;188&#34;
	height=&#34;291&#34;
	srcset=&#34;https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_480x0_resize_q75_box.jpg 480w, https://tonyfpy.github.io/p/computer-vision-intro-to-cv-and-images/white_hu36e283984737934d1af0bcfe1c0c61db_1665_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;64&#34;
		data-flex-basis=&#34;155px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;color&#34;&gt;Color&lt;/h3&gt;
&lt;p&gt;We handle color by having three arrays, one for each of &lt;strong&gt;red&lt;/strong&gt;, &lt;strong&gt;green&lt;/strong&gt; or &lt;strong&gt;blue&lt;/strong&gt; color channels. Combining the channels gives a color image. Practical matters when dealing with images in Python. We deal with color as an additional dimension in our arrays.&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        
    </channel>
</rss>
